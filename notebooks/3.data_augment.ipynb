{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation Pipeline v2.0\n",
    "\n",
    "**For Research Paper**: Complete pipeline for augmenting the symptom-disease dataset.\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "| Stage | Description | Output |\n",
    "|-------|-------------|--------|\n",
    "| **0** | Expand symptom vocabulary with Mayo Clinic symptoms | `symptom_columns.json` (updated) |\n",
    "| **1** | Generate synthetic samples for rare diseases (<20 samples) | `symptoms_augmented_no_demographics.csv` |\n",
    "| **2** | Add demographic variables (age, sex) | `symptoms_augmented_with_demographics.csv` |\n",
    "\n",
    "## Requirements\n",
    "- `data/rare_diseases_symptoms_template.json` - Filled with Mayo Clinic symptoms\n",
    "- `data/final_disease_demographics.json` - Demographics from ChatGPT + synthetic rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\Users\\henry\\Desktop\\Programming\\Python\\Multimodal_Diagnosis\n",
      "\n",
      "Input files:\n",
      "  Data: True - c:\\Users\\henry\\Desktop\\Programming\\Python\\Multimodal_Diagnosis\\data\\processed\\symptoms\\symptoms_to_disease_cleaned.csv\n",
      "  Vocab: True - c:\\Users\\henry\\Desktop\\Programming\\Python\\Multimodal_Diagnosis\\data\\symptom_vocabulary.json\n",
      "  Template: True - c:\\Users\\henry\\Desktop\\Programming\\Python\\Multimodal_Diagnosis\\data\\rare_diseases_symptoms_template.json\n",
      "  Demographics: True - c:\\Users\\henry\\Desktop\\Programming\\Python\\Multimodal_Diagnosis\\data\\final_disease_demographics.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import sys\n",
    "import gc\n",
    "from collections import Counter\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "from utils.symptom_normalizer import normalize_symptom\n",
    "\n",
    "# Paths\n",
    "# Input files\n",
    "data_path = project_root / \"data\" / \"processed\" / \"symptoms\" / \"symptoms_to_disease_cleaned.csv\"\n",
    "symptom_cols_path = project_root / \"data\" / \"symptom_vocabulary.json\"\n",
    "template_path = project_root / \"data\" / \"rare_diseases_symptoms_template.json\"\n",
    "category_map_path = project_root / \"data\" / \"disease_mapping.json\"\n",
    "demographics_path = project_root / \"data\" / \"final_disease_demographics.json\"\n",
    "\n",
    "# Output files - vocabulary saved to SAME file (overwrites original)\n",
    "expanded_vocab_path = symptom_cols_path  # Overwrites original\n",
    "output_no_demo_path = project_root / \"data\" / \"processed\" / \"symptoms\" / \"symptoms_augmented_no_demographics.csv\"\n",
    "output_with_demo_path = project_root / \"data\" / \"processed\" / \"symptoms\" / \"symptoms_augmented_with_demographics.csv\"\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"\\nInput files:\")\n",
    "print(f\"  Data: {data_path.exists()} - {data_path}\")\n",
    "print(f\"  Vocab: {symptom_cols_path.exists()} - {symptom_cols_path}\")\n",
    "print(f\"  Template: {template_path.exists()} - {template_path}\")\n",
    "print(f\"  Demographics: {demographics_path.exists()} - {demographics_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Stage 0: Expand Symptom Vocabulary\n",
    "\n",
    "The current vocabulary has 377 symptoms. Many Mayo Clinic symptoms don't have exact matches.\n",
    "\n",
    "**Strategy**: Add clinically important new symptoms to the vocabulary (appearing in 5+ diseases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vocabulary: 458 symptoms\n",
      "Diseases in template: 135\n"
     ]
    }
   ],
   "source": [
    "# Load current vocabulary\n",
    "with open(symptom_cols_path) as f:\n",
    "    ORIGINAL_VOCAB = json.load(f)\n",
    "ORIGINAL_SET = set(s.lower() for s in ORIGINAL_VOCAB)\n",
    "\n",
    "print(f\"Original vocabulary: {len(ORIGINAL_VOCAB)} symptoms\")\n",
    "\n",
    "# Load template with Mayo Clinic symptoms\n",
    "with open(template_path) as f:\n",
    "    template = json.load(f)\n",
    "\n",
    "print(f\"Diseases in template: {len(template)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting and normalizing symptoms from template...\n",
      "Total unique mayo symptoms: 690\n",
      "New symptoms (not in vocabulary): 525\n"
     ]
    }
   ],
   "source": [
    "# Extract all unique symptoms from template\n",
    "all_mayo_symptoms = set()\n",
    "symptom_counts = Counter()\n",
    "\n",
    "print(\"Extracting and normalizing symptoms from template...\")\n",
    "for disease, info in template.items():\n",
    "    mayo = info.get(\"mayo_clinic_symptoms\", [])\n",
    "    for sym in mayo:\n",
    "        # Normalize symptom using project specific normalizer\n",
    "        sym_norm = normalize_symptom(sym)\n",
    "        \n",
    "        if sym_norm and not sym_norm.startswith('\"notes\"'):\n",
    "            all_mayo_symptoms.add(sym_norm)\n",
    "            symptom_counts[sym_norm] += 1\n",
    "\n",
    "print(f\"Total unique mayo symptoms: {len(all_mayo_symptoms)}\")\n",
    "\n",
    "# Find symptoms NOT in current vocabulary\n",
    "new_symptoms = [s for s in all_mayo_symptoms if s not in ORIGINAL_SET]\n",
    "print(f\"New symptoms (not in vocabulary): {len(new_symptoms)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common new symptoms (top 50):\n",
      "------------------------------------------------------------\n",
      "  [ 1x] enlargement of the breast tissue\n",
      "  [ 1x] heavy menstrual bleeding\n",
      "  [ 1x] black stool\n",
      "  [ 1x] lines of rash\n",
      "  [ 1x] language difficulty\n",
      "  [ 1x] swelling of ankles and legs\n",
      "  [ 1x] poor balance or coordination\n",
      "  [ 1x] bowel obstruction\n",
      "  [ 1x] discoloration\n",
      "  [ 1x] neck stiffness\n",
      "  [ 1x] red discolored skin\n",
      "  [ 1x] urinary tract infections\n",
      "  [ 1x] blisters on chest\n",
      "  [ 1x] heart disorders\n",
      "  [ 1x] long-lasting cough with thick mucus\n",
      "  [ 1x] red streaks on skin\n",
      "  [ 1x] issues with cognitive development\n",
      "  [ 1x] bowed or bent bones\n",
      "  [ 1x] uterine tenderness\n",
      "  [ 1x] coordination problems\n",
      "  [ 1x] iris that jiggles\n",
      "  [ 1x] cloudy urine\n",
      "  [ 1x] trouble with speech\n",
      "  [ 1x] musculoskeletal pain\n",
      "  [ 1x] webbed nect\n",
      "  [ 1x] changes in alertness\n",
      "  [ 1x] increased sensitivity to cold\n",
      "  [ 1x] memory fog\n",
      "  [ 1x] enlarged head in infants\n",
      "  [ 1x] fluid buildup around the lungs\n",
      "  [ 1x] involuntary spasms\n",
      "  [ 1x] slow breathing\n",
      "  [ 1x] non-healing sores on feet or toes\n",
      "  [ 1x] flu\n",
      "  [ 1x] heavy bleeding during labour\n",
      "  [ 1x] electrolyte imbalance\n",
      "  [ 1x] muscle stiffness in back shoulders arms and legs\n",
      "  [ 1x] cutaneous skin\n",
      "  [ 1x] increased eye floaters\n",
      "  [ 1x] warts around genital area\n",
      "  [ 1x] puffy face\n",
      "  [ 1x] toe stiffness\n",
      "  [ 1x] excessive sweating\n",
      "  [ 1x] memory problems\n",
      "  [ 1x] joint ache\n",
      "  [ 1x] extra fuild behind a fetus' next\n",
      "  [ 1x] loss of feeling in arms and legs\n",
      "  [ 1x] eye muscle paralysis\n",
      "  [ 1x] trouble focusin\n",
      "  [ 1x] loss of reflexes\n"
     ]
    }
   ],
   "source": [
    "# Show most common new symptoms\n",
    "new_symptom_counts = [(s, symptom_counts[s]) for s in new_symptoms]\n",
    "new_symptom_counts.sort(key=lambda x: -x[1])\n",
    "\n",
    "print(\"Most common new symptoms (top 50):\")\n",
    "print(\"-\" * 60)\n",
    "for sym, count in new_symptom_counts[:50]:\n",
    "    print(f\"  [{count:2d}x] {sym}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symptoms appearing in >= 2 diseases: 0\n",
      "\n",
      "Symptoms to add:\n"
     ]
    }
   ],
   "source": [
    "# CONFIGURATION: Filter new symptoms\n",
    "# Only add symptoms that appear in at least MIN_DISEASE_COUNT diseases\n",
    "MIN_DISEASE_COUNT = 2  # Only add symptoms appearing in 5+ diseases\n",
    "\n",
    "symptoms_to_add = [s for s, count in new_symptom_counts if count >= MIN_DISEASE_COUNT]\n",
    "\n",
    "print(f\"Symptoms appearing in >= {MIN_DISEASE_COUNT} diseases: {len(symptoms_to_add)}\")\n",
    "print(\"\\nSymptoms to add:\")\n",
    "for s in symptoms_to_add:\n",
    "    print(f\"  - {s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vocabulary: 458 symptoms\n",
      "Expanded vocabulary: 458 symptoms (+0)\n",
      "\n",
      "Saved expanded vocabulary to: c:\\Users\\henry\\Desktop\\Programming\\Python\\Multimodal_Diagnosis\\data\\symptom_vocabulary.json\n",
      "NOTE: Original vocabulary file has been updated with new symptoms.\n"
     ]
    }
   ],
   "source": [
    "# Create expanded vocabulary\n",
    "EXPANDED_VOCAB = ORIGINAL_VOCAB + symptoms_to_add\n",
    "EXPANDED_SET = set(s.lower() for s in EXPANDED_VOCAB)\n",
    "\n",
    "print(f\"Original vocabulary: {len(ORIGINAL_VOCAB)} symptoms\")\n",
    "print(f\"Expanded vocabulary: {len(EXPANDED_VOCAB)} symptoms (+{len(symptoms_to_add)})\")\n",
    "\n",
    "# Save expanded vocabulary (overwrites original)\n",
    "with open(expanded_vocab_path, 'w') as f:\n",
    "    json.dump(EXPANDED_VOCAB, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved expanded vocabulary to: {expanded_vocab_path}\")\n",
    "print(\"NOTE: Original vocabulary file has been updated with new symptoms.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Stage 1: Symptom Mapping & Synthetic Data Generation\n",
    "\n",
    "1. Map Mayo Clinic symptoms to expanded vocabulary\n",
    "2. Generate synthetic samples for diseases with <20 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset: 206,267 rows, 627 diseases\n",
      "\n",
      "Diseases with <20 samples: 128\n",
      "Total samples in rare diseases: 1,085\n"
     ]
    }
   ],
   "source": [
    "# Load original dataset\n",
    "df = pd.read_csv(data_path)\n",
    "print(f\"Original dataset: {len(df):,} rows, {df['diseases'].nunique()} diseases\")\n",
    "\n",
    "# Get disease counts\n",
    "counts = df['diseases'].value_counts()\n",
    "rare_diseases = counts[counts < 20]\n",
    "print(f\"\\nDiseases with <20 samples: {len(rare_diseases)}\")\n",
    "print(f\"Total samples in rare diseases: {rare_diseases.sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 542 disease -> category mappings\n"
     ]
    }
   ],
   "source": [
    "# Load category mapping\n",
    "with open(category_map_path) as f:\n",
    "    category_map = json.load(f)\n",
    "\n",
    "# Create disease -> category lookup\n",
    "disease_to_category = {}\n",
    "for cat, diseases in category_map.items():\n",
    "    for d in diseases:\n",
    "        disease_to_category[d] = cat\n",
    "\n",
    "print(f\"Loaded {len(disease_to_category)} disease -> category mappings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined mapping and generation functions\n"
     ]
    }
   ],
   "source": [
    "def map_symptoms_to_vocab(symptoms_list, vocab_set):\n",
    "    \"\"\"\n",
    "    Map a list of symptoms to the vocabulary.\n",
    "    Returns symptoms that exist in the vocabulary.\n",
    "    \"\"\"\n",
    "    mapped = []\n",
    "    for sym in symptoms_list:\n",
    "        sym_clean = sym.strip().lower()\n",
    "        if sym_clean in vocab_set:\n",
    "            mapped.append(sym_clean)\n",
    "    return list(set(mapped))  # Remove duplicates\n",
    "\n",
    "\n",
    "def generate_synthetic_samples(disease: str, symptoms: list, n_samples: int,\n",
    "                               all_symptoms: list, min_sym: int = 4, max_sym: int = 8) -> list:\n",
    "    \"\"\"\n",
    "    Generate synthetic samples for a disease.\n",
    "    Each sample has random 4-8 symptoms selected from the symptom list.\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    category = disease_to_category.get(disease, \"Unknown Type\")\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        # Select random symptoms\n",
    "        n_sym = random.randint(min_sym, min(max_sym, len(symptoms)))\n",
    "        selected = random.sample(symptoms, n_sym)\n",
    "        \n",
    "        # Create row with all symptoms as 0\n",
    "        row = {col: 0 for col in all_symptoms}\n",
    "        \n",
    "        # Set selected symptoms to 1\n",
    "        for sym in selected:\n",
    "            if sym in row:\n",
    "                row[sym] = 1\n",
    "        \n",
    "        row['diseases'] = disease\n",
    "        row['disease_category'] = category\n",
    "        row['symptoms'] = \", \".join(selected)\n",
    "        \n",
    "        samples.append(row)\n",
    "    \n",
    "    return samples\n",
    "\n",
    "print(\"Defined mapping and generation functions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symptom mapping results:\n",
      "  Total mayo symptoms: 1255\n",
      "  Mapped to vocabulary: 578 (46.1%)\n",
      "  Diseases ready for synthesis (>=4 symptoms): 72\n"
     ]
    }
   ],
   "source": [
    "# Map symptoms for each disease in template\n",
    "disease_mapped_symptoms = {}\n",
    "mapping_stats = {'total': 0, 'mapped': 0, 'diseases_ready': 0}\n",
    "\n",
    "for disease, info in template.items():\n",
    "    mayo = info.get(\"mayo_clinic_symptoms\", [])\n",
    "    if not mayo:\n",
    "        continue\n",
    "    \n",
    "    mapped = map_symptoms_to_vocab(mayo, EXPANDED_SET)\n",
    "    mapping_stats['total'] += len(mayo)\n",
    "    mapping_stats['mapped'] += len(mapped)\n",
    "    \n",
    "    if len(mapped) >= 4:  # Minimum for synthetic generation\n",
    "        disease_mapped_symptoms[disease] = mapped\n",
    "        mapping_stats['diseases_ready'] += 1\n",
    "\n",
    "print(f\"Symptom mapping results:\")\n",
    "print(f\"  Total mayo symptoms: {mapping_stats['total']}\")\n",
    "print(f\"  Mapped to vocabulary: {mapping_stats['mapped']} ({100*mapping_stats['mapped']/mapping_stats['total']:.1f}%)\")\n",
    "print(f\"  Diseases ready for synthesis (>=4 symptoms): {mapping_stats['diseases_ready']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1,154 synthetic samples for 72 diseases\n",
      "\n",
      "Generation details:\n",
      "  rocky mountain spotted fever: 1 -> 25 (+24, 9 symptoms available)\n",
      "  myocarditis: 1 -> 25 (+24, 5 symptoms available)\n",
      "  kaposi sarcoma: 1 -> 25 (+24, 5 symptoms available)\n",
      "  chronic ulcer: 1 -> 25 (+24, 5 symptoms available)\n",
      "  gas gangrene: 1 -> 25 (+24, 5 symptoms available)\n",
      "  thalassemia: 1 -> 25 (+24, 4 symptoms available)\n",
      "  typhoid fever: 1 -> 25 (+24, 5 symptoms available)\n",
      "  rheumatic fever: 2 -> 25 (+23, 4 symptoms available)\n",
      "  human immunodeficiency virus infection (hiv): 2 -> 25 (+23, 8 symptoms available)\n",
      "  hashimoto thyroiditis: 2 -> 25 (+23, 9 symptoms available)\n",
      "  sporotrichosis: 3 -> 25 (+22, 6 symptoms available)\n",
      "  cat scratch disease: 3 -> 25 (+22, 6 symptoms available)\n",
      "  dengue fever: 3 -> 25 (+22, 7 symptoms available)\n",
      "  adrenal cancer: 3 -> 25 (+22, 6 symptoms available)\n",
      "  necrotizing fasciitis: 3 -> 25 (+22, 5 symptoms available)\n",
      "  connective tissue disorder: 3 -> 25 (+22, 6 symptoms available)\n",
      "  myelodysplastic syndrome: 3 -> 25 (+22, 5 symptoms available)\n",
      "  primary thrombocythemia: 3 -> 25 (+22, 7 symptoms available)\n",
      "  hemochromatosis: 3 -> 25 (+22, 6 symptoms available)\n",
      "  amyloidosis: 4 -> 25 (+21, 7 symptoms available)\n",
      "  ... and 52 more diseases\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic samples\n",
    "random.seed(42)\n",
    "TARGET_SAMPLES = 25  # Minimum samples per disease\n",
    "\n",
    "all_synthetic = []\n",
    "generation_log = []\n",
    "\n",
    "for disease, symptoms in disease_mapped_symptoms.items():\n",
    "    current_count = counts.get(disease, 0)\n",
    "    \n",
    "    if current_count >= TARGET_SAMPLES:\n",
    "        continue\n",
    "    \n",
    "    n_new = TARGET_SAMPLES - current_count\n",
    "    samples = generate_synthetic_samples(disease, symptoms, n_new, EXPANDED_VOCAB)\n",
    "    all_synthetic.extend(samples)\n",
    "    \n",
    "    generation_log.append({\n",
    "        'disease': disease,\n",
    "        'original': current_count,\n",
    "        'added': n_new,\n",
    "        'symptoms_available': len(symptoms)\n",
    "    })\n",
    "\n",
    "print(f\"Generated {len(all_synthetic):,} synthetic samples for {len(generation_log)} diseases\")\n",
    "print(\"\\nGeneration details:\")\n",
    "for log in generation_log[:20]:\n",
    "    print(f\"  {log['disease']}: {log['original']} -> {log['original'] + log['added']} (+{log['added']}, {log['symptoms_available']} symptoms available)\")\n",
    "if len(generation_log) > 20:\n",
    "    print(f\"  ... and {len(generation_log) - 20} more diseases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new columns to add.\n",
      "Expanded original dataset to 377 columns\n"
     ]
    }
   ],
   "source": [
    "# Create expanded base dataset with new symptom columns\n",
    "# Optimization: Use int8 and avoiding fragmentation by using concat instead of loop insert\n",
    "\n",
    "# Identify new columns to add\n",
    "new_cols_to_add = [sym for sym in symptoms_to_add if sym not in df.columns]\n",
    "\n",
    "if new_cols_to_add:\n",
    "    print(f\"Adding {len(new_cols_to_add)} new symptom columns (int8)...\")\n",
    "    # Create a separate DataFrame for new columns\n",
    "    new_data = pd.DataFrame(0, index=df.index, columns=new_cols_to_add, dtype='int8')\n",
    "    \n",
    "    # Concatenate once\n",
    "    df = pd.concat([df, new_data], axis=1)\n",
    "else:\n",
    "    print(\"No new columns to add.\")\n",
    "\n",
    "print(f\"Expanded original dataset to {len(df.columns)} columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original samples: 206,267\n",
      "Synthetic samples: 1,154\n",
      "Total augmented: 207,421\n",
      "Memory cleanup: deleted original df\n"
     ]
    }
   ],
   "source": [
    "# Combine original + synthetic\n",
    "if all_synthetic:\n",
    "    df_synthetic = pd.DataFrame(all_synthetic)\n",
    "    \n",
    "    # Ensure synthetic dataframe has all columns\n",
    "    # Optimization: Use reindex which is faster/cleaner\n",
    "    df_synthetic = df_synthetic.reindex(columns=df.columns, fill_value=0).astype(df.dtypes)\n",
    "    \n",
    "    # Concatenate\n",
    "    df_augmented = pd.concat([df, df_synthetic], ignore_index=True)\n",
    "    \n",
    "    print(f\"Original samples: {len(df):,}\")\n",
    "    print(f\"Synthetic samples: {len(df_synthetic):,}\")\n",
    "    print(f\"Total augmented: {len(df_augmented):,}\")\n",
    "    \n",
    "    # cleanup\n",
    "    del df_synthetic\n",
    "else:\n",
    "    df_augmented = df\n",
    "    print(\"No synthetic samples generated\")\n",
    "\n",
    "# Free up memory\n",
    "del df\n",
    "gc.collect()\n",
    "print(\"Memory cleanup: deleted original df\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before augmentation: 128 diseases with <20 samples\n",
      "After augmentation: 59 diseases with <20 samples\n",
      "\n",
      "Diseases still below 20 samples:\n",
      "  19  otosclerosis\n",
      "  18  cyst of the eyelid\n",
      "  14  fibrocystic breast disease\n",
      "  14  pneumoconiosis\n",
      "  13  congenital malformation syndrome\n",
      "  13  hpv\n",
      "  13  factitious disorder\n",
      "  12  raynaud disease\n",
      "  12  moyamoya disease\n",
      "  11  galactorrhea of unknown cause\n",
      "  11  zenker diverticulum\n",
      "  11  myoclonus\n",
      "  11  pulmonic valve disease\n",
      "  10  testicular cancer\n",
      "  10  vesicoureteral reflux\n",
      "  10  avascular necrosis\n",
      "  10  reactive arthritis\n",
      "  10  decubitus ulcer\n",
      "  10  optic neuritis\n",
      "  10  granuloma inguinale\n",
      "   9  vacterl syndrome\n",
      "   9  lichen planus\n",
      "   9  hemarthrosis\n",
      "   9  placenta previa\n",
      "   9  aphakia\n",
      "   9  thyroid cancer\n",
      "   8  hypercholesterolemia\n",
      "   8  spinocerebellar ataxia\n",
      "   7  omphalitis\n",
      "   7  pemphigus\n",
      "   6  empyema\n",
      "   6  priapism\n",
      "   6  vulvar cancer\n",
      "   6  breast cancer\n",
      "   6  hypertrophic obstructive cardiomyopathy (hocm)\n",
      "   6  tuberous sclerosis\n",
      "   6  g6pd enzyme deficiency\n",
      "   6  blepharospasm\n",
      "   6  edward syndrome\n",
      "   5  spherocytosis\n",
      "   5  uterine cancer\n",
      "   5  pelvic fistula\n",
      "   5  vitamin a deficiency\n",
      "   5  cryptorchidism\n",
      "   4  intussusception\n",
      "   4  oral leukoplakia\n",
      "   4  open wound of the hand\n",
      "   3  lymphangitis\n",
      "   3  esophageal varices\n",
      "   3  obesity\n",
      "   3  hyperlipidemia\n",
      "   2  diabetic kidney disease\n",
      "   2  carcinoid syndrome\n",
      "   1  hypergammaglobulinemia\n",
      "   1  diabetes\n",
      "   1  turner syndrome\n",
      "   1  huntington disease\n",
      "   1  foreign body in the nose\n",
      "   1  high blood pressure\n"
     ]
    }
   ],
   "source": [
    "# Verify rare disease counts improved\n",
    "new_counts = df_augmented['diseases'].value_counts()\n",
    "new_rare = new_counts[new_counts < 20]\n",
    "\n",
    "print(f\"Before augmentation: {len(rare_diseases)} diseases with <20 samples\")\n",
    "print(f\"After augmentation: {len(new_rare)} diseases with <20 samples\")\n",
    "print(f\"\\nDiseases still below 20 samples:\")\n",
    "for d, c in new_rare.items():\n",
    "    print(f\"  {c:2d}  {d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dataset WITHOUT demographics:\n",
      "  Path: c:\\Users\\henry\\Desktop\\Programming\\Python\\Multimodal_Diagnosis\\data\\processed\\symptoms\\symptoms_augmented_no_demographics.csv\n",
      "  Size: 156.8 MB\n",
      "  Rows: 207,421\n",
      "  Columns: 377\n"
     ]
    }
   ],
   "source": [
    "# Save dataset WITHOUT demographics\n",
    "df_augmented.to_csv(output_no_demo_path, index=False)\n",
    "\n",
    "print(f\"Saved dataset WITHOUT demographics:\")\n",
    "print(f\"  Path: {output_no_demo_path}\")\n",
    "print(f\"  Size: {output_no_demo_path.stat().st_size / 1024 / 1024:.1f} MB\")\n",
    "print(f\"  Rows: {len(df_augmented):,}\")\n",
    "print(f\"  Columns: {len(df_augmented.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Stage 2: Add Demographics (Age, Sex)\n",
    "\n",
    "Using merged demographics from ChatGPT + synthetic rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded demographics for 667 diseases\n",
      "\n",
      "Demographic coverage:\n",
      "  Total diseases in dataset: 630\n",
      "  Covered by demographics: 623 (98.9%)\n",
      "  Missing (will use defaults): 7\n"
     ]
    }
   ],
   "source": [
    "# Load demographics\n",
    "with open(demographics_path) as f:\n",
    "    demographics = json.load(f)\n",
    "\n",
    "print(f\"Loaded demographics for {len(demographics)} diseases\")\n",
    "\n",
    "# Check coverage\n",
    "all_diseases = set(df_augmented['diseases'].unique())\n",
    "demo_diseases = set(demographics.keys())\n",
    "covered = all_diseases & demo_diseases\n",
    "missing = all_diseases - demo_diseases\n",
    "\n",
    "print(f\"\\nDemographic coverage:\")\n",
    "print(f\"  Total diseases in dataset: {len(all_diseases)}\")\n",
    "print(f\"  Covered by demographics: {len(covered)} ({100*len(covered)/len(all_diseases):.1f}%)\")\n",
    "print(f\"  Missing (will use defaults): {len(missing)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined demographic sampling functions\n"
     ]
    }
   ],
   "source": [
    "# Default demographics for missing diseases\n",
    "DEFAULT_DEMO = {\n",
    "    \"age_min\": 10,\n",
    "    \"age_max\": 80,\n",
    "    \"age_peak\": 45,\n",
    "    \"male_pct\": 50\n",
    "}\n",
    "\n",
    "def sample_age(demo: dict) -> int:\n",
    "    \"\"\"Sample age from triangular distribution.\"\"\"\n",
    "    age_min = demo.get('age_min', 10)\n",
    "    age_max = demo.get('age_max', 80)\n",
    "    age_peak = demo.get('age_peak', 45)\n",
    "    \n",
    "    # Handle edge cases\n",
    "    if age_min == age_max:\n",
    "        return int(age_min)\n",
    "    \n",
    "    age_peak = max(age_min, min(age_peak, age_max))\n",
    "    \n",
    "    if age_min == age_peak or age_peak == age_max:\n",
    "        age = np.random.uniform(age_min, age_max)\n",
    "    else:\n",
    "        age = np.random.triangular(age_min, age_peak, age_max)\n",
    "    \n",
    "    return int(np.clip(age, 0, 100))\n",
    "\n",
    "\n",
    "def sample_sex(demo: dict) -> str:\n",
    "    \"\"\"Sample sex from Bernoulli distribution.\"\"\"\n",
    "    male_pct = demo.get('male_pct', 50)\n",
    "    return 'M' if np.random.random() * 100 < male_pct else 'F'\n",
    "\n",
    "print(\"Defined demographic sampling functions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 rows...\n",
      "Processed 50,000 rows...\n",
      "Processed 100,000 rows...\n",
      "Processed 150,000 rows...\n",
      "Processed 200,000 rows...\n",
      "\n",
      "Generated demographics for 207,421 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\henry\\AppData\\Local\\Temp\\ipykernel_20156\\1234163315.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_augmented['age'] = ages\n",
      "C:\\Users\\henry\\AppData\\Local\\Temp\\ipykernel_20156\\1234163315.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_augmented['sex'] = sexes\n"
     ]
    }
   ],
   "source": [
    "# Generate demographics for all rows\n",
    "np.random.seed(42)\n",
    "ages = []\n",
    "sexes = []\n",
    "\n",
    "for idx, row in df_augmented.iterrows():\n",
    "    disease = row['diseases']\n",
    "    demo = demographics.get(disease, DEFAULT_DEMO)\n",
    "    \n",
    "    ages.append(sample_age(demo))\n",
    "    sexes.append(sample_sex(demo))\n",
    "    \n",
    "    if idx % 50000 == 0:\n",
    "        print(f\"Processed {idx:,} rows...\")\n",
    "\n",
    "df_augmented['age'] = ages\n",
    "df_augmented['sex'] = sexes\n",
    "\n",
    "print(f\"\\nGenerated demographics for {len(df_augmented):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographics Summary:\n",
      "==================================================\n",
      "Age: min=0, max=99, mean=43.4\n",
      "Sex: 47.2% male, 52.8% female\n",
      "\n",
      "sex\n",
      "F    109440\n",
      "M     97981\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Summary statistics\n",
    "print(\"Demographics Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Age: min={df_augmented['age'].min()}, max={df_augmented['age'].max()}, mean={df_augmented['age'].mean():.1f}\")\n",
    "print(f\"Sex: {(df_augmented['sex'] == 'M').mean() * 100:.1f}% male, {(df_augmented['sex'] == 'F').mean() * 100:.1f}% female\")\n",
    "print(f\"\\n{df_augmented['sex'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification - Sample diseases:\n",
      "================================================================================\n",
      "prostate cancer           Age:  70.2 (exp:  70), Male: 100.0% (exp: 100%), n=135\n",
      "preeclampsia              Age:  30.3 (exp:  30), Male:   0.0% (exp:   0%), n=217\n",
      "migraine                  Age:  32.0 (exp:  30), Male:  30.3% (exp:  30%), n=221\n",
      "pyloric stenosis          Age:   0.0 (exp:   0), Male:  64.0% (exp:  80%), n=25\n",
      "diabetes                  Age:  43.0 (exp:  55), Male: 100.0% (exp:  52%), n=1\n"
     ]
    }
   ],
   "source": [
    "# Verify key diseases\n",
    "print(\"Verification - Sample diseases:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "verify_diseases = [\"prostate cancer\", \"preeclampsia\", \"migraine\", \"pyloric stenosis\", \"diabetes\"]\n",
    "\n",
    "for disease in verify_diseases:\n",
    "    subset = df_augmented[df_augmented['diseases'] == disease]\n",
    "    if len(subset) > 0:\n",
    "        male_pct = (subset['sex'] == 'M').mean() * 100\n",
    "        mean_age = subset['age'].mean()\n",
    "        expected = demographics.get(disease, DEFAULT_DEMO)\n",
    "        \n",
    "        print(f\"{disease:25} Age: {mean_age:5.1f} (exp: {expected.get('age_peak', '?'):>3}), \"\n",
    "              f\"Male: {male_pct:5.1f}% (exp: {expected.get('male_pct', '?'):>3}%), \"\n",
    "              f\"n={len(subset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reordered columns: 379 total\n",
      "First 10: ['diseases', 'disease_category', 'age', 'sex', 'anxiety and nervousness', 'depression', 'shortness of breath', 'depressive or psychotic symptoms', 'sharp chest pain', 'dizziness']\n"
     ]
    }
   ],
   "source": [
    "# Reorder columns: diseases, category, age, sex, then symptoms\n",
    "cols = df_augmented.columns.tolist()\n",
    "\n",
    "# Move key columns to front\n",
    "key_cols = ['diseases', 'disease_category', 'age', 'sex']\n",
    "symptom_cols = [c for c in cols if c not in key_cols + ['symptoms']]\n",
    "final_order = key_cols + symptom_cols + ['symptoms']\n",
    "\n",
    "# Only include columns that exist\n",
    "final_order = [c for c in final_order if c in df_augmented.columns]\n",
    "\n",
    "df_final = df_augmented[final_order]\n",
    "print(f\"Reordered columns: {len(df_final.columns)} total\")\n",
    "print(f\"First 10: {df_final.columns[:10].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dataset WITH demographics:\n",
      "  Path: c:\\Users\\henry\\Desktop\\Programming\\Python\\Multimodal_Diagnosis\\data\\processed\\symptoms\\symptoms_augmented_with_demographics.csv\n",
      "  Size: 157.8 MB\n",
      "  Rows: 207,421\n",
      "  Columns: 379\n"
     ]
    }
   ],
   "source": [
    "# Save dataset WITH demographics\n",
    "df_final.to_csv(output_with_demo_path, index=False)\n",
    "\n",
    "print(f\"Saved dataset WITH demographics:\")\n",
    "print(f\"  Path: {output_with_demo_path}\")\n",
    "print(f\"  Size: {output_with_demo_path.stat().st_size / 1024 / 1024:.1f} MB\")\n",
    "print(f\"  Rows: {len(df_final):,}\")\n",
    "print(f\"  Columns: {len(df_final.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "## Output Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA AUGMENTATION COMPLETE\n",
      "================================================================================\n",
      "\n",
      "1. EXPANDED VOCABULARY:\n",
      "   c:\\Users\\henry\\Desktop\\Programming\\Python\\Multimodal_Diagnosis\\data\\symptom_vocabulary.json\n",
      "   Original: 458 symptoms\n",
      "   Expanded: 458 symptoms (+0)\n",
      "\n",
      "2. DATASET WITHOUT DEMOGRAPHICS:\n",
      "   c:\\Users\\henry\\Desktop\\Programming\\Python\\Multimodal_Diagnosis\\data\\processed\\symptoms\\symptoms_augmented_no_demographics.csv\n",
      "   Rows: 207,421, Columns: 377\n",
      "   Size: 156.8 MB\n",
      "\n",
      "3. DATASET WITH DEMOGRAPHICS:\n",
      "   c:\\Users\\henry\\Desktop\\Programming\\Python\\Multimodal_Diagnosis\\data\\processed\\symptoms\\symptoms_augmented_with_demographics.csv\n",
      "   Rows: 207,421, Columns: 379\n",
      "   Size: 157.8 MB\n",
      "   Includes: age, sex columns\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"DATA AUGMENTATION COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1. EXPANDED VOCABULARY:\")\n",
    "print(f\"   {expanded_vocab_path}\")\n",
    "print(f\"   Original: {len(ORIGINAL_VOCAB)} symptoms\")\n",
    "print(f\"   Expanded: {len(EXPANDED_VOCAB)} symptoms (+{len(symptoms_to_add)})\")\n",
    "\n",
    "print(\"\\n2. DATASET WITHOUT DEMOGRAPHICS:\")\n",
    "print(f\"   {output_no_demo_path}\")\n",
    "if output_no_demo_path.exists():\n",
    "    df_check = pd.read_csv(output_no_demo_path, nrows=1)\n",
    "    print(f\"   Rows: {len(df_augmented):,}, Columns: {len(df_check.columns)}\")\n",
    "    print(f\"   Size: {output_no_demo_path.stat().st_size / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "print(\"\\n3. DATASET WITH DEMOGRAPHICS:\")\n",
    "print(f\"   {output_with_demo_path}\")\n",
    "if output_with_demo_path.exists():\n",
    "    df_check = pd.read_csv(output_with_demo_path, nrows=1)\n",
    "    print(f\"   Rows: {len(df_final):,}, Columns: {len(df_check.columns)}\")\n",
    "    print(f\"   Size: {output_with_demo_path.stat().st_size / 1024 / 1024:.1f} MB\")\n",
    "    print(f\"   Includes: age, sex columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Documentation for Research Paper\n",
    "\n",
    "> **Data Augmentation Pipeline**\n",
    ">\n",
    "> **Stage 0 - Vocabulary Expansion:**\n",
    "> 1. Collected symptom lists from Mayo Clinic and Cleveland Clinic for 135 rare diseases\n",
    "> 2. Identified symptoms appearing in >=5 diseases not in original vocabulary\n",
    "> 3. Expanded vocabulary from 377 to N symptoms (updated in place)\n",
    ">\n",
    "> **Stage 1 - Synthetic Symptom Data:**\n",
    "> 1. Mapped Mayo Clinic symptoms to expanded vocabulary\n",
    "> 2. For diseases with <20 training samples, generated synthetic samples\n",
    "> 3. Each synthetic sample: random 4-8 symptom subset from disease's symptom profile\n",
    "> 4. Increased rare disease representation to minimum 25 samples per disease\n",
    ">\n",
    "> **Stage 2 - Demographic Variables (Age/Sex):**\n",
    "> 1. Collected epidemiological demographics via GPT-4 queries\n",
    "> 2. Applied category-level defaults with keyword-based overrides for sex-specific diseases\n",
    "> 3. Age sampled from triangular distribution (min, peak, max)\n",
    "> 4. Sex sampled from Bernoulli distribution based on disease-specific male percentage\n",
    ">\n",
    "> Two output datasets were created:\n",
    "> - `symptoms_augmented_no_demographics.csv`: For symptom-only models\n",
    "> - `symptoms_augmented_with_demographics.csv`: For multimodal models incorporating age/sex"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
