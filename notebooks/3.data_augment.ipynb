{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation Pipeline v2.0\n",
    "\n",
    "**For Research Paper**: Complete pipeline for augmenting the symptom-disease dataset.\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "| Stage | Description | Output |\n",
    "|-------|-------------|--------|\n",
    "| **0** | Expand symptom vocabulary with Mayo Clinic symptoms | `symptom_columns.json` (updated) |\n",
    "| **1** | Generate synthetic samples for rare diseases (<20 samples) | `symptoms_augmented_no_demographics.csv` |\n",
    "| **2** | Add demographic variables (age, sex) | `symptoms_augmented_with_demographics.csv` |\n",
    "\n",
    "## Requirements\n",
    "- `data/rare_diseases_symptoms_template.json` - Filled with manually added symptoms\n",
    "- `data/final_disease_demographics.json` - Demographics from ChatGPT + synthetic rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\Users\\henry\\Desktop\\Programming\\Python\\Multimodal_Diagnosis\n",
      "\n",
      "Input files:\n",
      "  Data: True - c:\\Users\\henry\\Desktop\\Programming\\Python\\Multimodal_Diagnosis\\data\\processed\\symptoms\\symptoms_to_disease_cleaned.csv\n",
      "  Vocab: True - c:\\Users\\henry\\Desktop\\Programming\\Python\\Multimodal_Diagnosis\\data\\symptom_vocabulary.json\n",
      "  Template: True - c:\\Users\\henry\\Desktop\\Programming\\Python\\Multimodal_Diagnosis\\data\\rare_diseases_symptoms_template.json\n",
      "  Demographics: True - c:\\Users\\henry\\Desktop\\Programming\\Python\\Multimodal_Diagnosis\\data\\final_disease_demographics.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import sys\n",
    "import gc\n",
    "from collections import Counter\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import normalization utilities (consolidated from scripts/symptom_mapper.py)\n",
    "from utils.symptom_normalizer import normalize_symptom, find_similar_symptoms\n",
    "from utils.consts import SYNONYM_MAP, NON_SYMPTOM_COLS\n",
    "\n",
    "# Paths\n",
    "# Input files\n",
    "data_path = project_root / \"data\" / \"processed\" / \"symptoms\" / \"symptoms_to_disease_cleaned.csv\"\n",
    "symptom_cols_path = project_root / \"data\" / \"symptom_vocabulary.json\"\n",
    "template_path = project_root / \"data\" / \"rare_diseases_symptoms_template.json\"\n",
    "category_map_path = project_root / \"data\" / \"disease_mapping.json\"\n",
    "demographics_path = project_root / \"data\" / \"final_disease_demographics.json\"\n",
    "\n",
    "# Output files - vocabulary saved to SAME file (overwrites original)\n",
    "expanded_vocab_path = symptom_cols_path  # Overwrites original\n",
    "output_no_demo_path = project_root / \"data\" / \"processed\" / \"symptoms\" / \"symptoms_augmented_no_demographics.csv\"\n",
    "output_with_demo_path = project_root / \"data\" / \"processed\" / \"symptoms\" / \"symptoms_augmented_with_demographics.csv\"\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"\\nInput files:\")\n",
    "print(f\"  Data: {data_path.exists()} - {data_path}\")\n",
    "print(f\"  Vocab: {symptom_cols_path.exists()} - {symptom_cols_path}\")\n",
    "print(f\"  Template: {template_path.exists()} - {template_path}\")\n",
    "print(f\"  Demographics: {demographics_path.exists()} - {demographics_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Stage 0: Expand Symptom Vocabulary\n",
    "\n",
    "The current vocabulary has 377 symptoms. Many manual symptoms don't have exact matches.\n",
    "\n",
    "**Strategy**: Add clinically important new symptoms to the vocabulary (appearing in 5+ diseases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vocabulary: 456 symptoms\n",
      "Diseases in template: 135\n"
     ]
    }
   ],
   "source": [
    "# Load current vocabulary\n",
    "with open(symptom_cols_path) as f:\n",
    "    ORIGINAL_VOCAB = json.load(f)\n",
    "ORIGINAL_SET = set(s.lower() for s in ORIGINAL_VOCAB)\n",
    "\n",
    "print(f\"Original vocabulary: {len(ORIGINAL_VOCAB)} symptoms\")\n",
    "\n",
    "# Load template with Mayo Clinic symptoms\n",
    "with open(template_path) as f:\n",
    "    template = json.load(f)\n",
    "\n",
    "print(f\"Diseases in template: {len(template)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting and normalizing symptoms from template...\n",
      "Total unique mayo symptoms: 690\n",
      "New symptoms (not in vocabulary): 526\n"
     ]
    }
   ],
   "source": [
    "# Extract all unique symptoms from template\n",
    "all_mayo_symptoms = set()\n",
    "symptom_counts = Counter()\n",
    "\n",
    "print(\"Extracting and normalizing symptoms from template...\")\n",
    "for disease, info in template.items():\n",
    "    mayo = info.get(\"mayo_clinic_symptoms\", [])\n",
    "    for sym in mayo:\n",
    "        # Normalize symptom using project specific normalizer\n",
    "        sym_norm = normalize_symptom(sym)\n",
    "        \n",
    "        if sym_norm and not sym_norm.startswith('\"notes\"'):\n",
    "            all_mayo_symptoms.add(sym_norm)\n",
    "            symptom_counts[sym_norm] += 1\n",
    "\n",
    "print(f\"Total unique mayo symptoms: {len(all_mayo_symptoms)}\")\n",
    "\n",
    "# Find symptoms NOT in current vocabulary\n",
    "new_symptoms = [s for s in all_mayo_symptoms if s not in ORIGINAL_SET]\n",
    "print(f\"New symptoms (not in vocabulary): {len(new_symptoms)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common new symptoms (top 50):\n",
      "------------------------------------------------------------\n",
      "  [ 2x] low-set ears\n",
      "  [ 1x] cervical insufficiency\n",
      "  [ 1x] general feeling of discomfort\n",
      "  [ 1x] congenital heart disease\n",
      "  [ 1x] foul-smelling sputum\n",
      "  [ 1x] irritable\n",
      "  [ 1x] sleep problems in infants\n",
      "  [ 1x] weight gain in the face\n",
      "  [ 1x] extra fuild behind a fetus' next\n",
      "  [ 1x] shivering\n",
      "  [ 1x] disseminated sporotrichosis\n",
      "  [ 1x] sores on thighs\n",
      "  [ 1x] trouble processing information\n",
      "  [ 1x] skin changes in color on vulva\n",
      "  [ 1x] pain when talking\n",
      "  [ 1x] soreness\n",
      "  [ 1x] weakened tooth enamel\n",
      "  [ 1x] reduced mental sharpness\n",
      "  [ 1x] stiff neck\n",
      "  [ 1x] rough scaly skin\n",
      "  [ 1x] sudden abdominal pain\n",
      "  [ 1x] blisters with yellowish fluid\n",
      "  [ 1x] spasms\n",
      "  [ 1x] cold clammy skin\n",
      "  [ 1x] swelling of the legs\n",
      "  [ 1x] eye watering\n",
      "  [ 1x] rectal prolapse\n",
      "  [ 1x] itchiness\n",
      "  [ 1x] pain on vulva\n",
      "  [ 1x] gurgling noise at back of the throat\n",
      "  [ 1x] changes in voice\n",
      "  [ 1x] night blindness\n",
      "  [ 1x] involuntary movements\n",
      "  [ 1x] abnormal spots on skin\n",
      "  [ 1x] leaking urine\n",
      "  [ 1x] coughing\n",
      "  [ 1x] dark lines on nail\n",
      "  [ 1x] dull ache in the lower belly or groin\n",
      "  [ 1x] changes in skin texture\n",
      "  [ 1x] lumpy bruises\n",
      "  [ 1x] infections of the vagina\n",
      "  [ 1x] swelling in feet\n",
      "  [ 1x] small placenta\n",
      "  [ 1x] hearing loss in both ears\n",
      "  [ 1x] discoloration\n",
      "  [ 1x] foot deformities\n",
      "  [ 1x] blisters on chest\n",
      "  [ 1x] leg pain while walking\n",
      "  [ 1x] urinary retention\n",
      "  [ 1x] protein in urine\n"
     ]
    }
   ],
   "source": [
    "# Show most common new symptoms\n",
    "new_symptom_counts = [(s, symptom_counts[s]) for s in new_symptoms]\n",
    "new_symptom_counts.sort(key=lambda x: -x[1])\n",
    "\n",
    "print(\"Most common new symptoms (top 50):\")\n",
    "print(\"-\" * 60)\n",
    "for sym, count in new_symptom_counts[:50]:\n",
    "    print(f\"  [{count:2d}x] {sym}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New symptoms found in Mayo Clinic data: 526\n",
      "All mapped symptoms will be added as columns later\n",
      "\n",
      "Top 20 new symptoms by frequency:\n",
      "------------------------------------------------------------\n",
      "  [ 2x] low-set ears\n",
      "  [ 1x] cervical insufficiency\n",
      "  [ 1x] general feeling of discomfort\n",
      "  [ 1x] congenital heart disease\n",
      "  [ 1x] foul-smelling sputum\n",
      "  [ 1x] irritable\n",
      "  [ 1x] sleep problems in infants\n",
      "  [ 1x] weight gain in the face\n",
      "  [ 1x] extra fuild behind a fetus' next\n",
      "  [ 1x] shivering\n",
      "  [ 1x] disseminated sporotrichosis\n",
      "  [ 1x] sores on thighs\n",
      "  [ 1x] trouble processing information\n",
      "  [ 1x] skin changes in color on vulva\n",
      "  [ 1x] pain when talking\n",
      "  [ 1x] soreness\n",
      "  [ 1x] weakened tooth enamel\n",
      "  [ 1x] reduced mental sharpness\n",
      "  [ 1x] stiff neck\n",
      "  [ 1x] rough scaly skin\n"
     ]
    }
   ],
   "source": [
    "# NOTE: This cell shows statistics about new symptoms found in Mayo Clinic data\n",
    "# The actual column expansion happens AFTER symptom mapping (see later cell)\n",
    "# We no longer filter by MIN_DISEASE_COUNT - ALL mapped symptoms are used\n",
    "\n",
    "# Show statistics about new symptoms (informational only)\n",
    "new_symptom_counts = [(s, symptom_counts[s]) for s in new_symptoms]\n",
    "new_symptom_counts.sort(key=lambda x: -x[1])\n",
    "\n",
    "print(f\"New symptoms found in Mayo Clinic data: {len(new_symptoms)}\")\n",
    "print(\"All mapped symptoms will be added as columns later\")\n",
    "print(\"\\nTop 20 new symptoms by frequency:\")\n",
    "print(\"-\" * 60)\n",
    "for sym, count in new_symptom_counts[:20]:\n",
    "    print(f\"  [{count:2d}x] {sym}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting vocabulary: 456 symptoms\n",
      "Additional Mayo Clinic symptoms will be added after mapping\n"
     ]
    }
   ],
   "source": [
    "# Create expanded vocabulary\n",
    "# NOTE: We start with ORIGINAL_VOCAB, but additional columns will be added\n",
    "# dynamically during symptom mapping (see later cell for column expansion)\n",
    "EXPANDED_VOCAB = list(ORIGINAL_VOCAB)  # Start with original\n",
    "EXPANDED_SET = set(s.lower() for s in EXPANDED_VOCAB)\n",
    "\n",
    "print(f\"Starting vocabulary: {len(EXPANDED_VOCAB)} symptoms\")\n",
    "print(\"Additional Mayo Clinic symptoms will be added after mapping\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Stage 1: Symptom Mapping & Synthetic Data Generation\n",
    "\n",
    "1. Map Mayo Clinic symptoms to expanded vocabulary\n",
    "2. Generate synthetic samples for diseases with <20 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset: 206,267 rows, 627 diseases\n",
      "\n",
      "Diseases with <20 samples: 128\n",
      "Total samples in rare diseases: 1,085\n"
     ]
    }
   ],
   "source": [
    "# Load original dataset\n",
    "df = pd.read_csv(data_path)\n",
    "print(f\"Original dataset: {len(df):,} rows, {df['diseases'].nunique()} diseases\")\n",
    "\n",
    "# Get disease counts\n",
    "counts = df['diseases'].value_counts()\n",
    "rare_diseases = counts[counts < 20]\n",
    "print(f\"\\nDiseases with <20 samples: {len(rare_diseases)}\")\n",
    "print(f\"Total samples in rare diseases: {rare_diseases.sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 599 disease -> category mappings\n"
     ]
    }
   ],
   "source": [
    "# Load category mapping\n",
    "with open(category_map_path) as f:\n",
    "    category_map = json.load(f)\n",
    "\n",
    "# Create disease -> category lookup\n",
    "disease_to_category = {}\n",
    "for cat, diseases in category_map.items():\n",
    "    for d in diseases:\n",
    "        disease_to_category[d] = cat\n",
    "\n",
    "print(f\"Loaded {len(disease_to_category)} disease -> category mappings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined smart mapping functions (integrated from symptom_mapper.py)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SMART SYMPTOM MAPPING (Integrated from scripts/symptom_mapper.py)\n",
    "# Uses normalization + fuzzy matching for better mapping coverage\n",
    "# ============================================================================\n",
    "\n",
    "def smart_map_symptom(external_symptom: str, vocab_set: set) -> str | None:\n",
    "    \"\"\"\n",
    "    Map an external symptom to vocabulary using normalization + fuzzy matching.\n",
    "    Returns the matched symptom or None if no match found.\n",
    "    \"\"\"\n",
    "    # Step 1: Normalize with synonyms (e.g., 'belly pain' -> 'abdominal pain')\n",
    "    normalized = normalize_symptom(external_symptom, apply_synonyms=True)\n",
    "    if normalized in vocab_set:\n",
    "        return normalized\n",
    "    \n",
    "    # Step 2: Try without synonym mapping (in case original is better)\n",
    "    normalized_raw = normalize_symptom(external_symptom, apply_synonyms=False)\n",
    "    if normalized_raw in vocab_set:\n",
    "        return normalized_raw\n",
    "    \n",
    "    # Step 3: Fuzzy match (only for high-confidence matches >= 85%)\n",
    "    matches = find_similar_symptoms(external_symptom, list(vocab_set), threshold=0.85)\n",
    "    if matches and matches[0][1] >= 0.85:\n",
    "        return matches[0][0]\n",
    "    \n",
    "    return None  # No match found\n",
    "\n",
    "\n",
    "def map_symptoms_to_vocab(symptoms_list, vocab_set):\n",
    "    \"\"\"\n",
    "    Map a list of symptoms to the vocabulary using smart matching.\n",
    "    Returns list of symptoms that matched.\n",
    "    \"\"\"\n",
    "    mapped = []\n",
    "    for sym in symptoms_list:\n",
    "        result = smart_map_symptom(sym, vocab_set)\n",
    "        if result:\n",
    "            mapped.append(result)\n",
    "    return list(set(mapped))  # Remove duplicates\n",
    "\n",
    "\n",
    "def generate_synthetic_samples(disease: str, symptoms: list, n_samples: int,\n",
    "                               all_symptom_cols: list, min_sym: int = 4, max_sym: int = 8) -> list:\n",
    "    \"\"\"\n",
    "    Generate synthetic samples for a disease.\n",
    "    Each sample has random 4-8 symptoms selected from the symptom list.\n",
    "    NOTE: all_symptom_cols should be the DataFrame columns (includes new Mayo cols).\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    category = disease_to_category.get(disease, \"Unknown Type\")\n",
    "    all_symptom_set = set(all_symptom_cols)\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        # Select random symptoms\n",
    "        n_sym = random.randint(min_sym, min(max_sym, len(symptoms)))\n",
    "        selected = random.sample(symptoms, n_sym)\n",
    "        \n",
    "        # Create row with all symptoms as 0\n",
    "        row = {col: 0 for col in all_symptom_cols}\n",
    "        \n",
    "        # Set selected symptoms to 1 (only if column exists)\n",
    "        for sym in selected:\n",
    "            if sym in all_symptom_set:\n",
    "                row[sym] = 1\n",
    "        \n",
    "        row['diseases'] = disease\n",
    "        row['disease_category'] = category\n",
    "        row['symptoms'] = \", \".join(selected)\n",
    "        \n",
    "        samples.append(row)\n",
    "    \n",
    "    return samples\n",
    "\n",
    "print(\"Defined smart mapping functions (integrated from symptom_mapper.py)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symptom mapping results:\n",
      "  Total mayo symptoms: 1255\n",
      "  Mapped to vocabulary: 716 (57.1%)\n",
      "  Diseases ready for synthesis (>=4 symptoms): 81\n"
     ]
    }
   ],
   "source": [
    "# Map symptoms for each disease in template\n",
    "disease_mapped_symptoms = {}\n",
    "mapping_stats = {'total': 0, 'mapped': 0, 'diseases_ready': 0}\n",
    "\n",
    "for disease, info in template.items():\n",
    "    mayo = info.get(\"mayo_clinic_symptoms\", [])\n",
    "    if not mayo:\n",
    "        continue\n",
    "    \n",
    "    mapped = map_symptoms_to_vocab(mayo, EXPANDED_SET)\n",
    "    mapping_stats['total'] += len(mayo)\n",
    "    mapping_stats['mapped'] += len(mapped)\n",
    "    \n",
    "    if len(mapped) >= 4:  # Minimum for synthetic generation\n",
    "        disease_mapped_symptoms[disease] = mapped\n",
    "        mapping_stats['diseases_ready'] += 1\n",
    "\n",
    "print(f\"Symptom mapping results:\")\n",
    "print(f\"  Total mayo symptoms: {mapping_stats['total']}\")\n",
    "print(f\"  Mapped to vocabulary: {mapping_stats['mapped']} ({100*mapping_stats['mapped']/mapping_stats['total']:.1f}%)\")\n",
    "print(f\"  Diseases ready for synthesis (>=4 symptoms): {mapping_stats['diseases_ready']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique mapped symptoms from Mayo Clinic: 157\n",
      "Adding 81 new symptom columns from Mayo Clinic data...\n",
      "  Sample new columns: ['gas', 'brittle nails', 'swelling', 'sleep problems', 'heart palpitations', 'balance problems', 'rapid heartbeat', 'enlarged liver', 'vaginal bleeding', 'poor growth']...\n",
      "Expanded dataset now has 458 columns\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FIX: Add ALL mapped Mayo symptoms as columns\n",
    "# This ensures synthetic samples have proper columns for their symptoms\n",
    "# ============================================================================\n",
    "\n",
    "# Collect ALL symptoms that were successfully mapped from Mayo Clinic data\n",
    "all_mapped_symptoms = set()\n",
    "base_disease_set = set(df['diseases'].unique())\n",
    "\n",
    "for disease, symptoms in disease_mapped_symptoms.items():\n",
    "    # Only add diseases that exist in base data\n",
    "    if disease not in base_disease_set:\n",
    "        continue\n",
    "    all_mapped_symptoms.update(symptoms)\n",
    "\n",
    "print(f\"Total unique mapped symptoms from Mayo Clinic: {len(all_mapped_symptoms)}\")\n",
    "\n",
    "# Find symptoms that need to be added as new columns\n",
    "existing_cols = set(c.lower() for c in df.columns)\n",
    "new_cols_to_add = [sym for sym in all_mapped_symptoms if sym.lower() not in existing_cols]\n",
    "\n",
    "if new_cols_to_add:\n",
    "    print(f\"Adding {len(new_cols_to_add)} new symptom columns from Mayo Clinic data...\")\n",
    "    # Create a separate DataFrame for new columns (int8 for memory efficiency)\n",
    "    new_data = pd.DataFrame(0, index=df.index, columns=new_cols_to_add, dtype='int8')\n",
    "    \n",
    "    # Concatenate once (avoids fragmentation)\n",
    "    df = pd.concat([df, new_data], axis=1)\n",
    "    if len(new_cols_to_add) > 10:\n",
    "        print(f\"  Sample new columns: {new_cols_to_add[:10]}...\")\n",
    "    else:\n",
    "        print(f\"  Added: {new_cols_to_add}\")\n",
    "else:\n",
    "    print(\"No new columns to add (all mapped symptoms already exist).\")\n",
    "\n",
    "print(f\"Expanded dataset now has {len(df.columns)} columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base dataset has 627 unique diseases\n",
      "Using 456 symptom columns for synthetic generation\n",
      "\n",
      "Generated 1,251 synthetic samples for 78 diseases\n",
      "Skipped 3 diseases not in base data: ['poisoning due to antipsychotics', 'hypothermia', 'poisoning due to antihypertensives']\n",
      "\n",
      "Generation details:\n",
      "  rocky mountain spotted fever: 1 -> 25 (+24, 13 symptoms available)\n",
      "  myocarditis: 1 -> 25 (+24, 9 symptoms available)\n",
      "  kaposi sarcoma: 1 -> 25 (+24, 8 symptoms available)\n",
      "  chronic ulcer: 1 -> 25 (+24, 7 symptoms available)\n",
      "  diabetes: 1 -> 25 (+24, 6 symptoms available)\n",
      "  gas gangrene: 1 -> 25 (+24, 6 symptoms available)\n",
      "  thalassemia: 1 -> 25 (+24, 6 symptoms available)\n",
      "  typhoid fever: 1 -> 25 (+24, 8 symptoms available)\n",
      "  diabetic kidney disease: 2 -> 25 (+23, 9 symptoms available)\n",
      "  rheumatic fever: 2 -> 25 (+23, 5 symptoms available)\n",
      "  human immunodeficiency virus infection (hiv): 2 -> 25 (+23, 10 symptoms available)\n",
      "  hashimoto thyroiditis: 2 -> 25 (+23, 10 symptoms available)\n",
      "  carcinoid syndrome: 2 -> 25 (+23, 4 symptoms available)\n",
      "  sporotrichosis: 3 -> 25 (+22, 7 symptoms available)\n",
      "  cat scratch disease: 3 -> 25 (+22, 7 symptoms available)\n",
      "  dengue fever: 3 -> 25 (+22, 8 symptoms available)\n",
      "  adrenal cancer: 3 -> 25 (+22, 8 symptoms available)\n",
      "  necrotizing fasciitis: 3 -> 25 (+22, 5 symptoms available)\n",
      "  connective tissue disorder: 3 -> 25 (+22, 7 symptoms available)\n",
      "  myelodysplastic syndrome: 3 -> 25 (+22, 5 symptoms available)\n",
      "  ... and 58 more diseases\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic samples\n",
    "# IMPORTANT: Only augment diseases that EXIST in the base cleaned dataset\n",
    "# This prevents adding back diseases that were intentionally excluded\n",
    "random.seed(42)\n",
    "TARGET_SAMPLES = 25  # Minimum samples per disease\n",
    "\n",
    "# Get set of diseases in base data (for filtering)\n",
    "base_disease_set = set(df['diseases'].unique())\n",
    "print(f\"Base dataset has {len(base_disease_set)} unique diseases\")\n",
    "\n",
    "# Get symptom columns from expanded df (excluding non-symptom cols)\n",
    "symptom_cols = [c for c in df.columns if c.lower() not in {s.lower() for s in NON_SYMPTOM_COLS}]\n",
    "print(f\"Using {len(symptom_cols)} symptom columns for synthetic generation\")\n",
    "\n",
    "all_synthetic = []\n",
    "generation_log = []\n",
    "skipped_diseases = []\n",
    "\n",
    "for disease, symptoms in disease_mapped_symptoms.items():\n",
    "    # FILTER: Only augment diseases that exist in base data\n",
    "    if disease not in base_disease_set:\n",
    "        skipped_diseases.append(disease)\n",
    "        continue\n",
    "    \n",
    "    current_count = counts.get(disease, 0)\n",
    "    \n",
    "    if current_count >= TARGET_SAMPLES:\n",
    "        continue\n",
    "    \n",
    "    n_new = TARGET_SAMPLES - current_count\n",
    "    samples = generate_synthetic_samples(disease, symptoms, n_new, symptom_cols)\n",
    "    all_synthetic.extend(samples)\n",
    "    \n",
    "    generation_log.append({\n",
    "        'disease': disease,\n",
    "        'original': current_count,\n",
    "        'added': n_new,\n",
    "        'symptoms_available': len(symptoms)\n",
    "    })\n",
    "\n",
    "print(f\"\\nGenerated {len(all_synthetic):,} synthetic samples for {len(generation_log)} diseases\")\n",
    "if skipped_diseases:\n",
    "    print(f\"Skipped {len(skipped_diseases)} diseases not in base data: {skipped_diseases}\")\n",
    "print(\"\\nGeneration details:\")\n",
    "for log in generation_log[:20]:\n",
    "    print(f\"  {log['disease']}: {log['original']} -> {log['original'] + log['added']} (+{log['added']}, {log['symptoms_available']} symptoms available)\")\n",
    "if len(generation_log) > 20:\n",
    "    print(f\"  ... and {len(generation_log) - 20} more diseases\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original samples: 206,267\n",
      "Synthetic samples: 1,251\n",
      "Total augmented: 207,518\n",
      "Memory cleanup: deleted original df\n"
     ]
    }
   ],
   "source": [
    "# Combine original + synthetic\n",
    "if all_synthetic:\n",
    "    df_synthetic = pd.DataFrame(all_synthetic)\n",
    "    \n",
    "    # Ensure synthetic dataframe has all columns\n",
    "    # Optimization: Use reindex which is faster/cleaner\n",
    "    df_synthetic = df_synthetic.reindex(columns=df.columns, fill_value=0).astype(df.dtypes)\n",
    "    \n",
    "    # Concatenate\n",
    "    df_augmented = pd.concat([df, df_synthetic], ignore_index=True)\n",
    "    \n",
    "    print(f\"Original samples: {len(df):,}\")\n",
    "    print(f\"Synthetic samples: {len(df_synthetic):,}\")\n",
    "    print(f\"Total augmented: {len(df_augmented):,}\")\n",
    "    \n",
    "    # cleanup\n",
    "    del df_synthetic\n",
    "else:\n",
    "    df_augmented = df\n",
    "    print(\"No synthetic samples generated\")\n",
    "\n",
    "# Free up memory\n",
    "del df\n",
    "gc.collect()\n",
    "print(\"Memory cleanup: deleted original df\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before augmentation: 128 diseases with <20 samples\n",
      "After augmentation: 50 diseases with <20 samples\n",
      "\n",
      "Diseases still below 20 samples:\n",
      "  19  otosclerosis\n",
      "  18  cyst of the eyelid\n",
      "  14  pneumoconiosis\n",
      "  14  fibrocystic breast disease\n",
      "  13  factitious disorder\n",
      "  13  hpv\n",
      "  13  congenital malformation syndrome\n",
      "  12  raynaud disease\n",
      "  11  galactorrhea of unknown cause\n",
      "  11  zenker diverticulum\n",
      "  11  myoclonus\n",
      "  10  avascular necrosis\n",
      "  10  granuloma inguinale\n",
      "  10  optic neuritis\n",
      "  10  testicular cancer\n",
      "  10  decubitus ulcer\n",
      "  10  vesicoureteral reflux\n",
      "   9  vacterl syndrome\n",
      "   9  aphakia\n",
      "   9  lichen planus\n",
      "   9  thyroid cancer\n",
      "   9  hemarthrosis\n",
      "   9  placenta previa\n",
      "   8  hypercholesterolemia\n",
      "   8  spinocerebellar ataxia\n",
      "   7  pemphigus\n",
      "   7  omphalitis\n",
      "   6  blepharospasm\n",
      "   6  breast cancer\n",
      "   6  tuberous sclerosis\n",
      "   6  g6pd enzyme deficiency\n",
      "   6  edward syndrome\n",
      "   6  vulvar cancer\n",
      "   6  priapism\n",
      "   5  uterine cancer\n",
      "   5  vitamin a deficiency\n",
      "   5  pelvic fistula\n",
      "   5  spherocytosis\n",
      "   5  cryptorchidism\n",
      "   4  open wound of the hand\n",
      "   4  intussusception\n",
      "   4  oral leukoplakia\n",
      "   3  esophageal varices\n",
      "   3  obesity\n",
      "   3  hyperlipidemia\n",
      "   1  turner syndrome\n",
      "   1  foreign body in the nose\n",
      "   1  high blood pressure\n",
      "   1  huntington disease\n",
      "   1  hypergammaglobulinemia\n"
     ]
    }
   ],
   "source": [
    "# Verify rare disease counts improved\n",
    "new_counts = df_augmented['diseases'].value_counts()\n",
    "new_rare = new_counts[new_counts < 20]\n",
    "\n",
    "print(f\"Before augmentation: {len(rare_diseases)} diseases with <20 samples\")\n",
    "print(f\"After augmentation: {len(new_rare)} diseases with <20 samples\")\n",
    "print(f\"\\nDiseases still below 20 samples:\")\n",
    "for d, c in new_rare.items():\n",
    "    print(f\"  {c:2d}  {d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dataset WITHOUT demographics:\n",
      "  Path: c:\\Users\\henry\\Desktop\\Programming\\Python\\Multimodal_Diagnosis\\data\\processed\\symptoms\\symptoms_augmented_no_demographics.csv\n",
      "  Size: 189.1 MB\n",
      "  Rows: 207,518\n",
      "  Columns: 458\n"
     ]
    }
   ],
   "source": [
    "# Save dataset WITHOUT demographics\n",
    "df_augmented.to_csv(output_no_demo_path, index=False)\n",
    "\n",
    "print(f\"Saved dataset WITHOUT demographics:\")\n",
    "print(f\"  Path: {output_no_demo_path}\")\n",
    "print(f\"  Size: {output_no_demo_path.stat().st_size / 1024 / 1024:.1f} MB\")\n",
    "print(f\"  Rows: {len(df_augmented):,}\")\n",
    "print(f\"  Columns: {len(df_augmented.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Stage 2: Add Demographics (Age, Sex)\n",
    "\n",
    "Using merged demographics from ChatGPT + synthetic rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded demographics for 667 diseases\n",
      "\n",
      "Demographic coverage:\n",
      "  Total diseases in dataset: 627\n",
      "  Covered by demographics: 620 (98.9%)\n",
      "  Missing (will use defaults): 7\n"
     ]
    }
   ],
   "source": [
    "# Load demographics\n",
    "with open(demographics_path) as f:\n",
    "    demographics = json.load(f)\n",
    "\n",
    "print(f\"Loaded demographics for {len(demographics)} diseases\")\n",
    "\n",
    "# Check coverage\n",
    "all_diseases = set(df_augmented['diseases'].unique())\n",
    "demo_diseases = set(demographics.keys())\n",
    "covered = all_diseases & demo_diseases\n",
    "missing = all_diseases - demo_diseases\n",
    "\n",
    "print(f\"\\nDemographic coverage:\")\n",
    "print(f\"  Total diseases in dataset: {len(all_diseases)}\")\n",
    "print(f\"  Covered by demographics: {len(covered)} ({100*len(covered)/len(all_diseases):.1f}%)\")\n",
    "print(f\"  Missing (will use defaults): {len(missing)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined demographic sampling functions\n"
     ]
    }
   ],
   "source": [
    "# Default demographics for missing diseases\n",
    "DEFAULT_DEMO = {\n",
    "    \"age_min\": 10,\n",
    "    \"age_max\": 80,\n",
    "    \"age_peak\": 45,\n",
    "    \"male_pct\": 50\n",
    "}\n",
    "\n",
    "def sample_age(demo: dict) -> int:\n",
    "    \"\"\"Sample age from triangular distribution.\"\"\"\n",
    "    age_min = demo.get('age_min', 10)\n",
    "    age_max = demo.get('age_max', 80)\n",
    "    age_peak = demo.get('age_peak', 45)\n",
    "    \n",
    "    # Handle edge cases\n",
    "    if age_min == age_max:\n",
    "        return int(age_min)\n",
    "    \n",
    "    age_peak = max(age_min, min(age_peak, age_max))\n",
    "    \n",
    "    if age_min == age_peak or age_peak == age_max:\n",
    "        age = np.random.uniform(age_min, age_max)\n",
    "    else:\n",
    "        age = np.random.triangular(age_min, age_peak, age_max)\n",
    "    \n",
    "    return int(np.clip(age, 0, 100))\n",
    "\n",
    "\n",
    "def sample_sex(demo: dict) -> str:\n",
    "    \"\"\"Sample sex from Bernoulli distribution.\"\"\"\n",
    "    male_pct = demo.get('male_pct', 50)\n",
    "    return 'M' if np.random.random() * 100 < male_pct else 'F'\n",
    "\n",
    "print(\"Defined demographic sampling functions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 rows...\n",
      "Processed 50,000 rows...\n",
      "Processed 100,000 rows...\n",
      "Processed 150,000 rows...\n",
      "Processed 200,000 rows...\n",
      "\n",
      "Generated demographics for 207,518 rows\n"
     ]
    }
   ],
   "source": [
    "# Generate demographics for all rows\n",
    "np.random.seed(42)\n",
    "ages = []\n",
    "sexes = []\n",
    "\n",
    "for idx, row in df_augmented.iterrows():\n",
    "    disease = row['diseases']\n",
    "    demo = demographics.get(disease, DEFAULT_DEMO)\n",
    "    \n",
    "    ages.append(sample_age(demo))\n",
    "    sexes.append(sample_sex(demo))\n",
    "    \n",
    "    if idx % 50000 == 0:\n",
    "        print(f\"Processed {idx:,} rows...\")\n",
    "\n",
    "df_augmented = df_augmented.assign(\n",
    "    age=ages,\n",
    "    sex=sexes\n",
    ")\n",
    "\n",
    "print(f\"\\nGenerated demographics for {len(df_augmented):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographics Summary:\n",
      "==================================================\n",
      "Age: min=0, max=99, mean=43.4\n",
      "Sex: 47.2% male, 52.8% female\n",
      "\n",
      "sex\n",
      "F    109472\n",
      "M     98046\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Summary statistics\n",
    "print(\"Demographics Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Age: min={df_augmented['age'].min()}, max={df_augmented['age'].max()}, mean={df_augmented['age'].mean():.1f}\")\n",
    "print(f\"Sex: {(df_augmented['sex'] == 'M').mean() * 100:.1f}% male, {(df_augmented['sex'] == 'F').mean() * 100:.1f}% female\")\n",
    "print(f\"\\n{df_augmented['sex'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification - Sample diseases:\n",
      "================================================================================\n",
      "prostate cancer           Age:  70.2 (exp:  70), Male: 100.0% (exp: 100%), n=135\n",
      "preeclampsia              Age:  30.3 (exp:  30), Male:   0.0% (exp:   0%), n=217\n",
      "migraine                  Age:  32.0 (exp:  30), Male:  30.3% (exp:  30%), n=221\n",
      "pyloric stenosis          Age:   0.0 (exp:   0), Male:  76.0% (exp:  80%), n=25\n",
      "diabetes                  Age:  58.6 (exp:  55), Male:  80.0% (exp:  52%), n=25\n"
     ]
    }
   ],
   "source": [
    "# Verify key diseases\n",
    "print(\"Verification - Sample diseases:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "verify_diseases = [\"prostate cancer\", \"preeclampsia\", \"migraine\", \"pyloric stenosis\", \"diabetes\"]\n",
    "\n",
    "for disease in verify_diseases:\n",
    "    subset = df_augmented[df_augmented['diseases'] == disease]\n",
    "    if len(subset) > 0:\n",
    "        male_pct = (subset['sex'] == 'M').mean() * 100\n",
    "        mean_age = subset['age'].mean()\n",
    "        expected = demographics.get(disease, DEFAULT_DEMO)\n",
    "        \n",
    "        print(f\"{disease:25} Age: {mean_age:5.1f} (exp: {expected.get('age_peak', '?'):>3}), \"\n",
    "              f\"Male: {male_pct:5.1f}% (exp: {expected.get('male_pct', '?'):>3}%), \"\n",
    "              f\"n={len(subset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reordered columns: 460 total\n",
      "First 10: ['diseases', 'disease_category', 'age', 'sex', 'anxiety and nervousness', 'depression', 'shortness of breath', 'depressive or psychotic symptoms', 'sharp chest pain', 'dizziness']\n"
     ]
    }
   ],
   "source": [
    "# Reorder columns: diseases, category, age, sex, then symptoms\n",
    "cols = df_augmented.columns.tolist()\n",
    "\n",
    "# Move key columns to front\n",
    "key_cols = ['diseases', 'disease_category', 'age', 'sex']\n",
    "symptom_cols = [c for c in cols if c not in key_cols + ['symptoms']]\n",
    "final_order = key_cols + symptom_cols + ['symptoms']\n",
    "\n",
    "# Only include columns that exist\n",
    "final_order = [c for c in final_order if c in df_augmented.columns]\n",
    "\n",
    "df_final = df_augmented[final_order]\n",
    "print(f\"Reordered columns: {len(df_final.columns)} total\")\n",
    "print(f\"First 10: {df_final.columns[:10].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dataset WITH demographics:\n",
      "  Path: c:\\Users\\henry\\Desktop\\Programming\\Python\\Multimodal_Diagnosis\\data\\processed\\symptoms\\symptoms_augmented_with_demographics.csv\n",
      "  Size: 190.1 MB\n",
      "  Rows: 207,518\n",
      "  Columns: 460\n"
     ]
    }
   ],
   "source": [
    "# Save dataset WITH demographics\n",
    "df_final.to_csv(output_with_demo_path, index=False)\n",
    "\n",
    "print(f\"Saved dataset WITH demographics:\")\n",
    "print(f\"  Path: {output_with_demo_path}\")\n",
    "print(f\"  Size: {output_with_demo_path.stat().st_size / 1024 / 1024:.1f} MB\")\n",
    "print(f\"  Rows: {len(df_final):,}\")\n",
    "print(f\"  Columns: {len(df_final.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "## Output Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA AUGMENTATION COMPLETE\n",
      "================================================================================\n",
      "\n",
      "1. VOCABULARY:\n",
      "   c:\\Users\\henry\\Desktop\\Programming\\Python\\Multimodal_Diagnosis\\data\\symptom_vocabulary.json\n",
      "   Original: 456 symptoms\n",
      "   Final dataset: 456 symptom columns\n",
      "\n",
      "2. DATASET WITHOUT DEMOGRAPHICS:\n",
      "   c:\\Users\\henry\\Desktop\\Programming\\Python\\Multimodal_Diagnosis\\data\\processed\\symptoms\\symptoms_augmented_no_demographics.csv\n",
      "   Rows: 207,518, Columns: 458\n",
      "   Size: 189.1 MB\n",
      "\n",
      "3. DATASET WITH DEMOGRAPHICS:\n",
      "   c:\\Users\\henry\\Desktop\\Programming\\Python\\Multimodal_Diagnosis\\data\\processed\\symptoms\\symptoms_augmented_with_demographics.csv\n",
      "   Rows: 207,518, Columns: 460\n",
      "   Size: 190.1 MB\n",
      "   Includes: age, sex columns\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"DATA AUGMENTATION COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Count actual symptom columns in final dataset\n",
    "final_symptom_count = len([c for c in df_final.columns \n",
    "                           if c.lower() not in {s.lower() for s in NON_SYMPTOM_COLS}\n",
    "                           and c != \"symptoms\"])\n",
    "\n",
    "print(\"\\n1. VOCABULARY:\")\n",
    "print(f\"   {symptom_cols_path}\")\n",
    "print(f\"   Original: {len(ORIGINAL_VOCAB)} symptoms\")\n",
    "print(f\"   Final dataset: {final_symptom_count} symptom columns\")\n",
    "\n",
    "print(\"\\n2. DATASET WITHOUT DEMOGRAPHICS:\")\n",
    "print(f\"   {output_no_demo_path}\")\n",
    "if output_no_demo_path.exists():\n",
    "    df_check = pd.read_csv(output_no_demo_path, nrows=1)\n",
    "    print(f\"   Rows: {len(df_augmented):,}, Columns: {len(df_check.columns)}\")\n",
    "    print(f\"   Size: {output_no_demo_path.stat().st_size / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "print(\"\\n3. DATASET WITH DEMOGRAPHICS:\")\n",
    "print(f\"   {output_with_demo_path}\")\n",
    "if output_with_demo_path.exists():\n",
    "    df_check = pd.read_csv(output_with_demo_path, nrows=1)\n",
    "    print(f\"   Rows: {len(df_final):,}, Columns: {len(df_check.columns)}\")\n",
    "    print(f\"   Size: {output_with_demo_path.stat().st_size / 1024 / 1024:.1f} MB\")\n",
    "    print(f\"   Includes: age, sex columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated vocabulary file: c:\\Users\\henry\\Desktop\\Programming\\Python\\Multimodal_Diagnosis\\data\\symptom_vocabulary.json\n",
      "  Original vocabulary: 456 symptoms\n",
      "  Final vocabulary: 456 symptoms\n",
      "  Net change: +0 symptoms\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FINAL STEP: Update vocabulary file to match actual dataset columns\n",
    "# This ensures vocabulary.json stays in sync with the augmented data\n",
    "# ============================================================================\n",
    "\n",
    "# Get final symptom columns from augmented dataset\n",
    "final_symptom_cols = [c for c in df_final.columns \n",
    "                       if c.lower() not in {s.lower() for s in NON_SYMPTOM_COLS} \n",
    "                       and c != 'symptoms']\n",
    "\n",
    "# Sort alphabetically for consistency\n",
    "final_symptom_cols = sorted(final_symptom_cols)\n",
    "\n",
    "# Save updated vocabulary\n",
    "with open(symptom_cols_path, 'w') as f:\n",
    "    json.dump(final_symptom_cols, f, indent=2)\n",
    "\n",
    "print(f\"Updated vocabulary file: {symptom_cols_path}\")\n",
    "print(f\"  Original vocabulary: {len(ORIGINAL_VOCAB)} symptoms\")\n",
    "print(f\"  Final vocabulary: {len(final_symptom_cols)} symptoms\")\n",
    "print(f\"  Net change: {len(final_symptom_cols) - len(ORIGINAL_VOCAB):+d} symptoms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Documentation for Research Paper\n",
    "\n",
    "> **Data Augmentation Pipeline**\n",
    ">\n",
    "> **Stage 0 - Vocabulary Expansion:**\n",
    "> 1. Collected symptom lists from Mayo Clinic and Cleveland Clinic for 135 rare diseases\n",
    "> 2. Identified symptoms appearing in >=5 diseases not in original vocabulary\n",
    "> 3. Expanded vocabulary from 377 to N symptoms (updated in place)\n",
    ">\n",
    "> **Stage 1 - Synthetic Symptom Data:**\n",
    "> 1. Mapped Mayo Clinic symptoms to expanded vocabulary\n",
    "> 2. For diseases with <20 training samples, generated synthetic samples\n",
    "> 3. Each synthetic sample: random 4-8 symptom subset from disease's symptom profile\n",
    "> 4. Increased rare disease representation to minimum 25 samples per disease\n",
    ">\n",
    "> **Stage 2 - Demographic Variables (Age/Sex):**\n",
    "> 1. Collected epidemiological demographics via GPT-4 queries\n",
    "> 2. Applied category-level defaults with keyword-based overrides for sex-specific diseases\n",
    "> 3. Age sampled from triangular distribution (min, peak, max)\n",
    "> 4. Sex sampled from Bernoulli distribution based on disease-specific male percentage\n",
    ">\n",
    "> Two output datasets were created:\n",
    "> - `symptoms_augmented_no_demographics.csv`: For symptom-only models\n",
    "> - `symptoms_augmented_with_demographics.csv`: For multimodal models incorporating age/sex"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
