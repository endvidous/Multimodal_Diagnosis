{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume utilities loaded. Checkpoints stored in: c:\\Users\\henry\\Desktop\\Programming\\Python\\Multimodal_Diagnosis\\notebooks\\checkpoints\n"
     ]
    }
   ],
   "source": [
    "# ==== Resume / Checkpoint Utilities (Fixed) ====\n",
    "import os, pickle\n",
    "from pathlib import Path\n",
    "\n",
    "CKPT_DIR = Path('checkpoints')\n",
    "CKPT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def get_cv_state_file(cv_name: str) -> Path:\n",
    "    \"\"\"Get state file path for a specific CV run.\"\"\"\n",
    "    return CKPT_DIR / f'cv_state_{cv_name}.pkl'\n",
    "\n",
    "def load_cv_state(cv_name: str) -> dict:\n",
    "    \"\"\"Load checkpoint state for a CV run.\"\"\"\n",
    "    state_file = get_cv_state_file(cv_name)\n",
    "    if state_file.exists():\n",
    "        with open(state_file, 'rb') as f:\n",
    "            state = pickle.load(f)\n",
    "            print(f\"  Loaded checkpoint for '{cv_name}': {len(state.get('fold_results', {}))} folds completed\")\n",
    "            return state\n",
    "    return {'fold_results': {}}  # fold_idx -> results dict\n",
    "\n",
    "def save_cv_state(cv_name: str, state: dict):\n",
    "    \"\"\"Save checkpoint state for a CV run.\"\"\"\n",
    "    state_file = get_cv_state_file(cv_name)\n",
    "    with open(state_file, 'wb') as f:\n",
    "        pickle.dump(state, f)\n",
    "\n",
    "def clear_cv_state(cv_name: str):\n",
    "    \"\"\"Clear checkpoint for a CV run (use when starting fresh).\"\"\"\n",
    "    state_file = get_cv_state_file(cv_name)\n",
    "    if state_file.exists():\n",
    "        os.remove(state_file)\n",
    "        print(f\"  Cleared checkpoint for '{cv_name}'\")\n",
    "\n",
    "print('Resume utilities loaded. Checkpoints stored in:', CKPT_DIR.absolute())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rigorous Evaluation for Research Paper\n",
    "\n",
    "This notebook addresses reviewer feedback by providing:\n",
    "1. **Real vs Synthetic evaluation** - Separate performance on base data vs augmented data\n",
    "2. **5-Fold Cross-Validation** - Statistical validity of results\n",
    "3. **Ablation Studies** - Contribution of each component\n",
    "4. **Error Analysis** - Confusion matrices and failure patterns\n",
    "\n",
    "**Update**: Evaluating using the custom **Hierarchical Symptom-to-Disease** architecture (Category -> Disease) instead of flat classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\Users\\henry\\Desktop\\Programming\\Python\\Multimodal_Diagnosis\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json, joblib, warnings, sys, os, time, re\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "project_root = Path(os.getcwd()).parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (classification_report, accuracy_score, top_k_accuracy_score,\n",
    "    confusion_matrix, f1_score, precision_score, recall_score)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Import custom hierarchical models\n",
    "from models.architectures.symptom_classifier import SymptomCategoryClassifier, SymptomDiseaseClassifier\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Load All Data Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base data exists: True\n",
      "Augmented (no demo) exists: True\n",
      "Augmented (with demo) exists: True\n"
     ]
    }
   ],
   "source": [
    "# Data paths\n",
    "base_data_path = project_root / \"data\" / \"processed\" / \"symptoms\" / \"symptoms_to_disease_cleaned.csv\"\n",
    "augmented_no_demo_path = project_root / \"data\" / \"processed\" / \"symptoms\" / \"symptoms_augmented_no_demographics.csv\"\n",
    "augmented_with_demo_path = project_root / \"data\" / \"processed\" / \"symptoms\" / \"symptoms_augmented_with_demographics.csv\"\n",
    "\n",
    "# Load symptom vocabulary\n",
    "with open(project_root / \"data\" / \"symptom_vocabulary.json\") as f:\n",
    "    symptom_cols = json.load(f)\n",
    "\n",
    "print(f\"Base data exists: {base_data_path.exists()}\")\n",
    "print(f\"Augmented (no demo) exists: {augmented_no_demo_path.exists()}\")\n",
    "print(f\"Augmented (with demo) exists: {augmented_with_demo_path.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base dataset: 206,267 rows, 627 diseases\n",
      "Augmented dataset: 207,518 rows, 627 diseases\n",
      "Augmented + Demographics: 207,518 rows, 627 diseases\n"
     ]
    }
   ],
   "source": [
    "# Load data \n",
    "df_base = pd.read_csv(base_data_path)\n",
    "print(f\"Base dataset: {len(df_base):,} rows, {df_base['diseases'].nunique()} diseases\")\n",
    "\n",
    "df_augmented = pd.read_csv(augmented_no_demo_path)\n",
    "print(f\"Augmented dataset: {len(df_augmented):,} rows, {df_augmented['diseases'].nunique()} diseases\")\n",
    "\n",
    "df_demo = pd.read_csv(augmented_with_demo_path)\n",
    "print(f\"Augmented + Demographics: {len(df_demo):,} rows, {df_demo['diseases'].nunique()} diseases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitization disabled - using natural symptom names with spaces\n"
     ]
    }
   ],
   "source": [
    "# REMOVED: sanitize_column_names function\n",
    "# LightGBM works fine with spaces in column names.\n",
    "# This function was overly aggressive and broke symptom mapping.\n",
    "print(\"Sanitization disabled - using natural symptom names with spaces\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 206,267 rows, 627 diseases\n",
      "Filtered: 206,173 rows, 588 diseases\n",
      "Removed: 39 diseases with <5 samples\n",
      "Original: 207,518 rows, 627 diseases\n",
      "Filtered: 207,492 rows, 616 diseases\n",
      "Removed: 11 diseases with <5 samples\n",
      "Original: 207,518 rows, 627 diseases\n",
      "Filtered: 207,492 rows, 616 diseases\n",
      "Removed: 11 diseases with <5 samples\n"
     ]
    }
   ],
   "source": [
    "def filter_min_samples(df, min_samples=2):\n",
    "    \"\"\"Filter to keep only diseases with at least min_samples.\"\"\"\n",
    "    disease_counts = df['diseases'].value_counts()\n",
    "    valid_diseases = disease_counts[disease_counts >= min_samples].index\n",
    "    df_filtered = df[df['diseases'].isin(valid_diseases)].copy()\n",
    "    \n",
    "    print(f\"Original: {len(df):,} rows, {df['diseases'].nunique()} diseases\")\n",
    "    print(f\"Filtered: {len(df_filtered):,} rows, {df_filtered['diseases'].nunique()} diseases\")\n",
    "    print(f\"Removed: {df['diseases'].nunique() - df_filtered['diseases'].nunique()} diseases with <{min_samples} samples\")\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "# Filter for stable evaluation\n",
    "df_base_filtered = filter_min_samples(df_base, min_samples=5)\n",
    "df_augmented_filtered = filter_min_samples(df_augmented, min_samples=5)\n",
    "df_demo_filtered = filter_min_samples(df_demo, min_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Define Hierarchical Model Utilities\n",
    "\n",
    "We need to replicate the 2-stage inference process:\n",
    "1. **Category Classifier** identifies likely disease categories\n",
    "2. **Specialist Classifiers** predict specific diseases within those categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(df, feature_cols):\n",
    "    \"\"\"Prepare features and labels from dataframe.\"\"\"\n",
    "    available_cols = [c for c in feature_cols if c in df.columns]\n",
    "    # Return DataFrame to preserve column names for feature importance/LGBM\n",
    "    X = df[available_cols]\n",
    "    y_disease = df['diseases'].values\n",
    "    return X, y_disease, available_cols\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "def _train_specialist(category, X_train, y_train, y_categories):\n",
    "    mask = y_categories == category\n",
    "    X_cat = X_train[mask]\n",
    "    y_cat = y_train[mask]\n",
    "\n",
    "    if len(y_cat) < 5:\n",
    "        return None, None\n",
    "\n",
    "    model = SymptomDiseaseClassifier(category=category, n_estimators=100)\n",
    "    model.fit(X_cat, y_cat)\n",
    "    return category, model\n",
    "\n",
    "\n",
    "def train_hierarchical_model(X_train, y_train, df_train_full, n_jobs=None):\n",
    "    \"\"\"Train hierarchical model (Category -> Specialist Disease Classifiers).\"\"\"\n",
    "\n",
    "    if n_jobs is None:\n",
    "        n_jobs = max(1, multiprocessing.cpu_count() - 1)\n",
    "\n",
    "    # 1. Train Category Classifier (keep serial)\n",
    "    print(\"  Training Category Classifier...\")\n",
    "    y_categories = df_train_full['disease_category'].values\n",
    "\n",
    "    cat_encoder = LabelEncoder()\n",
    "    y_cat_encoded = cat_encoder.fit_transform(y_categories)\n",
    "\n",
    "    cat_clf = SymptomCategoryClassifier(n_estimators=100)\n",
    "    cat_clf.fit(X_train, y_cat_encoded)\n",
    "    cat_clf.encoder_ = cat_encoder\n",
    "\n",
    "    # 2. Train specialist models (PARALLEL)\n",
    "    unique_categories = np.unique(y_categories)\n",
    "    print(f\"  Training {len(unique_categories)} Specialist Models (parallel, n_jobs={n_jobs})...\")\n",
    "\n",
    "    results = Parallel(n_jobs=n_jobs, backend=\"loky\", verbose=0)(\n",
    "        delayed(_train_specialist)(cat, X_train, y_train, y_categories)\n",
    "        for cat in unique_categories\n",
    "    )\n",
    "\n",
    "    specialist_models = {\n",
    "        cat: model for cat, model in results if model is not None\n",
    "    }\n",
    "\n",
    "    return cat_clf, specialist_models\n",
    "\n",
    "def predict_hierarchical(X_test, cat_clf, specialist_models, all_possible_diseases):\n",
    "    \"\"\"Hierarchical prediction logic.\"\"\"\n",
    "    # Get category probabilities\n",
    "    cat_probs = cat_clf.predict_proba(X_test)\n",
    "    cat_encoder = cat_clf.encoder_\n",
    "    \n",
    "    # Initialize output matrix\n",
    "    final_probs = np.zeros((len(X_test), len(all_possible_diseases)))\n",
    "    disease_to_idx = {d: i for i, d in enumerate(all_possible_diseases)}\n",
    "    \n",
    "    # Route predictions\n",
    "    # cat_clf.categories consists of the sorted integer labels from training\n",
    "    for i, cat_idx in enumerate(cat_clf.categories):\n",
    "        # Decode to string name to find correct specialist\n",
    "        cat_name = cat_encoder.inverse_transform([cat_idx])[0]\n",
    "        \n",
    "        if cat_name not in specialist_models: continue\n",
    "        \n",
    "        model = specialist_models[cat_name]\n",
    "        # Predict only for this category's diseases\n",
    "        specialist_probs = model.predict_proba(X_test)\n",
    "        \n",
    "        # Weight by P(Category)\n",
    "        weight = cat_probs[:, i][:, np.newaxis]\n",
    "        \n",
    "        # Add weighted probs to final matrix\n",
    "        for local_idx, disease in enumerate(model.diseases):\n",
    "            if disease in disease_to_idx:\n",
    "                global_idx = disease_to_idx[disease]\n",
    "                final_probs[:, global_idx] += weight.ravel() * specialist_probs[:, local_idx]\n",
    "                \n",
    "    return final_probs\n",
    "\n",
    "def evaluate_hierarchical(X_train, X_test, y_train, y_test, df_train_full, all_disease_classes):\n",
    "    \"\"\"Train and evaluate using hierarchical approach.\"\"\"\n",
    "    # Train\n",
    "    cat_clf, specialist_models = train_hierarchical_model(X_train, y_train, df_train_full)\n",
    "    \n",
    "    # Predict\n",
    "    y_proba = predict_hierarchical(X_test, cat_clf, specialist_models, all_disease_classes)\n",
    "    \n",
    "    # Map true labels to indices\n",
    "    y_pred_idx = np.argmax(y_proba, axis=1)\n",
    "    # We need to map y_test strings to indices in all_disease_classes\n",
    "    disease_to_idx = {d: i for i, d in enumerate(all_disease_classes)}\n",
    "    y_test_idx = np.array([disease_to_idx.get(d, -1) for d in y_test])\n",
    "    \n",
    "    # Filter out valid labels (in case test set has disease not in training classes, though unlikely with split)\n",
    "    valid_mask = y_test_idx != -1\n",
    "    \n",
    "    all_labels = np.arange(len(all_disease_classes))\n",
    "    \n",
    "    results = {\n",
    "        'Top-1': accuracy_score(y_test_idx[valid_mask], y_pred_idx[valid_mask]),\n",
    "        'Top-3': top_k_accuracy_score(y_test_idx[valid_mask], y_proba[valid_mask], k=3, labels=all_labels),\n",
    "        'Top-5': top_k_accuracy_score(y_test_idx[valid_mask], y_proba[valid_mask], k=5, labels=all_labels),\n",
    "        'Macro-F1': f1_score(y_test_idx[valid_mask], y_pred_idx[valid_mask], average='macro')\n",
    "    }\n",
    "    \n",
    "    return results, cat_clf, specialist_models, y_proba\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature columns: 375\n"
     ]
    }
   ],
   "source": [
    "# Define feature columns (same for all)\n",
    "non_feature_cols = ['diseases', 'disease_category', 'symptoms', 'age', 'sex', 'age_normalized', 'sex_encoded']\n",
    "feature_cols_base = [c for c in df_base.columns if c not in non_feature_cols]\n",
    "print(f\"Feature columns: {len(feature_cols_base)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Evaluation on Base/Real Data Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EVALUATION ON BASE/REAL DATA ONLY (Hierarchical)\n",
      "======================================================================\n",
      "  Training Category Classifier...\n",
      "Training SymptomCategoryClassifier with shape (185555, 375)\n",
      "  Training 17 Specialist Models...\n",
      "Training SymptomDiseaseClassifier for category 'Allergy and Immunology' with shape (2486, 375)\n",
      "Training SymptomDiseaseClassifier for category 'Cardiovascular and Circulatory' with shape (16633, 375)\n",
      "Training SymptomDiseaseClassifier for category 'Dermatological' with shape (15106, 375)\n",
      "Training SymptomDiseaseClassifier for category 'Endocrine and Metabolic' with shape (6882, 375)\n",
      "Training SymptomDiseaseClassifier for category 'Gastrointestinal and Hepatic' with shape (22510, 375)\n",
      "Training SymptomDiseaseClassifier for category 'Genetic and Congenital Disorders' with shape (139, 375)\n",
      "Training SymptomDiseaseClassifier for category 'Genitourinary and Reproductive' with shape (24769, 375)\n",
      "Training SymptomDiseaseClassifier for category 'Hematology and Oncology' with shape (4021, 375)\n",
      "Training SymptomDiseaseClassifier for category 'Infectious Diseases' with shape (5741, 375)\n",
      "Training SymptomDiseaseClassifier for category 'Mental and Behavioral Health' with shape (16742, 375)\n",
      "Training SymptomDiseaseClassifier for category 'Musculoskeletal' with shape (17978, 375)\n",
      "Training SymptomDiseaseClassifier for category 'Neurological Disorders' with shape (10913, 375)\n",
      "Training SymptomDiseaseClassifier for category 'Obstetrics and Neonatal' with shape (3613, 375)\n",
      "Training SymptomDiseaseClassifier for category 'Ophthalmology and ENT' with shape (24487, 375)\n",
      "Training SymptomDiseaseClassifier for category 'Respiratory System' with shape (10577, 375)\n",
      "Training SymptomDiseaseClassifier for category 'Trauma, Poisoning and Environmental' with shape (526, 375)\n",
      "Training SymptomDiseaseClassifier for category 'Unknown Type' with shape (2432, 375)\n",
      "\n",
      "Results on Real Data Only:\n",
      "  Top-1: 81.87%\n",
      "  Top-3: 94.26%\n",
      "  Top-5: 96.68%\n",
      "  Macro-F1: 69.99%\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EVALUATION ON BASE/REAL DATA ONLY (Hierarchical)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "X_base, y_base, cols_base = prepare_features(df_base_filtered, feature_cols_base)\n",
    "all_diseases_base = np.unique(y_base)\n",
    "\n",
    "# Split indices\n",
    "indices = np.arange(len(df_base_filtered))\n",
    "train_idx, test_idx = train_test_split(\n",
    "    indices, test_size=0.1, random_state=42, stratify=y_base\n",
    ")\n",
    "\n",
    "X_train_base = X_base.iloc[train_idx]\n",
    "X_test_base = X_base.iloc[test_idx]\n",
    "y_train_base = y_base[train_idx]\n",
    "y_test_base = y_base[test_idx]\n",
    "df_train_base = df_base_filtered.iloc[train_idx]\n",
    "\n",
    "results_base, cat_clf_base, _, _ = evaluate_hierarchical(\n",
    "    X_train_base, X_test_base, y_train_base, y_test_base, df_train_base, all_diseases_base\n",
    ")\n",
    "\n",
    "print(f\"\\nResults on Real Data Only:\")\n",
    "for metric, value in results_base.items():\n",
    "    print(f\"  {metric}: {value*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Evaluation on Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EVALUATION ON AUGMENTED DATA (Hierarchical)\n",
      "======================================================================\n",
      "  Training Category Classifier...\n",
      "Training SymptomCategoryClassifier with shape (186742, 456)\n",
      "  Training 17 Specialist Models...\n",
      "Training SymptomDiseaseClassifier for category 'Allergy and Immunology' with shape (2486, 456)\n",
      "Training SymptomDiseaseClassifier for category 'Cardiovascular and Circulatory' with shape (16741, 456)\n",
      "Training SymptomDiseaseClassifier for category 'Dermatological' with shape (15151, 456)\n",
      "Training SymptomDiseaseClassifier for category 'Endocrine and Metabolic' with shape (7148, 456)\n",
      "Training SymptomDiseaseClassifier for category 'Gastrointestinal and Hepatic' with shape (22590, 456)\n",
      "Training SymptomDiseaseClassifier for category 'Genetic and Congenital Disorders' with shape (139, 456)\n",
      "Training SymptomDiseaseClassifier for category 'Genitourinary and Reproductive' with shape (24814, 456)\n",
      "Training SymptomDiseaseClassifier for category 'Hematology and Oncology' with shape (4180, 456)\n",
      "Training SymptomDiseaseClassifier for category 'Infectious Diseases' with shape (6002, 456)\n",
      "Training SymptomDiseaseClassifier for category 'Mental and Behavioral Health' with shape (16740, 456)\n",
      "Training SymptomDiseaseClassifier for category 'Musculoskeletal' with shape (18022, 456)\n",
      "Training SymptomDiseaseClassifier for category 'Neurological Disorders' with shape (10993, 456)\n",
      "Training SymptomDiseaseClassifier for category 'Obstetrics and Neonatal' with shape (3653, 456)\n",
      "Training SymptomDiseaseClassifier for category 'Ophthalmology and ENT' with shape (24497, 456)\n",
      "Training SymptomDiseaseClassifier for category 'Respiratory System' with shape (10622, 456)\n",
      "Training SymptomDiseaseClassifier for category 'Trauma, Poisoning and Environmental' with shape (526, 456)\n",
      "Training SymptomDiseaseClassifier for category 'Unknown Type' with shape (2438, 456)\n",
      "\n",
      "Results on Augmented Data:\n",
      "  Top-1: 82.21%\n",
      "  Top-3: 94.32%\n",
      "  Top-5: 96.73%\n",
      "  Macro-F1: 71.96%\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EVALUATION ON AUGMENTED DATA (Hierarchical)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "feature_cols_aug = [c for c in df_augmented_filtered.columns if c not in non_feature_cols]\n",
    "X_aug, y_aug, cols_aug = prepare_features(df_augmented_filtered, feature_cols_aug)\n",
    "all_diseases_aug = np.unique(y_aug)\n",
    "\n",
    "indices_aug = np.arange(len(df_augmented_filtered))\n",
    "train_idx_aug, test_idx_aug = train_test_split(\n",
    "    indices_aug, test_size=0.1, random_state=42, stratify=y_aug\n",
    ")\n",
    "\n",
    "X_train_aug = X_aug.iloc[train_idx_aug]\n",
    "X_test_aug = X_aug.iloc[test_idx_aug]\n",
    "y_train_aug = y_aug[train_idx_aug]\n",
    "y_test_aug = y_aug[test_idx_aug]\n",
    "df_train_aug = df_augmented_filtered.iloc[train_idx_aug]\n",
    "\n",
    "results_aug, cat_clf_aug, _, _ = evaluate_hierarchical(\n",
    "    X_train_aug, X_test_aug, y_train_aug, y_test_aug, df_train_aug, all_diseases_aug\n",
    ")\n",
    "\n",
    "print(f\"\\nResults on Augmented Data:\")\n",
    "for metric, value in results_aug.items():\n",
    "    print(f\"  {metric}: {value*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EVALUATION ON AUGMENTED DATA + Demographics (Hierarchical)\n",
      "======================================================================\n",
      "  Training Category Classifier...\n",
      "Training SymptomCategoryClassifier with shape (186742, 456)\n",
      "  Training 17 Specialist Models...\n",
      "Training SymptomDiseaseClassifier for category 'Allergy and Immunology' with shape (2486, 456)\n",
      "Training SymptomDiseaseClassifier for category 'Cardiovascular and Circulatory' with shape (16741, 456)\n",
      "Training SymptomDiseaseClassifier for category 'Dermatological' with shape (15151, 456)\n",
      "Training SymptomDiseaseClassifier for category 'Endocrine and Metabolic' with shape (7148, 456)\n",
      "Training SymptomDiseaseClassifier for category 'Gastrointestinal and Hepatic' with shape (22590, 456)\n",
      "Training SymptomDiseaseClassifier for category 'Genetic and Congenital Disorders' with shape (139, 456)\n",
      "Training SymptomDiseaseClassifier for category 'Genitourinary and Reproductive' with shape (24814, 456)\n",
      "Training SymptomDiseaseClassifier for category 'Hematology and Oncology' with shape (4180, 456)\n",
      "Training SymptomDiseaseClassifier for category 'Infectious Diseases' with shape (6002, 456)\n",
      "Training SymptomDiseaseClassifier for category 'Mental and Behavioral Health' with shape (16740, 456)\n",
      "Training SymptomDiseaseClassifier for category 'Musculoskeletal' with shape (18022, 456)\n",
      "Training SymptomDiseaseClassifier for category 'Neurological Disorders' with shape (10993, 456)\n",
      "Training SymptomDiseaseClassifier for category 'Obstetrics and Neonatal' with shape (3653, 456)\n",
      "Training SymptomDiseaseClassifier for category 'Ophthalmology and ENT' with shape (24497, 456)\n",
      "Training SymptomDiseaseClassifier for category 'Respiratory System' with shape (10622, 456)\n",
      "Training SymptomDiseaseClassifier for category 'Trauma, Poisoning and Environmental' with shape (526, 456)\n",
      "Training SymptomDiseaseClassifier for category 'Unknown Type' with shape (2438, 456)\n",
      "\n",
      "Results on Augmented + Demographics Data:\n",
      "  Top-1: 82.21%\n",
      "  Top-3: 94.32%\n",
      "  Top-5: 96.73%\n",
      "  Macro-F1: 71.96%\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EVALUATION ON AUGMENTED DATA + Demographics (Hierarchical)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "feature_cols_demo = [c for c in df_demo_filtered.columns if c not in non_feature_cols]\n",
    "X_demo, y_demo, cols_demo = prepare_features(df_demo_filtered, feature_cols_demo)\n",
    "all_diseases_demo = np.unique(y_demo)\n",
    "\n",
    "indices_demo = np.arange(len(df_demo_filtered))\n",
    "train_idx_demo, test_idx_demo = train_test_split(\n",
    "    indices_demo, test_size=0.1, random_state=42, stratify=y_demo\n",
    ")\n",
    "\n",
    "X_train_demo = X_demo.iloc[train_idx_demo]\n",
    "X_test_demo = X_demo.iloc[test_idx_demo]\n",
    "y_train_demo = y_demo[train_idx_demo]\n",
    "y_test_demo = y_demo[test_idx_demo]\n",
    "df_train_demo = df_demo_filtered.iloc[train_idx_demo]\n",
    "\n",
    "results_demo, cat_clf_demo, _, _ = evaluate_hierarchical(\n",
    "    X_train_demo, X_test_demo, y_train_demo, y_test_demo, df_train_demo, all_diseases_demo\n",
    ")\n",
    "\n",
    "print(f\"\\nResults on Augmented + Demographics Data:\")\n",
    "for metric, value in results_demo.items():\n",
    "    print(f\"  {metric}: {value*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "COMPARISON: Real Data vs Augmented Data (Hierarchical)\n",
      "======================================================================\n",
      "  Metric Real Only Augmented Difference\n",
      "   Top-1    81.87%    82.21%     +0.35%\n",
      "   Top-3    94.26%    94.32%     +0.07%\n",
      "   Top-5    96.68%    96.73%     +0.05%\n",
      "Macro-F1    69.99%    71.96%     +1.97%\n"
     ]
    }
   ],
   "source": [
    "# Comparison summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARISON: Real Data vs Augmented Data (Hierarchical)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Metric': list(results_base.keys()),\n",
    "    'Real Only': [f\"{v*100:.2f}%\" for v in results_base.values()],\n",
    "    'Augmented': [f\"{v*100:.2f}%\" for v in results_aug.values()],\n",
    "    'Difference': [f\"{(results_aug[k] - results_base[k])*100:+.2f}%\" for k in results_base.keys()]\n",
    "})\n",
    "\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 5: 5-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Resume-aware CV loop (fixed version) ---\n",
    "def cross_validate(X, y, df_full, disease_classes, n_folds=5, cv_name='default'):\n",
    "    \"\"\"\n",
    "    Perform stratified k-fold cross-validation with Hierarchical model.\n",
    "    \n",
    "    Supports resuming from checkpoint: completed folds and their results\n",
    "    are saved to disk after each fold.\n",
    "    \n",
    "    Args:\n",
    "        cv_name: Unique identifier for this CV run (e.g., 'base', 'augmented').\n",
    "                 Used to create separate checkpoint files for different runs.\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Load any previously completed folds\n",
    "    cv_state = load_cv_state(cv_name)\n",
    "    fold_results_dict = cv_state.get('fold_results', {})\n",
    "    \n",
    "    indices = np.arange(len(y))\n",
    "    \n",
    "    for fold, (train_idx_cv, test_idx_cv) in enumerate(skf.split(indices, y)):\n",
    "        # Skip if already completed\n",
    "        if fold in fold_results_dict:\n",
    "            print(f\"  Fold {fold+1}: SKIPPED (already completed, Top-1={fold_results_dict[fold]['Top-1']*100:.2f}%)\")\n",
    "            continue\n",
    "        \n",
    "        # Train and evaluate this fold\n",
    "        X_train_cv = X.iloc[train_idx_cv]\n",
    "        X_test_cv = X.iloc[test_idx_cv]\n",
    "        y_train_cv = y[train_idx_cv]\n",
    "        y_test_cv = y[test_idx_cv]\n",
    "        df_train_cv = df_full.iloc[train_idx_cv]\n",
    "        \n",
    "        results, _, _, _ = evaluate_hierarchical(\n",
    "            X_train_cv, X_test_cv, y_train_cv, y_test_cv, df_train_cv, disease_classes\n",
    "        )\n",
    "        \n",
    "        # Save fold results immediately after completion\n",
    "        fold_results_dict[fold] = results\n",
    "        cv_state['fold_results'] = fold_results_dict\n",
    "        save_cv_state(cv_name, cv_state)\n",
    "        \n",
    "        print(f\"  Fold {fold+1}: Top-1={results['Top-1']*100:.2f}% (saved)\")\n",
    "    \n",
    "    # Aggregate ALL results (including restored ones)\n",
    "    if len(fold_results_dict) == 0:\n",
    "        raise ValueError(\"No fold results available!\")\n",
    "    \n",
    "    fold_results = [fold_results_dict[i] for i in sorted(fold_results_dict.keys())]\n",
    "    \n",
    "    summary = {}\n",
    "    for metric in fold_results[0].keys():\n",
    "        values = [r[metric] for r in fold_results]\n",
    "        summary[metric] = {\n",
    "            'mean': np.mean(values),\n",
    "            'std': np.std(values)\n",
    "        }\n",
    "    \n",
    "    print(f\"  CV complete: {len(fold_results)} folds aggregated\")\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "5-FOLD CV ON BASE/REAL DATA\n",
      "======================================================================\n",
      "  Loaded checkpoint for 'base': 4 folds completed\n",
      "  Fold 1: SKIPPED (already completed, Top-1=82.09%)\n",
      "  Fold 2: SKIPPED (already completed, Top-1=81.91%)\n",
      "  Fold 3: SKIPPED (already completed, Top-1=81.54%)\n",
      "  Fold 4: SKIPPED (already completed, Top-1=81.99%)\n",
      "  Training Category Classifier...\n",
      "Training SymptomCategoryClassifier with shape (164939, 375)\n",
      "  Training 17 Specialist Models (parallel, n_jobs=3)...\n",
      "  Fold 5: Top-1=81.71% (saved)\n",
      "  CV complete: 5 folds aggregated\n",
      "\n",
      "Cross-Validation Results (Real Data):\n",
      "  Top-1: 81.85% Â± 0.20%\n",
      "  Top-3: 94.23% Â± 0.07%\n",
      "  Top-5: 96.68% Â± 0.05%\n",
      "  Macro-F1: 69.94% Â± 0.25%\n"
     ]
    }
   ],
   "source": [
    "# 5-Fold CV on BASE data\n",
    "print(\"=\"*70)\n",
    "print(\"5-FOLD CV ON BASE/REAL DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "cv_results_base = cross_validate(X_base, y_base, df_base_filtered, all_diseases_base, cv_name='base')\n",
    "\n",
    "print(f\"\\nCross-Validation Results (Real Data):\")\n",
    "for metric, stats in cv_results_base.items():\n",
    "    print(f\"  {metric}: {stats['mean']*100:.2f}% Â± {stats['std']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "5-FOLD CV ON AUGMENTED DATA\n",
      "======================================================================\n",
      "  Training Category Classifier...\n",
      "Training SymptomCategoryClassifier with shape (165993, 456)\n",
      "  Training 17 Specialist Models (parallel, n_jobs=3)...\n",
      "  Fold 1: Top-1=81.71% (saved)\n",
      "  Training Category Classifier...\n",
      "Training SymptomCategoryClassifier with shape (165993, 456)\n",
      "  Training 17 Specialist Models (parallel, n_jobs=3)...\n",
      "  Fold 2: Top-1=81.50% (saved)\n",
      "  Training Category Classifier...\n",
      "Training SymptomCategoryClassifier with shape (165994, 456)\n",
      "  Training 17 Specialist Models (parallel, n_jobs=3)...\n",
      "  Fold 3: Top-1=81.62% (saved)\n",
      "  Training Category Classifier...\n",
      "Training SymptomCategoryClassifier with shape (165994, 456)\n",
      "  Training 17 Specialist Models (parallel, n_jobs=3)...\n",
      "  Fold 4: Top-1=81.66% (saved)\n",
      "  Training Category Classifier...\n",
      "Training SymptomCategoryClassifier with shape (165994, 456)\n",
      "  Training 17 Specialist Models (parallel, n_jobs=3)...\n",
      "  Fold 5: Top-1=81.63% (saved)\n",
      "  CV complete: 5 folds aggregated\n",
      "\n",
      "Cross-Validation Results (Augmented):\n",
      "  Top-1: 81.62% Â± 0.07%\n",
      "  Top-3: 94.02% Â± 0.06%\n",
      "  Top-5: 96.54% Â± 0.03%\n",
      "  Macro-F1: 70.27% Â± 0.21%\n"
     ]
    }
   ],
   "source": [
    "# 5-Fold CV on AUGMENTED data\n",
    "print(\"=\"*70)\n",
    "print(\"5-FOLD CV ON AUGMENTED DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "cv_results_aug = cross_validate(X_aug, y_aug, df_augmented_filtered, all_diseases_aug, cv_name='augmented')\n",
    "\n",
    "print(f\"\\nCross-Validation Results (Augmented):\")\n",
    "for metric, stats in cv_results_aug.items():\n",
    "    print(f\"  {metric}: {stats['mean']*100:.2f}% Â± {stats['std']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 6: Ablation (Demographics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "5-FOLD CV ON DEMOGRAPHICS DATA\n",
      "======================================================================\n",
      "  Training Category Classifier...\n",
      "Training SymptomCategoryClassifier with shape (165993, 458)\n",
      "  Training 17 Specialist Models (parallel, n_jobs=3)...\n",
      "  Fold 1: Top-1=84.06% (saved)\n",
      "  Training Category Classifier...\n",
      "Training SymptomCategoryClassifier with shape (165993, 458)\n",
      "  Training 17 Specialist Models (parallel, n_jobs=3)...\n",
      "  Fold 2: Top-1=84.10% (saved)\n",
      "  Training Category Classifier...\n",
      "Training SymptomCategoryClassifier with shape (165994, 458)\n",
      "  Training 17 Specialist Models (parallel, n_jobs=3)...\n",
      "  Fold 3: Top-1=84.04% (saved)\n",
      "  Training Category Classifier...\n",
      "Training SymptomCategoryClassifier with shape (165994, 458)\n",
      "  Training 17 Specialist Models (parallel, n_jobs=3)...\n",
      "  Fold 4: Top-1=84.11% (saved)\n",
      "  Training Category Classifier...\n",
      "Training SymptomCategoryClassifier with shape (165994, 458)\n",
      "  Training 17 Specialist Models (parallel, n_jobs=3)...\n",
      "  Fold 5: Top-1=84.05% (saved)\n",
      "  CV complete: 5 folds aggregated\n",
      "\n",
      "Cross-Validation Results (With Demographics):\n",
      "  Top-1: 84.07% Â± 0.03%\n",
      "  Top-3: 95.28% Â± 0.05%\n",
      "  Top-5: 97.34% Â± 0.07%\n",
      "  Macro-F1: 72.69% Â± 0.37%\n",
      "\n",
      "ðŸ“Š Demographics Contribution (5-Fold CV):\n",
      "  Augmented (no demo) Top-1: 81.62% Â± 0.07%\n",
      "  With Demographics Top-1:   84.07% Â± 0.03%\n",
      "  Demographics Effect:       +2.45%\n"
     ]
    }
   ],
   "source": [
    "# Demographics: Data Preparation + 5-Fold Cross-Validation\n",
    "print(\"=\"*70)\n",
    "print(\"5-FOLD CV ON DEMOGRAPHICS DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Prepare demographic features\n",
    "df_demo_filtered['sex_encoded'] = (df_demo_filtered['sex'] == 'M').astype(int)\n",
    "df_demo_filtered['age_normalized'] = df_demo_filtered['age'] / 100.0\n",
    "\n",
    "feature_cols_demo = [c for c in df_augmented_filtered.columns if c not in non_feature_cols] + ['age_normalized', 'sex_encoded']\n",
    "feature_cols_demo = [c for c in feature_cols_demo if c in df_demo_filtered.columns]\n",
    "\n",
    "X_demo = df_demo_filtered[feature_cols_demo]\n",
    "y_demo = df_demo_filtered['diseases'].values\n",
    "all_diseases_demo = np.unique(y_demo)\n",
    "\n",
    "# 5-Fold Cross-Validation on Demographics data\n",
    "cv_results_demo = cross_validate(X_demo, y_demo, df_demo_filtered, all_diseases_demo, cv_name='demographics')\n",
    "\n",
    "print(f\"\\nCross-Validation Results (With Demographics):\")\n",
    "for metric, stats in cv_results_demo.items():\n",
    "    print(f\"  {metric}: {stats['mean']*100:.2f}% Â± {stats['std']*100:.2f}%\")\n",
    "\n",
    "# Demographics contribution (comparing CV results)\n",
    "print(f\"\\nðŸ“Š Demographics Contribution (5-Fold CV):\")\n",
    "print(f\"  Augmented (no demo) Top-1: {cv_results_aug['Top-1']['mean']*100:.2f}% Â± {cv_results_aug['Top-1']['std']*100:.2f}%\")\n",
    "print(f\"  With Demographics Top-1:   {cv_results_demo['Top-1']['mean']*100:.2f}% Â± {cv_results_demo['Top-1']['std']*100:.2f}%\")\n",
    "demo_contribution = (cv_results_demo['Top-1']['mean'] - cv_results_aug['Top-1']['mean']) * 100\n",
    "print(f\"  Demographics Effect:       {demo_contribution:+.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ABLATION STUDY SUMMARY (5-Fold Cross-Validation)\n",
      "======================================================================\n",
      "           Configuration Top-1 Mean Top-1 Std Top-5 Mean\n",
      "          Real Data Only     81.85%    Â±0.20%     96.68%\n",
      "     Augmented (no demo)     81.62%    Â±0.07%     96.54%\n",
      "Augmented + Demographics     84.07%    Â±0.03%     97.34%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ABLATION STUDY SUMMARY (5-Fold Cross-Validation)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "ablation_cv_df = pd.DataFrame({\n",
    "    'Configuration': [\n",
    "        'Real Data Only', \n",
    "        'Augmented (no demo)', \n",
    "        'Augmented + Demographics'\n",
    "    ],\n",
    "    'Top-1 Mean': [\n",
    "        f\"{cv_results_base['Top-1']['mean']*100:.2f}%\",\n",
    "        f\"{cv_results_aug['Top-1']['mean']*100:.2f}%\",\n",
    "        f\"{cv_results_demo['Top-1']['mean']*100:.2f}%\"\n",
    "    ],\n",
    "    'Top-1 Std': [\n",
    "        f\"Â±{cv_results_base['Top-1']['std']*100:.2f}%\",\n",
    "        f\"Â±{cv_results_aug['Top-1']['std']*100:.2f}%\",\n",
    "        f\"Â±{cv_results_demo['Top-1']['std']*100:.2f}%\"\n",
    "    ],\n",
    "    'Top-5 Mean': [\n",
    "        f\"{cv_results_base['Top-5']['mean']*100:.2f}%\",\n",
    "        f\"{cv_results_aug['Top-5']['mean']*100:.2f}%\",\n",
    "        f\"{cv_results_demo['Top-5']['mean']*100:.2f}%\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(ablation_cv_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 7: Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating confusion matrix for Real Data...\n",
      "  Training Category Classifier...\n",
      "Training SymptomCategoryClassifier with shape (185555, 375)\n",
      "  Training 17 Specialist Models (parallel, n_jobs=3)...\n",
      "Confusion matrix shape: (584, 584)\n"
     ]
    }
   ],
   "source": [
    "# Re-run prediction to get confusion matrix from Hierarchical model (Base Data)\n",
    "print(\"Generating confusion matrix for Real Data...\")\n",
    "\n",
    "cat_clf_err, specialist_err = train_hierarchical_model(X_train_base, y_train_base, df_train_base)\n",
    "y_proba_err = predict_hierarchical(X_test_base, cat_clf_err, specialist_err, all_diseases_base)\n",
    "\n",
    "# Map to indices\n",
    "y_pred_err_idx = np.argmax(y_proba_err, axis=1)\n",
    "disease_to_idx = {d: i for i, d in enumerate(all_diseases_base)}\n",
    "y_test_err_idx = np.array([disease_to_idx.get(d, -1) for d in y_test_base])\n",
    "\n",
    "# Filter valid only\n",
    "valid = y_test_err_idx != -1\n",
    "cm = confusion_matrix(y_test_err_idx[valid], y_pred_err_idx[valid])\n",
    "\n",
    "print(f\"Confusion matrix shape: {cm.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 Most Confused Disease Pairs:\n",
      "--------------------------------------------------------------------------------\n",
      "   59x: neurosis                            â†’ impulse control disorder\n",
      "   37x: syphilis                            â†’ bladder disorder\n",
      "   23x: cholecystitis                       â†’ galactorrhea of unknown cause\n",
      "   20x: uterine cancer                      â†’ testicular torsion\n",
      "   19x: plantar fasciitis                   â†’ acute bronchospasm\n",
      "   18x: drug withdrawal                     â†’ corneal disorder\n",
      "   17x: chronic glaucoma                    â†’ foreign body in the ear\n",
      "   17x: cysticercosis                       â†’ syphilis\n",
      "   17x: dementia                            â†’ polycystic ovarian syndrome (pcos)\n",
      "   17x: osteoporosis                        â†’ acute otitis media\n",
      "   16x: sarcoidosis                         â†’ pseudotumor cerebri\n",
      "   14x: acute pancreatitis                  â†’ galactorrhea of unknown cause\n",
      "   14x: heart failure                       â†’ sickle cell crisis\n",
      "   14x: lymphogranuloma venereum            â†’ diabetic peripheral neuropathy\n",
      "   14x: neonatal jaundice                   â†’ fibrocystic breast disease\n"
     ]
    }
   ],
   "source": [
    "# Find most confused pairs\n",
    "def find_confusion_pairs(cm, disease_names, top_k=10):\n",
    "    \"\"\"Find most confused disease pairs.\"\"\"\n",
    "    n = cm.shape[0]\n",
    "    pairs = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j and cm[i, j] > 0:\n",
    "                pairs.append({\n",
    "                    'true': disease_names[i],\n",
    "                    'pred': disease_names[j],\n",
    "                    'count': cm[i, j]\n",
    "                })\n",
    "    \n",
    "    pairs.sort(key=lambda x: -x['count'])\n",
    "    return pairs[:top_k]\n",
    "\n",
    "confused_pairs = find_confusion_pairs(cm, all_diseases_base, top_k=15)\n",
    "\n",
    "print(\"Top 15 Most Confused Disease Pairs:\")\n",
    "print(\"-\" * 80)\n",
    "for p in confused_pairs:\n",
    "    print(f\"  {p['count']:3d}x: {p['true'][:35]:35s} â†’ {p['pred'][:35]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Worst Performing Diseases (Lowest Accuracy):\n",
      "------------------------------------------------------------\n",
      "    0.0% (  2 samples): abscess of the lung\n",
      "    0.0% (  4 samples): acariasis\n",
      "    0.0% (  1 samples): acute fatty liver of pregnancy (aflp)\n",
      "    0.0% (  4 samples): anemia of chronic disease\n",
      "    0.0% (  1 samples): avascular necrosis\n",
      "    0.0% (  1 samples): blepharospasm\n",
      "    0.0% (  1 samples): breast cancer\n",
      "    0.0% (  2 samples): breast cyst\n",
      "    0.0% (  1 samples): chronic kidney disease\n",
      "    0.0% (  1 samples): cushing syndrome\n",
      "    0.0% (  2 samples): cyst of the eyelid\n",
      "    0.0% (  1 samples): cystic fibrosis\n",
      "    0.0% (  4 samples): ectropion\n",
      "    0.0% (  1 samples): edward syndrome\n",
      "    0.0% (  1 samples): empyema\n"
     ]
    }
   ],
   "source": [
    "# Per-class accuracy\n",
    "def per_class_accuracy(y_true_idx, y_pred_idx, disease_names, top_worst=10):\n",
    "    \"\"\"Calculate per-class accuracy.\"\"\"\n",
    "    accuracies = []\n",
    "    \n",
    "    for i, cls in enumerate(disease_names):\n",
    "        mask = y_true_idx == i\n",
    "        if mask.sum() > 0:\n",
    "            acc = (y_pred_idx[mask] == i).mean()\n",
    "            accuracies.append((cls, acc, mask.sum()))\n",
    "    \n",
    "    accuracies.sort(key=lambda x: x[1])\n",
    "    return accuracies[:top_worst]\n",
    "\n",
    "worst_classes = per_class_accuracy(y_test_err_idx[valid], y_pred_err_idx[valid], all_diseases_base, top_worst=15)\n",
    "\n",
    "print(\"\\nWorst Performing Diseases (Lowest Accuracy):\")\n",
    "print(\"-\" * 60)\n",
    "for cls, acc, count in worst_classes:\n",
    "    print(f\"  {acc*100:5.1f}% ({count:3d} samples): {cls}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 8: Save Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to figures/rigorous_eval_results.json\n",
      "\n",
      "======================================================================\n",
      "FINAL RESULTS SUMMARY (Hierarchical Model)\n",
      "======================================================================\n",
      "           Configuration Top-1 Mean Top-1 Std Top-5 Mean\n",
      "          Real Data Only     81.85%    Â±0.20%     96.68%\n",
      "     Augmented (no demo)     81.62%    Â±0.07%     96.54%\n",
      "Augmented + Demographics     84.07%    Â±0.03%     97.34%\n",
      "\n",
      "Cross-Validation (Real):  81.85% Â± 0.20%\n"
     ]
    }
   ],
   "source": [
    "# Save results to JSON\n",
    "final_results = {\n",
    "    'real_only': results_base,\n",
    "    'augmented': results_aug,\n",
    "    'augmented_demo': results_demo,\n",
    "    'cv_real': {k: {'mean': v['mean'], 'std': v['std']} for k, v in cv_results_base.items()},\n",
    "    'cv_augmented': {k: {'mean': v['mean'], 'std': v['std']} for k, v in cv_results_aug.items()}\n",
    "}\n",
    "\n",
    "with open(project_root / 'notebooks' / 'figures' / 'rigorous_eval_results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2, default=float)\n",
    "\n",
    "print(\"Results saved to figures/rigorous_eval_results.json\")\n",
    "\n",
    "# Print final summary table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL RESULTS SUMMARY (Hierarchical Model)\")\n",
    "print(\"=\"*70)\n",
    "print(ablation_cv_df.to_string(index=False))\n",
    "print(\"\\nCross-Validation (Real): \", f\"{cv_results_base['Top-1']['mean']*100:.2f}% Â± {cv_results_base['Top-1']['std']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 9: Semantic Encoder Evaluation\n",
    "\n",
    "Evaluates how well the `SemanticSymptomEncoder` converts raw symptom text into the 377-dim evidence vector.\n",
    "\n",
    "**Metrics:**\n",
    "- **Precision@K**: Of top-K activated symptoms, how many are ground truth?\n",
    "- **Recall@K**: Of ground truth symptoms, how many are in top-K?\n",
    "- **MRR**: Mean Reciprocal Rank of ground truth symptoms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import models.architectures.semantic_symptom_encoder as sse_module\n",
    "importlib.reload(sse_module)\n",
    "from models.architectures.semantic_symptom_encoder import SemanticSymptomEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MULTI-MODEL + CALIBRATION TUNING (Using YOUR SemanticSymptomEncoder)\n",
      "Testing 6 models Ã— 7 thresholds Ã— 5 exponents\n",
      "======================================================================\n",
      "\n",
      "â–¶ Testing: multi-qa-mpnet-base-dot-v1\n",
      "[Encoder] Loading model: multi-qa-mpnet-base-dot-v1\n",
      "[Encoder] Cached embeddings mismatch, recomputing\n",
      "[Encoder] Computing symptom embeddings (one-time)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:42<00:00,  2.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: multi-qa-mpnet-base-dot-v1\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: multi-qa-mpnet-base-dot-v1\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: multi-qa-mpnet-base-dot-v1\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: multi-qa-mpnet-base-dot-v1\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: multi-qa-mpnet-base-dot-v1\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: multi-qa-mpnet-base-dot-v1\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: multi-qa-mpnet-base-dot-v1\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: multi-qa-mpnet-base-dot-v1\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: multi-qa-mpnet-base-dot-v1\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: multi-qa-mpnet-base-dot-v1\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: multi-qa-mpnet-base-dot-v1\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: multi-qa-mpnet-base-dot-v1\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: multi-qa-mpnet-base-dot-v1\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: multi-qa-mpnet-base-dot-v1\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: multi-qa-mpnet-base-dot-v1\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: multi-qa-mpnet-base-dot-v1\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: multi-qa-mpnet-base-dot-v1\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: multi-qa-mpnet-base-dot-v1\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: multi-qa-mpnet-base-dot-v1\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: multi-qa-mpnet-base-dot-v1\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: multi-qa-mpnet-base-dot-v1\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: multi-qa-mpnet-base-dot-v1\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: multi-qa-mpnet-base-dot-v1\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: multi-qa-mpnet-base-dot-v1\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: multi-qa-mpnet-base-dot-v1\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: multi-qa-mpnet-base-dot-v1\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: multi-qa-mpnet-base-dot-v1\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: multi-qa-mpnet-base-dot-v1\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: multi-qa-mpnet-base-dot-v1\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: multi-qa-mpnet-base-dot-v1\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: multi-qa-mpnet-base-dot-v1\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: multi-qa-mpnet-base-dot-v1\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: multi-qa-mpnet-base-dot-v1\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: multi-qa-mpnet-base-dot-v1\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "   Best: threshold=0.15, exponent=1.00\n",
      "   P@5: 34.4% | R@5: 50.6% | F1: 40.9%\n",
      "\n",
      "â–¶ Testing: all-mpnet-base-v2\n",
      "[Encoder] Loading model: all-mpnet-base-v2\n",
      "[Encoder] Cached embeddings mismatch, recomputing\n",
      "[Encoder] Computing symptom embeddings (one-time)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:43<00:00,  2.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "   Best: threshold=0.15, exponent=1.00\n",
      "   P@5: 35.9% | R@5: 52.8% | F1: 42.7%\n",
      "\n",
      "â–¶ Testing: all-MiniLM-L12-v2\n",
      "[Encoder] Loading model: all-MiniLM-L12-v2\n",
      "[Encoder] Cached embeddings mismatch, recomputing\n",
      "[Encoder] Computing symptom embeddings (one-time)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:14<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-MiniLM-L12-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-MiniLM-L12-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-MiniLM-L12-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-MiniLM-L12-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-MiniLM-L12-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-MiniLM-L12-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-MiniLM-L12-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-MiniLM-L12-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-MiniLM-L12-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-MiniLM-L12-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-MiniLM-L12-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-MiniLM-L12-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-MiniLM-L12-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-MiniLM-L12-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-MiniLM-L12-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-MiniLM-L12-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-MiniLM-L12-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-MiniLM-L12-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-MiniLM-L12-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-MiniLM-L12-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-MiniLM-L12-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-MiniLM-L12-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-MiniLM-L12-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-MiniLM-L12-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-MiniLM-L12-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-MiniLM-L12-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-MiniLM-L12-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-MiniLM-L12-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-MiniLM-L12-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-MiniLM-L12-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-MiniLM-L12-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-MiniLM-L12-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-MiniLM-L12-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: all-MiniLM-L12-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "   Best: threshold=0.15, exponent=1.00\n",
      "   P@5: 29.5% | R@5: 43.7% | F1: 35.2%\n",
      "\n",
      "â–¶ Testing: paraphrase-mpnet-base-v2\n",
      "[Encoder] Loading model: paraphrase-mpnet-base-v2\n",
      "[Encoder] Cached embeddings mismatch, recomputing\n",
      "[Encoder] Computing symptom embeddings (one-time)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:41<00:00,  2.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "   Best: threshold=0.15, exponent=1.00\n",
      "   P@5: 28.3% | R@5: 41.3% | F1: 33.5%\n",
      "\n",
      "â–¶ Testing: paraphrase-MiniLM-L6-v2\n",
      "[Encoder] Loading model: paraphrase-MiniLM-L6-v2\n",
      "[Encoder] Cached embeddings mismatch, recomputing\n",
      "[Encoder] Computing symptom embeddings (one-time)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:07<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-MiniLM-L6-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-MiniLM-L6-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-MiniLM-L6-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-MiniLM-L6-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-MiniLM-L6-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-MiniLM-L6-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-MiniLM-L6-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-MiniLM-L6-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-MiniLM-L6-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-MiniLM-L6-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-MiniLM-L6-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-MiniLM-L6-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-MiniLM-L6-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-MiniLM-L6-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-MiniLM-L6-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-MiniLM-L6-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-MiniLM-L6-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-MiniLM-L6-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-MiniLM-L6-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-MiniLM-L6-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-MiniLM-L6-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-MiniLM-L6-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-MiniLM-L6-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-MiniLM-L6-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-MiniLM-L6-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-MiniLM-L6-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-MiniLM-L6-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-MiniLM-L6-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-MiniLM-L6-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-MiniLM-L6-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-MiniLM-L6-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-MiniLM-L6-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-MiniLM-L6-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: paraphrase-MiniLM-L6-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "   Best: threshold=0.15, exponent=1.00\n",
      "   P@5: 25.1% | R@5: 36.1% | F1: 29.6%\n",
      "\n",
      "â–¶ Testing: sentence-transformers/msmarco-distilbert-cos-v5\n",
      "[Encoder] Loading model: sentence-transformers/msmarco-distilbert-cos-v5\n",
      "[Encoder] Cached embeddings mismatch, recomputing\n",
      "[Encoder] Computing symptom embeddings (one-time)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:21<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: sentence-transformers/msmarco-distilbert-cos-v5\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: sentence-transformers/msmarco-distilbert-cos-v5\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: sentence-transformers/msmarco-distilbert-cos-v5\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: sentence-transformers/msmarco-distilbert-cos-v5\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: sentence-transformers/msmarco-distilbert-cos-v5\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: sentence-transformers/msmarco-distilbert-cos-v5\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: sentence-transformers/msmarco-distilbert-cos-v5\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: sentence-transformers/msmarco-distilbert-cos-v5\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: sentence-transformers/msmarco-distilbert-cos-v5\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: sentence-transformers/msmarco-distilbert-cos-v5\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: sentence-transformers/msmarco-distilbert-cos-v5\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: sentence-transformers/msmarco-distilbert-cos-v5\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: sentence-transformers/msmarco-distilbert-cos-v5\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: sentence-transformers/msmarco-distilbert-cos-v5\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: sentence-transformers/msmarco-distilbert-cos-v5\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: sentence-transformers/msmarco-distilbert-cos-v5\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: sentence-transformers/msmarco-distilbert-cos-v5\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: sentence-transformers/msmarco-distilbert-cos-v5\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: sentence-transformers/msmarco-distilbert-cos-v5\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: sentence-transformers/msmarco-distilbert-cos-v5\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: sentence-transformers/msmarco-distilbert-cos-v5\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: sentence-transformers/msmarco-distilbert-cos-v5\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: sentence-transformers/msmarco-distilbert-cos-v5\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: sentence-transformers/msmarco-distilbert-cos-v5\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: sentence-transformers/msmarco-distilbert-cos-v5\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: sentence-transformers/msmarco-distilbert-cos-v5\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: sentence-transformers/msmarco-distilbert-cos-v5\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: sentence-transformers/msmarco-distilbert-cos-v5\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: sentence-transformers/msmarco-distilbert-cos-v5\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: sentence-transformers/msmarco-distilbert-cos-v5\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: sentence-transformers/msmarco-distilbert-cos-v5\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: sentence-transformers/msmarco-distilbert-cos-v5\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: sentence-transformers/msmarco-distilbert-cos-v5\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "[Encoder] Loading model: sentence-transformers/msmarco-distilbert-cos-v5\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "   Best: threshold=0.15, exponent=1.00\n",
      "   P@5: 17.9% | R@5: 25.7% | F1: 21.1%\n",
      "\n",
      "======================================================================\n",
      "RESULTS (sorted by F1)\n",
      "======================================================================\n",
      "Model                                      Thresh   Exp    P@5    R@5     F1\n",
      "---------------------------------------------------------------------------\n",
      "all-mpnet-base-v2                            0.15  1.00  35.9%  52.8%  42.7%\n",
      "multi-qa-mpnet-base-dot-v1                   0.15  1.00  34.4%  50.6%  40.9%\n",
      "all-MiniLM-L12-v2                            0.15  1.00  29.5%  43.7%  35.2%\n",
      "paraphrase-mpnet-base-v2                     0.15  1.00  28.3%  41.3%  33.5%\n",
      "paraphrase-MiniLM-L6-v2                      0.15  1.00  25.1%  36.1%  29.6%\n",
      "sentence-transformers/msmarco-distilbert-cos-v5   0.15  1.00  17.9%  25.7%  21.1%\n",
      "\n",
      "ðŸ† BEST: all-mpnet-base-v2 (threshold=0.15, exponent=1.0)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL: Multi-Model + Calibration Tuning (Using YOUR SemanticSymptomEncoder)\n",
    "# =============================================================================\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from itertools import product\n",
    "\n",
    "CANDIDATE_MODELS = [\n",
    "    \"multi-qa-mpnet-base-dot-v1\",\n",
    "    \"all-mpnet-base-v2\",\n",
    "    \"all-MiniLM-L12-v2\",\n",
    "    \"paraphrase-mpnet-base-v2\",\n",
    "    \"paraphrase-MiniLM-L6-v2\",\n",
    "    \"sentence-transformers/msmarco-distilbert-cos-v5\",\n",
    "]\n",
    "\n",
    "THRESHOLDS = [0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45]\n",
    "EXPONENTS = [1.0, 1.25, 1.5, 1.75, 2.0]\n",
    "\n",
    "# Load paraphrases\n",
    "paraphrase_path = project_root / \"data\" / \"symptom_paraphrases.json\"\n",
    "with open(paraphrase_path, 'r', encoding='utf-8') as f:\n",
    "    paraphrases = json.load(f)\n",
    "\n",
    "def evaluate_encoder(encoder, n_tests=200):\n",
    "    \"\"\"Evaluate YOUR encoder using paraphrases.\"\"\"\n",
    "    testable = [s for s in encoder.symptoms if s in paraphrases]\n",
    "    random.seed(42)\n",
    "    \n",
    "    precision_5, recall_5, mrr = [], [], []\n",
    "    \n",
    "    for _ in range(n_tests):\n",
    "        n = random.randint(2, 5)\n",
    "        gt_symptoms = set(random.sample(testable, min(n, len(testable))))\n",
    "        \n",
    "        phrases = [random.choice(paraphrases[s]) for s in gt_symptoms]\n",
    "        text = \". \".join(phrases)\n",
    "        \n",
    "        # Use YOUR encoder's methods\n",
    "        result = encoder.encode_symptoms(text)\n",
    "        top_5 = encoder.get_top_symptoms(result['symptom_vector'], top_k=5, threshold=0.0)\n",
    "        top_5_names = [s[0] for s in top_5]\n",
    "        \n",
    "        hits = len(set(top_5_names) & gt_symptoms)\n",
    "        precision_5.append(hits / 5)\n",
    "        recall_5.append(hits / len(gt_symptoms))\n",
    "        \n",
    "        for rank, (sym, _) in enumerate(top_5, 1):\n",
    "            if sym in gt_symptoms:\n",
    "                mrr.append(1.0 / rank)\n",
    "                break\n",
    "        else:\n",
    "            mrr.append(0.0)\n",
    "    \n",
    "    p, r = np.mean(precision_5), np.mean(recall_5)\n",
    "    return {'P@5': p, 'R@5': r, 'MRR': np.mean(mrr), 'F1': 2*p*r/(p+r+1e-8)}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MULTI-MODEL + CALIBRATION TUNING (Using YOUR SemanticSymptomEncoder)\")\n",
    "print(f\"Testing {len(CANDIDATE_MODELS)} models Ã— {len(THRESHOLDS)} thresholds Ã— {len(EXPONENTS)} exponents\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for model_name in CANDIDATE_MODELS:\n",
    "    print(f\"\\nâ–¶ Testing: {model_name}\")\n",
    "    \n",
    "    best_score, best_params, best_metrics = 0, (0.25, 1.5), {}\n",
    "    \n",
    "    try:\n",
    "        start = time.time()\n",
    "        \n",
    "        for thresh, exp in product(THRESHOLDS, EXPONENTS):\n",
    "            # Create YOUR encoder with this model + calibration\n",
    "            encoder = SemanticSymptomEncoder(\n",
    "                model_name=model_name,\n",
    "                device='cpu',\n",
    "                threshold=thresh,\n",
    "                exponent=exp\n",
    "            )\n",
    "            \n",
    "            metrics = evaluate_encoder(encoder, n_tests=150)\n",
    "            \n",
    "            if metrics['F1'] > best_score:\n",
    "                best_score = metrics['F1']\n",
    "                best_params = (thresh, exp)\n",
    "                best_metrics = metrics\n",
    "            \n",
    "            del encoder\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        all_results.append({\n",
    "            'model': model_name,\n",
    "            'threshold': best_params[0],\n",
    "            'exponent': best_params[1],\n",
    "            **best_metrics,\n",
    "            'time': elapsed\n",
    "        })\n",
    "        \n",
    "        print(f\"   Best: threshold={best_params[0]:.2f}, exponent={best_params[1]:.2f}\")\n",
    "        print(f\"   P@5: {best_metrics['P@5']*100:.1f}% | R@5: {best_metrics['R@5']*100:.1f}% | F1: {best_metrics['F1']*100:.1f}%\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Failed: {e}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RESULTS (sorted by F1)\")\n",
    "print(\"=\" * 70)\n",
    "results_sorted = sorted(all_results, key=lambda x: x['F1'], reverse=True)\n",
    "print(f\"{'Model':<42} {'Thresh':>6} {'Exp':>5} {'P@5':>6} {'R@5':>6} {'F1':>6}\")\n",
    "print(\"-\" * 75)\n",
    "for r in results_sorted:\n",
    "    print(f\"{r['model']:<42} {r['threshold']:>6.2f} {r['exponent']:>5.2f} {r['P@5']*100:>5.1f}% {r['R@5']*100:>5.1f}% {r['F1']*100:>5.1f}%\")\n",
    "\n",
    "BEST = results_sorted[0]\n",
    "print(f\"\\nðŸ† BEST: {BEST['model']} (threshold={BEST['threshold']}, exponent={BEST['exponent']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DETAILED EVALUATION: all-mpnet-base-v2\n",
      "======================================================================\n",
      "[Encoder] Loading model: all-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "\n",
      "ðŸ“Š Evaluation on BASE Data (3000 samples):\n",
      "  Samples: 1846\n",
      "  Precision@5:  34.08%\n",
      "  Precision@10: 22.36%\n",
      "  Recall@5:     60.96%\n",
      "  Recall@10:    78.12%\n",
      "  MRR:          0.7128\n",
      "\n",
      "======================================================================\n",
      "QUALITATIVE EXAMPLES\n",
      "======================================================================\n",
      "\n",
      "ðŸ“ \"my head is killing me and I feel like throwing up\"\n",
      "   â€¢ vomiting: 0.197\n",
      "   â€¢ nausea and vomiting: 0.169\n",
      "   â€¢ nausea: 0.168\n",
      "   â€¢ vomiting blood: 0.135\n",
      "   â€¢ ache all over: 0.124\n",
      "\n",
      "ðŸ“ \"I can't breathe properly and my chest feels tight\"\n",
      "   â€¢ hurts to breath: 0.272\n",
      "   â€¢ difficulty breathing: 0.255\n",
      "   â€¢ breathing fast: 0.242\n",
      "   â€¢ chest pain: 0.226\n",
      "   â€¢ congestion in chest: 0.221\n",
      "\n",
      "ðŸ“ \"burning when I pee and having to go frequently\"\n",
      "   â€¢ painful urination: 0.277\n",
      "   â€¢ symptoms of bladder: 0.265\n",
      "   â€¢ frequent urination: 0.236\n",
      "   â€¢ excessive urination at night: 0.220\n",
      "   â€¢ symptoms of the kidneys: 0.167\n",
      "\n",
      "ðŸ“ \"I've been feeling really sad and can't sleep at night\"\n",
      "   â€¢ depression: 0.197\n",
      "   â€¢ sleep problems: 0.178\n",
      "   â€¢ nightmares: 0.166\n",
      "   â€¢ depressive or psychotic symptoms: 0.164\n",
      "   â€¢ sleepiness: 0.141\n",
      "\n",
      "ðŸ“ \"my joints ache and I'm exhausted all the time\"\n",
      "   â€¢ ache all over: 0.231\n",
      "   â€¢ fatigue: 0.211\n",
      "   â€¢ joint pain: 0.183\n",
      "   â€¢ back weakness: 0.171\n",
      "   â€¢ stiffness all over: 0.166\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL: Detailed Evaluation with Best Configuration\n",
    "# =============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(f\"DETAILED EVALUATION: {BEST['model']}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create encoder with BEST settings\n",
    "encoder = SemanticSymptomEncoder(\n",
    "    model_name=BEST['model'],\n",
    "    device='cpu',\n",
    "    threshold=BEST['threshold'],\n",
    "    exponent=BEST['exponent']\n",
    ")\n",
    "\n",
    "# Large-scale evaluation on BASE data\n",
    "print(\"\\nðŸ“Š Evaluation on BASE Data (3000 samples):\")\n",
    "\n",
    "sample_df = df_base_filtered.sample(n=3000, random_state=42)\n",
    "precision_5, recall_5, precision_10, recall_10, mrr = [], [], [], [], []\n",
    "\n",
    "for idx, row in sample_df.iterrows():\n",
    "    gt_symptoms = set()\n",
    "    phrases = []\n",
    "    \n",
    "    for col in cols_base:\n",
    "        sym = col.lower().replace('_', ' ')\n",
    "        if col in row and row[col] > 0.5 and sym in encoder.symptom_to_idx and sym in paraphrases:\n",
    "            gt_symptoms.add(sym)\n",
    "            phrases.append(random.choice(paraphrases[sym]))\n",
    "    \n",
    "    if len(gt_symptoms) < 2:\n",
    "        continue\n",
    "    \n",
    "    text = \". \".join(phrases)\n",
    "    result = encoder.encode_symptoms(text)\n",
    "    \n",
    "    top_10 = encoder.get_top_symptoms(result['symptom_vector'], top_k=10, threshold=0.0)\n",
    "    top_10_names = [s[0] for s in top_10]\n",
    "    top_5_names = top_10_names[:5]\n",
    "    \n",
    "    hits_5 = len(set(top_5_names) & gt_symptoms)\n",
    "    hits_10 = len(set(top_10_names) & gt_symptoms)\n",
    "    \n",
    "    precision_5.append(hits_5 / 5)\n",
    "    precision_10.append(hits_10 / 10)\n",
    "    recall_5.append(hits_5 / len(gt_symptoms))\n",
    "    recall_10.append(hits_10 / len(gt_symptoms))\n",
    "    \n",
    "    for rank, sym in enumerate(top_10_names, 1):\n",
    "        if sym in gt_symptoms:\n",
    "            mrr.append(1.0 / rank)\n",
    "            break\n",
    "    else:\n",
    "        mrr.append(0.0)\n",
    "\n",
    "print(f\"  Samples: {len(precision_5)}\")\n",
    "print(f\"  Precision@5:  {np.mean(precision_5)*100:.2f}%\")\n",
    "print(f\"  Precision@10: {np.mean(precision_10)*100:.2f}%\")\n",
    "print(f\"  Recall@5:     {np.mean(recall_5)*100:.2f}%\")\n",
    "print(f\"  Recall@10:    {np.mean(recall_10)*100:.2f}%\")\n",
    "print(f\"  MRR:          {np.mean(mrr):.4f}\")\n",
    "\n",
    "# Qualitative examples\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"QUALITATIVE EXAMPLES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "test_inputs = [\n",
    "    \"my head is killing me and I feel like throwing up\",\n",
    "    \"I can't breathe properly and my chest feels tight\",\n",
    "    \"burning when I pee and having to go frequently\",\n",
    "    \"I've been feeling really sad and can't sleep at night\",\n",
    "    \"my joints ache and I'm exhausted all the time\",\n",
    "]\n",
    "\n",
    "for text in test_inputs:\n",
    "    result = encoder.encode_symptoms(text)\n",
    "    top = encoder.get_top_symptoms(result['symptom_vector'], top_k=5, threshold=0.0)\n",
    "    print(f\"\\nðŸ“ \\\"{text}\\\"\")\n",
    "    for sym, score in top:\n",
    "        print(f\"   â€¢ {sym}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 10: End-to-End Pipeline Evaluation\n",
    " \n",
    "Tests the full pipeline: **Symptom text â†’ Encoder â†’ Hierarchical Classifier â†’ Predictions**\n",
    " \n",
    "Compares how using the encoder (vs pre-computed binary features) affects classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "END-TO-END PIPELINE EVALUATION\n",
      "======================================================================\n",
      "\n",
      "  Samples: 1248\n",
      "  Pipeline Top-1: 0.08%\n",
      "  Pipeline Top-5: 0.56%\n",
      "\n",
      "ðŸ“Š Comparison to Binary Features:\n",
      "  Binary Top-1: 81.87%\n",
      "  Pipeline Top-1: 0.08%\n",
      "  Degradation: 81.79%\n",
      "\n",
      "======================================================================\n",
      "FINAL RECOMMENDATION\n",
      "======================================================================\n",
      "  Model:     all-mpnet-base-v2\n",
      "  Threshold: 0.15\n",
      "  Exponent:  1.0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL: Full Pipeline (Encoder â†’ Hierarchical Classifier)\n",
    "# =============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"END-TO-END PIPELINE EVALUATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "sample_df = df_base_filtered.sample(n=2000, random_state=42)\n",
    "y_true, y_proba_all = [], []\n",
    "\n",
    "for idx, row in sample_df.iterrows():\n",
    "    active_cols = [col for col in cols_base if col in row and row[col] > 0.5]\n",
    "    phrases = []\n",
    "    \n",
    "    for col in active_cols:\n",
    "        sym = col.lower().replace('_', ' ')\n",
    "        if sym in paraphrases and paraphrases[sym]:\n",
    "            phrases.append(random.choice(paraphrases[sym]))\n",
    "    \n",
    "    if len(phrases) < 2:\n",
    "        continue\n",
    "    \n",
    "    text = \". \".join(phrases)\n",
    "    result = encoder.encode_symptoms(text)\n",
    "    \n",
    "    # Build classifier features\n",
    "    features = np.zeros(len(cols_base))\n",
    "    for i, col in enumerate(cols_base):\n",
    "        sym = col.lower().replace('_', ' ')\n",
    "        features[i] = encoder.get_symptom_score(result['symptom_vector'], sym)\n",
    "    \n",
    "    X_row = pd.DataFrame([features], columns=cols_base)\n",
    "    proba = predict_hierarchical(X_row, cat_clf_base, specialist_err, all_diseases_base)\n",
    "    \n",
    "    y_true.append(row['diseases'])\n",
    "    y_proba_all.append(proba[0])\n",
    "\n",
    "y_proba_all = np.array(y_proba_all)\n",
    "disease_to_idx = {d: i for i, d in enumerate(all_diseases_base)}\n",
    "y_true_idx = np.array([disease_to_idx.get(d, -1) for d in y_true])\n",
    "valid = y_true_idx != -1\n",
    "y_pred_idx = np.argmax(y_proba_all[valid], axis=1)\n",
    "\n",
    "pipeline_top1 = accuracy_score(y_true_idx[valid], y_pred_idx)\n",
    "pipeline_top5 = top_k_accuracy_score(y_true_idx[valid], y_proba_all[valid], k=5, labels=np.arange(len(all_diseases_base)))\n",
    "\n",
    "print(f\"\\n  Samples: {valid.sum()}\")\n",
    "print(f\"  Pipeline Top-1: {pipeline_top1*100:.2f}%\")\n",
    "print(f\"  Pipeline Top-5: {pipeline_top5*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Comparison to Binary Features:\")\n",
    "print(f\"  Binary Top-1: {results_base['Top-1']*100:.2f}%\")\n",
    "print(f\"  Pipeline Top-1: {pipeline_top1*100:.2f}%\")\n",
    "degradation = (results_base['Top-1'] - pipeline_top1) * 100\n",
    "print(f\"  Degradation: {degradation:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL RECOMMENDATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  Model:     {BEST['model']}\")\n",
    "print(f\"  Threshold: {BEST['threshold']}\")\n",
    "print(f\"  Exponent:  {BEST['exponent']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base symptoms: 375\n",
      "Augmented symptoms: 456\n",
      "Demo (demographic) features: 456\n"
     ]
    }
   ],
   "source": [
    "def extract_symptom_columns(df, non_feature_cols):\n",
    "    return [\n",
    "        c for c in df.columns\n",
    "        if c not in non_feature_cols\n",
    "    ]\n",
    "\n",
    "\n",
    "cols_base = extract_symptom_columns(df_base, non_feature_cols)\n",
    "cols_augmented = extract_symptom_columns(df_augmented, non_feature_cols)\n",
    "cols_demo = extract_symptom_columns(df_demo, non_feature_cols)\n",
    "\n",
    "print(\"Base symptoms:\", len(cols_base))\n",
    "print(\"Augmented symptoms:\", len(cols_augmented))\n",
    "print(\"Demo (demographic) features:\", len(cols_demo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base âˆ© Augmented: 375\n",
      "Base âˆ© Demo: 375\n",
      "Augmented âˆ© Demo: 456\n"
     ]
    }
   ],
   "source": [
    "def overlap(a, b):\n",
    "    return len(set(a) & set(b))\n",
    "\n",
    "print(\"Base âˆ© Augmented:\", overlap(cols_base, cols_augmented))\n",
    "print(\"Base âˆ© Demo:\", overlap(cols_base, cols_demo))\n",
    "print(\"Augmented âˆ© Demo:\", overlap(cols_augmented, cols_demo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_base_true: (206267, 375)\n",
      "X_augmented_true: (207518, 456)\n",
      "X_demo: (207518, 456)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Ground truth matrices\n",
    "# -----------------------------\n",
    "\n",
    "X_base_true = (\n",
    "    df_base[cols_base]\n",
    "    .fillna(0)\n",
    "    .astype(float)\n",
    "    .values\n",
    ")\n",
    "\n",
    "X_augmented_true = (\n",
    "    df_augmented[cols_augmented]\n",
    "    .fillna(0)\n",
    "    .astype(float)\n",
    "    .values\n",
    ")\n",
    "\n",
    "X_demo = (\n",
    "    df_demo[cols_demo]\n",
    "    .fillna(0)\n",
    "    .astype(float)\n",
    "    .values\n",
    ")\n",
    "\n",
    "print(\"X_base_true:\", X_base_true.shape)\n",
    "print(\"X_augmented_true:\", X_augmented_true.shape)\n",
    "print(\"X_demo:\", X_demo.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Encoder] Loading model: all-mpnet-base-v2\n",
      "[Encoder] Cached embeddings mismatch, recomputing\n",
      "[Encoder] Computing symptom embeddings (one-time)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:36<00:00,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Encoder] Initialized with 375 symptoms\n",
      "Encoder initialized with base vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "encoder_base = SemanticSymptomEncoder(\n",
    "    model_name=BEST[\"model\"],   # best you found\n",
    "    threshold=BEST[\"threshold\"],\n",
    "    exponent=BEST[\"exponent\"],\n",
    "    symptom_list=cols_base,           # ðŸ”¥ dataset-derived vocab\n",
    ")\n",
    "\n",
    "assert len(encoder_base.symptoms) == len(cols_base)\n",
    "print(\"Encoder initialized with base vocab\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_col_to_symptom_map(symptom_cols, encoder_symptoms):\n",
    "    \"\"\"\n",
    "    Enforces 1:1 alignment between dataset columns and encoder vocab.\n",
    "    \"\"\"\n",
    "    enc_set = set(encoder_symptoms)\n",
    "    mapping = {}\n",
    "\n",
    "    for col in symptom_cols:\n",
    "        norm = col.lower().replace(\"_\", \" \")\n",
    "        if norm in enc_set:\n",
    "            mapping[col] = norm\n",
    "        else:\n",
    "            # fail fast â€“ do NOT silently ignore\n",
    "            raise ValueError(f\"Unmapped symptom column: {col}\")\n",
    "\n",
    "    print(f\"âœ… Mapped {len(mapping)} symptoms\")\n",
    "    return mapping\n",
    "\n",
    "\n",
    "col_to_symptom = build_col_to_symptom_map(\n",
    "    cols_base,\n",
    "    encoder.symptoms\n",
    ")\n",
    "\n",
    "def symptoms_to_text(active_symptoms):\n",
    "    \"\"\"\n",
    "    Deterministic, controlled text generation.\n",
    "    \"\"\"\n",
    "    return \". \".join(active_symptoms)\n",
    "\n",
    "def evaluate_user_in_loop(\n",
    "    encoder,\n",
    "    X_true,                # binary symptom matrix\n",
    "    y_true,                # disease labels\n",
    "    symptom_cols,\n",
    "    col_to_symptom,\n",
    "    predict_fn,            # predict_hierarchical\n",
    "    cat_clf,\n",
    "    specialist_models,\n",
    "    all_diseases,\n",
    "    top_k=20,\n",
    "    max_samples=3000\n",
    "):\n",
    "    idxs = np.random.choice(\n",
    "        len(X_true),\n",
    "        size=min(max_samples, len(X_true)),\n",
    "        replace=False\n",
    "    )\n",
    "\n",
    "    y_true_idx = []\n",
    "    y_pred_proba = []\n",
    "\n",
    "    disease_to_idx = {d: i for i, d in enumerate(all_diseases)}\n",
    "\n",
    "    for i in idxs:\n",
    "        # 1. Ground truth symptoms\n",
    "        active_cols = [\n",
    "            col for j, col in enumerate(symptom_cols)\n",
    "            if X_true[i, j] == 1.0\n",
    "        ]\n",
    "\n",
    "        if len(active_cols) < 2:\n",
    "            continue\n",
    "\n",
    "        active_syms = [col_to_symptom[c] for c in active_cols]\n",
    "\n",
    "        # 2. Generate text\n",
    "        text = symptoms_to_text(active_syms)\n",
    "\n",
    "        # 3. Encode & rank\n",
    "        evidence = encoder.encode_symptoms(text)[\"symptom_vector\"]\n",
    "        top_idx = np.argsort(evidence)[::-1][:top_k]\n",
    "        top_syms = {encoder.symptoms[j] for j in top_idx}\n",
    "\n",
    "        # 4. Simulate user confirmation\n",
    "        X_sim = np.zeros(len(symptom_cols))\n",
    "        for j, col in enumerate(symptom_cols):\n",
    "            sym = col_to_symptom[col]\n",
    "            if sym in top_syms and sym in active_syms:\n",
    "                X_sim[j] = 1.0\n",
    "\n",
    "        # 5. Predict\n",
    "        proba = predict_fn(\n",
    "            X_sim.reshape(1, -1),\n",
    "            cat_clf,\n",
    "            specialist_models,\n",
    "            all_diseases\n",
    "        )[0]\n",
    "\n",
    "        y_pred_proba.append(proba)\n",
    "        y_true_idx.append(disease_to_idx[y_true[i]])\n",
    "\n",
    "    y_pred_proba = np.array(y_pred_proba)\n",
    "    y_true_idx = np.array(y_true_idx)\n",
    "\n",
    "    top1 = accuracy_score(y_true_idx, np.argmax(y_pred_proba, axis=1))\n",
    "    top5 = top_k_accuracy_score(\n",
    "        y_true_idx,\n",
    "        y_pred_proba,\n",
    "        k=5,\n",
    "        labels=np.arange(len(all_diseases))\n",
    "    )\n",
    "\n",
    "    return top1, top5, len(y_true_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"FINAL PIPELINE EVALUATION (USER-IN-THE-LOOP)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "top1, top5, n = evaluate_user_in_loop(\n",
    "    encoder=encoder,\n",
    "    X_true=X_base_true,\n",
    "    y_true=df_base_filtered[\"diseases\"].values,\n",
    "    symptom_cols=cols_base,\n",
    "    col_to_symptom=col_to_symptom,\n",
    "    predict_fn=predict_hierarchical,\n",
    "    cat_clf=cat_clf_base,\n",
    "    specialist_models=specialist_err,\n",
    "    all_diseases=all_diseases_base,\n",
    "    top_k=20,\n",
    "    max_samples=2500\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Samples Evaluated: 2499\n",
      "Pipeline Top-1 Accuracy: 74.15%\n",
      "Pipeline Top-5 Accuracy: 86.19%\n",
      "\n",
      "Gold Standard Top-1: 81.87%\n",
      "Degradation: 7.72%\n",
      "âš ï¸ ACCEPTABLE\n"
     ]
    }
   ],
   "source": [
    "print(\"-\" * 50)\n",
    "print(f\"Samples Evaluated: {n}\")\n",
    "print(f\"Pipeline Top-1 Accuracy: {top1*100:.2f}%\")\n",
    "print(f\"Pipeline Top-5 Accuracy: {top5*100:.2f}%\")\n",
    "\n",
    "baseline = results_base[\"Top-1\"]\n",
    "degradation = (baseline - top1) * 100\n",
    "\n",
    "print(f\"\\nGold Standard Top-1: {baseline*100:.2f}%\")\n",
    "print(f\"Degradation: {degradation:.2f}%\")\n",
    "\n",
    "if degradation < 5:\n",
    "    print(\"âœ… EXCELLENT\")\n",
    "elif degradation < 15:\n",
    "    print(\"âš ï¸ ACCEPTABLE\")\n",
    "else:\n",
    "    print(\"âŒ NEEDS IMPROVEMENT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Mapped 456 symptoms\n"
     ]
    }
   ],
   "source": [
    "col_to_symptom_demo = build_col_to_symptom_map(\n",
    "    cols_demo,\n",
    "    encoder.symptoms\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training Category Classifier...\n",
      "Training SymptomCategoryClassifier with shape (186742, 456)\n",
      "  Training 17 Specialist Models (parallel, n_jobs=3)...\n"
     ]
    }
   ],
   "source": [
    "# Train specialist models for demographics evaluation\n",
    "cat_clf_demo_err, specialist_demo = train_hierarchical_model(\n",
    "    X_train_demo, y_train_demo, df_train_demo\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "top1_demo, top5_demo, n_demo = evaluate_user_in_loop(\n",
    "    encoder=encoder,\n",
    "    X_true=df_demo_filtered[cols_demo].fillna(0).values,\n",
    "    y_true=df_demo_filtered[\"diseases\"].values,\n",
    "    symptom_cols=cols_demo,\n",
    "    col_to_symptom=col_to_symptom_demo,\n",
    "    predict_fn=predict_hierarchical,\n",
    "    cat_clf=cat_clf_demo,\n",
    "    specialist_models=specialist_demo,\n",
    "    all_diseases=all_diseases_demo,\n",
    "    top_k=20,\n",
    "    max_samples=2500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Samples Evaluated: 2499\n",
      "Pipeline Top-1 Accuracy: 83.81%\n",
      "Pipeline Top-5 Accuracy: 97.64%\n",
      "\n",
      "Gold Standard Top-1: 81.87%\n",
      "Degradation: -1.95%\n",
      "âœ… EXCELLENT\n"
     ]
    }
   ],
   "source": [
    "print(\"-\" * 50)\n",
    "print(f\"Samples Evaluated: {n}\")\n",
    "print(f\"Pipeline Top-1 Accuracy: {top1_demo*100:.2f}%\")\n",
    "print(f\"Pipeline Top-5 Accuracy: {top5_demo*100:.2f}%\")\n",
    "\n",
    "baseline = results_base[\"Top-1\"]\n",
    "degradation = (baseline - top1_demo) * 100\n",
    "\n",
    "print(f\"\\nGold Standard Top-1: {baseline*100:.2f}%\")\n",
    "print(f\"Degradation: {degradation:.2f}%\")\n",
    "\n",
    "if degradation < 5:\n",
    "    print(\"âœ… EXCELLENT\")\n",
    "elif degradation < 15:\n",
    "    print(\"âš ï¸ ACCEPTABLE\")\n",
    "else:\n",
    "    print(\"âŒ NEEDS IMPROVEMENT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All results saved to: c:\\Users\\henry\\Desktop\\Programming\\Python\\Multimodal_Diagnosis\\notebooks\\figures\\rigorous_eval_results.json\n",
      "   - CV results (base, augmented, demographics)\n",
      "   - Encoder comparison (6 configs tested)\n",
      "   - Pipeline evaluation (user-in-the-loop)\n"
     ]
    }
   ],
   "source": [
    "# === Save ALL Encoder & Pipeline Results ===\n",
    "results_path = project_root / 'notebooks' / 'figures' / 'rigorous_eval_results.json'\n",
    "\n",
    "# Load existing CV results\n",
    "with open(results_path, 'r') as f:\n",
    "    existing = json.load(f)\n",
    "\n",
    "# Add encoder comparison + full pipeline results\n",
    "existing['encoder_comparison'] = {\n",
    "    'all_results': all_results,           # 6 models Ã— grid search\n",
    "    'best_config': BEST,                  # Best model/threshold/exponent\n",
    "}\n",
    "\n",
    "existing['pipeline_user_in_loop'] = {\n",
    "    'base_dataset': {\n",
    "        'top1': top1,\n",
    "        'top5': top5,\n",
    "        'samples': n,\n",
    "        'gold_standard_top1': results_base['Top-1'],\n",
    "        'degradation': (results_base['Top-1'] - top1) * 100\n",
    "    },\n",
    "    'augmented + demographics': {\n",
    "        'top1': top1_demo,\n",
    "        'top5': top5_demo,\n",
    "        'samples': n_demo,\n",
    "        'gold_standard_top1': results_base['Top-1'],\n",
    "        'degradation': (results_base['Top-1'] - top1_demo) * 100\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(existing, f, indent=2, default=float)\n",
    "\n",
    "print(f\"âœ… All results saved to: {results_path}\")\n",
    "print(f\"   - CV results (base, augmented, demographics)\")\n",
    "print(f\"   - Encoder comparison ({len(all_results)} configs tested)\")\n",
    "print(f\"   - Pipeline evaluation (user-in-the-loop)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-populate with completed results\n",
    "quick_results = [\n",
    "    {'threshold': 0.10, 'exponent': 0.5, 'top1': 0.823},\n",
    "    {'threshold': 0.10, 'exponent': 1.0, 'top1': 0.838},\n",
    "    {'threshold': 0.10, 'exponent': 1.5, 'top1': 0.816},\n",
    "    {'threshold': 0.10, 'exponent': 2.0, 'top1': 0.846},\n",
    "    {'threshold': 0.10, 'exponent': 2.5, 'top1': 0.850},\n",
    "    {'threshold': 0.15, 'exponent': 0.5, 'top1': 0.864},\n",
    "    {'threshold': 0.15, 'exponent': 1.0, 'top1': 0.820},\n",
    "    {'threshold': 0.15, 'exponent': 1.5, 'top1': 0.844},\n",
    "    {'threshold': 0.15, 'exponent': 2.0, 'top1': 0.815},\n",
    "    {'threshold': 0.15, 'exponent': 2.5, 'top1': 0.856},\n",
    "    {'threshold': 0.20, 'exponent': 0.5, 'top1': 0.850},\n",
    "    {'threshold': 0.20, 'exponent': 1.0, 'top1': 0.844},\n",
    "    {'threshold': 0.20, 'exponent': 1.5, 'top1': 0.827},\n",
    "    {'threshold': 0.20, 'exponent': 2.0, 'top1': 0.789},\n",
    "    {'threshold': 0.20, 'exponent': 2.5, 'top1': 0.832},\n",
    "    {'threshold': 0.25, 'exponent': 0.5, 'top1': 0.823},\n",
    "    {'threshold': 0.25, 'exponent': 1.0, 'top1': 0.829},\n",
    "    {'threshold': 0.25, 'exponent': 1.5, 'top1': 0.836},\n",
    "    {'threshold': 0.25, 'exponent': 2.0, 'top1': 0.808},\n",
    "    {'threshold': 0.25, 'exponent': 2.5, 'top1': 0.802},\n",
    "    {'threshold': 0.30, 'exponent': 0.5, 'top1': 0.824},\n",
    "    {'threshold': 0.30, 'exponent': 1.0, 'top1': 0.842},\n",
    "    {'threshold': 0.30, 'exponent': 1.5, 'top1': 0.852},\n",
    "    {'threshold': 0.30, 'exponent': 2.0, 'top1': 0.833},\n",
    "    {'threshold': 0.30, 'exponent': 2.5, 'top1': 0.815},\n",
    "    {'threshold': 0.35, 'exponent': 0.5, 'top1': 0.838},\n",
    "    {'threshold': 0.35, 'exponent': 1.0, 'top1': 0.837},\n",
    "    {'threshold': 0.35, 'exponent': 1.5, 'top1': 0.801},\n",
    "    {'threshold': 0.35, 'exponent': 2.0, 'top1': 0.856},\n",
    "    {'threshold': 0.35, 'exponent': 2.5, 'top1': 0.860},\n",
    "    {'threshold': 0.40, 'exponent': 0.5, 'top1': 0.872},\n",
    "    {'threshold': 0.40, 'exponent': 1.0, 'top1': 0.826},\n",
    "    {'threshold': 0.40, 'exponent': 1.5, 'top1': 0.861},\n",
    "    {'threshold': 0.40, 'exponent': 2.0, 'top1': 0.838},\n",
    "    {'threshold': 0.40, 'exponent': 2.5, 'top1': 0.829},\n",
    "    {'threshold': 0.45, 'exponent': 0.5, 'top1': 0.830},\n",
    "    {'threshold': 0.45, 'exponent': 1.0, 'top1': 0.817},\n",
    "    {'threshold': 0.45, 'exponent': 1.5, 'top1': 0.834},\n",
    "    {'threshold': 0.45, 'exponent': 2.0, 'top1': 0.802},\n",
    "    {'threshold': 0.45, 'exponent': 2.5, 'top1': 0.856},\n",
    "    {'threshold': 0.50, 'exponent': 0.5, 'top1': 0.818},\n",
    "    {'threshold': 0.50, 'exponent': 1.0, 'top1': 0.822},\n",
    "]\n",
    "\n",
    "# Mark what's already done\n",
    "completed = {(r['threshold'], r['exponent']) for r in quick_results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Encoder] Loading model: all-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "Training hierarchical models...\n",
      "  Training Category Classifier...\n",
      "Training SymptomCategoryClassifier with shape (186742, 456)\n",
      "  Training 17 Specialist Models (parallel, n_jobs=3)...\n",
      "Models trained!\n",
      "\n",
      "ðŸ† Best: threshold=0.4, exponent=0.5, Top-1=87.2%\n"
     ]
    }
   ],
   "source": [
    "from models.architectures.semantic_symptom_encoder import SemanticSymptomEncoder\n",
    "\n",
    "non_feature_cols = ['diseases', 'disease_category', 'symptoms', 'age', 'sex', 'age_normalized', 'sex_encoded']\n",
    "# --- 2. Prepare features ---\n",
    "feature_cols_demo = [c for c in df_demo_filtered.columns if c not in non_feature_cols]\n",
    "cols_demo = feature_cols_demo\n",
    "X_demo_values = df_demo_filtered[cols_demo].fillna(0).values\n",
    "y_demo_values = df_demo_filtered[\"diseases\"].values\n",
    "all_diseases_demo = np.unique(y_demo_values)\n",
    "# --- 3. Build symptom mapping (needs an encoder first) ---\n",
    "temp_encoder = SemanticSymptomEncoder(model_name=\"all-mpnet-base-v2\", symptom_list=cols_demo)\n",
    "col_to_symptom_demo = {col: col.lower().replace(\"_\", \" \") for col in cols_demo}\n",
    "# --- 4. Train models (this is the slow part, ~2-3 min) ---\n",
    "print(\"Training hierarchical models...\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "indices_demo = np.arange(len(df_demo_filtered))\n",
    "train_idx_demo, test_idx_demo = train_test_split(\n",
    "    indices_demo, test_size=0.1, random_state=42, stratify=y_demo_values\n",
    ")\n",
    "X_train_demo = df_demo_filtered[cols_demo].iloc[train_idx_demo]\n",
    "y_train_demo = y_demo_values[train_idx_demo]\n",
    "df_train_demo = df_demo_filtered.iloc[train_idx_demo]\n",
    "cat_clf_demo, specialist_demo = train_hierarchical_model(X_train_demo, y_train_demo, df_train_demo)\n",
    "print(\"Models trained!\")\n",
    "# --- 5. Run the sweep ---\n",
    "BEST_MODEL = \"all-mpnet-base-v2\"\n",
    "THRESHOLDS = [0.35, 0.40, 0.45]  # Focus around best region\n",
    "EXPONENTS = [0.5, 1.0]\n",
    "\n",
    "for thresh in THRESHOLDS:\n",
    "    for exp in EXPONENTS:\n",
    "        if (thresh, exp) in completed:\n",
    "            continue\n",
    "        \n",
    "        encoder = SemanticSymptomEncoder(\n",
    "            model_name=BEST_MODEL,\n",
    "            threshold=thresh,\n",
    "            exponent=exp,\n",
    "            symptom_list=cols_demo\n",
    "        )\n",
    "        \n",
    "        top1, top5, n = evaluate_user_in_loop(\n",
    "            encoder=encoder,\n",
    "            X_true=X_demo_values,\n",
    "            y_true=y_demo_values,\n",
    "            symptom_cols=cols_demo,\n",
    "            col_to_symptom=col_to_symptom_demo,\n",
    "            predict_fn=predict_hierarchical,\n",
    "            cat_clf=cat_clf_demo,\n",
    "            specialist_models=specialist_demo,\n",
    "            all_diseases=all_diseases_demo,\n",
    "            top_k=20,\n",
    "            max_samples=2500  # Full sample for reliable numbers\n",
    "        )\n",
    "        \n",
    "        quick_results.append({\n",
    "            'threshold': thresh,\n",
    "            'exponent': exp,\n",
    "            'top1': top1,\n",
    "            'top5': top5\n",
    "        })\n",
    "        print(f\"thresh={thresh}, exp={exp}: Top-1={top1*100:.1f}%, Top-5={top5*100:.1f}%\")\n",
    "best = max(quick_results, key=lambda x: x['top1'])\n",
    "print(f\"\\nðŸ† Best: threshold={best['threshold']}, exponent={best['exponent']}, Top-1={best['top1']*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running full evaluation for best config: threshold=0.4, exponent=0.5\n",
      "[Encoder] Loading model: all-mpnet-base-v2\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "\n",
      "ðŸ† BEST CONFIG RESULTS\n",
      "   Threshold: 0.4\n",
      "   Exponent:  0.5\n",
      "   Samples:   2496\n",
      "   Top-1:     84.01%\n",
      "   Top-5:     97.64%\n",
      "\n",
      "âœ… Results saved to: c:\\Users\\henry\\Desktop\\Programming\\Python\\Multimodal_Diagnosis\\notebooks\\figures\\rigorous_eval_results.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# RUN BEST CONFIG WITH FULL SAMPLES & SAVE\n",
    "# ============================================\n",
    "\n",
    "# Get best from sweep\n",
    "BEST_THRESH = best['threshold']\n",
    "BEST_EXP = best['exponent']\n",
    "\n",
    "print(f\"Running full evaluation for best config: threshold={BEST_THRESH}, exponent={BEST_EXP}\")\n",
    "\n",
    "encoder_best = SemanticSymptomEncoder(\n",
    "    model_name=\"all-mpnet-base-v2\",\n",
    "    threshold=BEST_THRESH,\n",
    "    exponent=BEST_EXP,\n",
    "    symptom_list=cols_demo\n",
    ")\n",
    "\n",
    "top1_final, top5_final, n_final = evaluate_user_in_loop(\n",
    "    encoder=encoder_best,\n",
    "    X_true=X_demo_values,\n",
    "    y_true=y_demo_values,\n",
    "    symptom_cols=cols_demo,\n",
    "    col_to_symptom=col_to_symptom_demo,\n",
    "    predict_fn=predict_hierarchical,\n",
    "    cat_clf=cat_clf_demo,\n",
    "    specialist_models=specialist_demo,\n",
    "    all_diseases=all_diseases_demo,\n",
    "    top_k=20,\n",
    "    max_samples=2500  # Full sample\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ† BEST CONFIG RESULTS\")\n",
    "print(f\"   Threshold: {BEST_THRESH}\")\n",
    "print(f\"   Exponent:  {BEST_EXP}\")\n",
    "print(f\"   Samples:   {n_final}\")\n",
    "print(f\"   Top-1:     {top1_final*100:.2f}%\")\n",
    "print(f\"   Top-5:     {top5_final*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Results saved to: c:\\Users\\henry\\Desktop\\Programming\\Python\\Multimodal_Diagnosis\\notebooks\\figures\\rigorous_eval_results.json\n"
     ]
    }
   ],
   "source": [
    "results_path = project_root / 'notebooks' / 'figures' / 'rigorous_eval_results.json'\n",
    "\n",
    "with open(results_path, 'r') as f:\n",
    "    existing = json.load(f)\n",
    "\n",
    "existing['pipeline_user_in_loop']['optimized_demo'] = {\n",
    "    'threshold': BEST_THRESH,\n",
    "    'exponent': BEST_EXP,\n",
    "    'top1': top1_final,\n",
    "    'top5': top5_final,\n",
    "    'samples': n_final,\n",
    "    \"gold_standard_top1\": 0.8186536036472984,\n",
    "    'degradation': (0.8186536036472984 - top1_final) * 100,\n",
    "    'model': 'all-mpnet-base-v2'\n",
    "}\n",
    "\n",
    "existing['threshold_sweep'] = quick_results\n",
    "\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(existing, f, indent=2, default=float)\n",
    "\n",
    "print(f\"\\nâœ… Results saved to: {results_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
