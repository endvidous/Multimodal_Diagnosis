{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rigorous Evaluation for Research Paper\n",
    "\n",
    "This notebook provides:\n",
    "1. **5-Fold Cross-Validation** - Statistical validity of results\n",
    "2. **Ablation Studies** - Contribution of each component (augmentation, demographics)\n",
    "3. **Encoder Comparison** - Testing different sentence transformer models\n",
    "4. **User-in-Loop Pipeline** - End-to-end evaluation with user confirmation\n",
    "5. **Error Analysis** - Confusion patterns\n",
    "\n",
    "**Note**: Uses checkpointing to resume interrupted runs. Delete `checkpoints/*.pkl` to restart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume utilities loaded. Checkpoints stored in: c:\\Users\\henry\\Desktop\\Programming\\Python\\Multimodal_Diagnosis\\notebooks\\checkpoints\n"
     ]
    }
   ],
   "source": [
    "# ==== Resume / Checkpoint Utilities ====\n",
    "import os, pickle\n",
    "from pathlib import Path\n",
    "\n",
    "CKPT_DIR = Path('checkpoints')\n",
    "CKPT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def get_cv_state_file(cv_name: str) -> Path:\n",
    "    return CKPT_DIR / f'cv_state_{cv_name}.pkl'\n",
    "\n",
    "def load_cv_state(cv_name: str) -> dict:\n",
    "    state_file = get_cv_state_file(cv_name)\n",
    "    if state_file.exists():\n",
    "        with open(state_file, 'rb') as f:\n",
    "            state = pickle.load(f)\n",
    "            print(f\"  Loaded checkpoint for '{cv_name}': {len(state.get('fold_results', {}))} folds completed\")\n",
    "            return state\n",
    "    return {'fold_results': {}}\n",
    "\n",
    "def save_cv_state(cv_name: str, state: dict):\n",
    "    with open(get_cv_state_file(cv_name), 'wb') as f:\n",
    "        pickle.dump(state, f)\n",
    "\n",
    "def clear_cv_state(cv_name: str):\n",
    "    state_file = get_cv_state_file(cv_name)\n",
    "    if state_file.exists():\n",
    "        os.remove(state_file)\n",
    "        print(f\"  Cleared checkpoint for '{cv_name}'\")\n",
    "\n",
    "print('Resume utilities loaded. Checkpoints stored in:', CKPT_DIR.absolute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\Users\\henry\\Desktop\\Programming\\Python\\Multimodal_Diagnosis\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json, joblib, warnings, sys, os, time\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "project_root = Path(os.getcwd()).parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (accuracy_score, top_k_accuracy_score,\n",
    "    confusion_matrix, f1_score)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "from models.architectures.symptom_classifier import SymptomCategoryClassifier, SymptomDiseaseClassifier\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base dataset: 206,267 rows, 627 diseases\n",
      "Augmented dataset: 207,518 rows, 627 diseases\n",
      "Augmented + Demographics: 207,518 rows, 627 diseases\n"
     ]
    }
   ],
   "source": [
    "# Data paths\n",
    "base_data_path = project_root / \"data\" / \"processed\" / \"symptoms\" / \"symptoms_to_disease_cleaned.csv\"\n",
    "augmented_no_demo_path = project_root / \"data\" / \"processed\" / \"symptoms\" / \"symptoms_augmented_no_demographics.csv\"\n",
    "augmented_with_demo_path = project_root / \"data\" / \"processed\" / \"symptoms\" / \"symptoms_augmented_with_demographics.csv\"\n",
    "\n",
    "# Symptom vocabulary\n",
    "with open(project_root / \"data\" / \"symptom_vocabulary.json\") as f:\n",
    "    symptom_cols = json.load(f)\n",
    "\n",
    "# Load datasets\n",
    "df_base = pd.read_csv(base_data_path)\n",
    "df_augmented = pd.read_csv(augmented_no_demo_path)\n",
    "df_demo = pd.read_csv(augmented_with_demo_path)\n",
    "\n",
    "print(f\"Base dataset: {len(df_base):,} rows, {df_base['diseases'].nunique()} diseases\")\n",
    "print(f\"Augmented dataset: {len(df_augmented):,} rows, {df_augmented['diseases'].nunique()} diseases\")\n",
    "print(f\"Augmented + Demographics: {len(df_demo):,} rows, {df_demo['diseases'].nunique()} diseases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 206,267 rows, 627 diseases\n",
      "Filtered: 206,173 rows, 588 diseases\n",
      "Original: 207,518 rows, 627 diseases\n",
      "Filtered: 207,492 rows, 616 diseases\n",
      "Original: 207,518 rows, 627 diseases\n",
      "Filtered: 207,492 rows, 616 diseases\n"
     ]
    }
   ],
   "source": [
    "def filter_min_samples(df, min_samples=5):\n",
    "    \"\"\"Filter to keep only diseases with at least min_samples.\"\"\"\n",
    "    disease_counts = df['diseases'].value_counts()\n",
    "    valid_diseases = disease_counts[disease_counts >= min_samples].index\n",
    "    df_filtered = df[df['diseases'].isin(valid_diseases)].copy()\n",
    "    \n",
    "    print(f\"Original: {len(df):,} rows, {df['diseases'].nunique()} diseases\")\n",
    "    print(f\"Filtered: {len(df_filtered):,} rows, {df_filtered['diseases'].nunique()} diseases\")\n",
    "    return df_filtered\n",
    "\n",
    "# Filter for stable evaluation\n",
    "df_base_filtered = filter_min_samples(df_base)\n",
    "df_augmented_filtered = filter_min_samples(df_augmented)\n",
    "df_demo_filtered = filter_min_samples(df_demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Model Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature columns\n",
    "non_feature_cols = ['diseases', 'disease_category', 'symptoms', 'age', 'sex', 'age_normalized', 'sex_encoded']\n",
    "\n",
    "def prepare_features(df, feature_cols):\n",
    "    \"\"\"Prepare features and labels from dataframe.\"\"\"\n",
    "    available_cols = [c for c in feature_cols if c in df.columns]\n",
    "    X = df[available_cols]\n",
    "    y = df['diseases'].values\n",
    "    return X, y, available_cols\n",
    "\n",
    "def _train_specialist(category, X_train, y_train, y_categories):\n",
    "    \"\"\"Train a single specialist model (for parallel execution).\"\"\"\n",
    "    mask = y_categories == category\n",
    "    X_cat = X_train[mask]\n",
    "    y_cat = y_train[mask]\n",
    "\n",
    "    if len(y_cat) < 5:\n",
    "        return None, None\n",
    "\n",
    "    model = SymptomDiseaseClassifier(category=category, n_estimators=100)\n",
    "    model.fit(X_cat, y_cat)\n",
    "    return category, model\n",
    "\n",
    "def train_hierarchical_model(X_train, y_train, df_train_full, n_jobs=None):\n",
    "    \"\"\"Train hierarchical model (Category -> Specialist Disease Classifiers).\"\"\"\n",
    "    if n_jobs is None:\n",
    "        n_jobs = max(1, multiprocessing.cpu_count() - 1)\n",
    "\n",
    "    # 1. Train Category Classifier\n",
    "    print(\"  Training Category Classifier...\")\n",
    "    y_categories = df_train_full['disease_category'].values\n",
    "\n",
    "    cat_encoder = LabelEncoder()\n",
    "    y_cat_encoded = cat_encoder.fit_transform(y_categories)\n",
    "\n",
    "    cat_clf = SymptomCategoryClassifier(n_estimators=100)\n",
    "    cat_clf.fit(X_train, y_cat_encoded)\n",
    "    cat_clf.encoder_ = cat_encoder\n",
    "\n",
    "    # 2. Train specialist models (PARALLEL)\n",
    "    unique_categories = np.unique(y_categories)\n",
    "    print(f\"  Training {len(unique_categories)} Specialist Models (n_jobs={n_jobs})...\")\n",
    "\n",
    "    results = Parallel(n_jobs=n_jobs, backend=\"loky\", verbose=0)(\n",
    "        delayed(_train_specialist)(cat, X_train, y_train, y_categories)\n",
    "        for cat in unique_categories\n",
    "    )\n",
    "\n",
    "    specialist_models = {cat: model for cat, model in results if model is not None}\n",
    "    return cat_clf, specialist_models\n",
    "\n",
    "def predict_hierarchical(X_test, cat_clf, specialist_models, all_possible_diseases):\n",
    "    \"\"\"Hierarchical prediction logic.\"\"\"\n",
    "    cat_probs = cat_clf.predict_proba(X_test)\n",
    "    cat_encoder = cat_clf.encoder_\n",
    "    \n",
    "    final_probs = np.zeros((len(X_test), len(all_possible_diseases)))\n",
    "    disease_to_idx = {d: i for i, d in enumerate(all_possible_diseases)}\n",
    "    \n",
    "    for i, cat_idx in enumerate(cat_clf.categories):\n",
    "        cat_name = cat_encoder.inverse_transform([cat_idx])[0]\n",
    "        \n",
    "        if cat_name not in specialist_models:\n",
    "            continue\n",
    "        \n",
    "        model = specialist_models[cat_name]\n",
    "        specialist_probs = model.predict_proba(X_test)\n",
    "        weight = cat_probs[:, i][:, np.newaxis]\n",
    "        \n",
    "        for local_idx, disease in enumerate(model.diseases):\n",
    "            if disease in disease_to_idx:\n",
    "                global_idx = disease_to_idx[disease]\n",
    "                final_probs[:, global_idx] += weight.ravel() * specialist_probs[:, local_idx]\n",
    "                \n",
    "    return final_probs\n",
    "\n",
    "def evaluate_hierarchical(X_train, X_test, y_train, y_test, df_train_full, all_disease_classes):\n",
    "    \"\"\"Train and evaluate using hierarchical approach.\"\"\"\n",
    "    cat_clf, specialist_models = train_hierarchical_model(X_train, y_train, df_train_full)\n",
    "    y_proba = predict_hierarchical(X_test, cat_clf, specialist_models, all_disease_classes)\n",
    "    \n",
    "    y_pred_idx = np.argmax(y_proba, axis=1)\n",
    "    disease_to_idx = {d: i for i, d in enumerate(all_disease_classes)}\n",
    "    y_test_idx = np.array([disease_to_idx.get(d, -1) for d in y_test])\n",
    "    \n",
    "    valid_mask = y_test_idx != -1\n",
    "    all_labels = np.arange(len(all_disease_classes))\n",
    "    \n",
    "    results = {\n",
    "        'Top-1': accuracy_score(y_test_idx[valid_mask], y_pred_idx[valid_mask]),\n",
    "        'Top-3': top_k_accuracy_score(y_test_idx[valid_mask], y_proba[valid_mask], k=3, labels=all_labels),\n",
    "        'Top-5': top_k_accuracy_score(y_test_idx[valid_mask], y_proba[valid_mask], k=5, labels=all_labels),\n",
    "        'Macro-F1': f1_score(y_test_idx[valid_mask], y_pred_idx[valid_mask], average='macro')\n",
    "    }\n",
    "    \n",
    "    return results, cat_clf, specialist_models, y_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "# Part 2b: Qualitative Encoder Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.architectures.semantic_symptom_encoder import SemanticSymptomEncoder\n",
    "# Initialize used encoder\n",
    "encoder = SemanticSymptomEncoder()\n",
    "\n",
    "test_cases = {\n",
    "    \"headache\": [\"my head is killing me\", \"terrible migraine\", \"pain in my head\"],\n",
    "    \"shortness of breath\": [\"can't breathe\", \"difficulty breathing\", \"out of breath\"],\n",
    "    \"nausea\": [\"feeling sick\", \"want to throw up\", \"queasy stomach\"],\n",
    "    \"fever\": [\"high temperature\", \"running a fever\", \"burning up\"],\n",
    "    \"chest pain\": [\"hurts in my chest\", \"chest pressure\", \"sharp chest pain\"]\n",
    "}\n",
    "\n",
    "print(\"SEMANTIC SYMPTOM MATCHING:\")\n",
    "total_correct, total = 0, 0\n",
    "for target, phrases in test_cases.items():\n",
    "    correct_count = 0\n",
    "    for p in phrases:\n",
    "        res = encoder.encode_symptoms(p)\n",
    "        top_syms = [s[0] for s in encoder.get_top_symptoms(res['symptom_vector'], top_k=3)]\n",
    "        if any(target in s or s in target for s in top_syms):\n",
    "            correct_count += 1\n",
    "    total_correct += correct_count\n",
    "    total += len(phrases)\n",
    "    print(f\"  {target}: {correct_count}/{len(phrases)}\")\n",
    "print(f\"\\nOverall Accuracy: {total_correct/total*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2c: Semantic Encoder Comparison (6 Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CANDIDATE_MODELS = [\n",
    "    \"all-mpnet-base-v2\",\n",
    "    \"multi-qa-mpnet-base-dot-v1\",\n",
    "    \"all-MiniLM-L12-v2\",\n",
    "    \"paraphrase-mpnet-base-v2\",\n",
    "    \"paraphrase-MiniLM-L6-v2\",\n",
    "    \"sentence-transformers/msmarco-distilbert-cos-v5\",\n",
    "]\n",
    "\n",
    "# Default config for model comparison\n",
    "DEFAULT_THRESHOLD = 0.15\n",
    "DEFAULT_EXPONENT = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_encoder_on_paraphrases(encoder, paraphrases, n_tests=300, seed=42):\n",
    "    \"\"\"\n",
    "    Evaluate encoder using natural language paraphrases.\n",
    "    Returns P@5, R@5, F1@5, P@10, R@10, F1@10, MRR metrics.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    testable = [s for s in encoder.symptoms if s in paraphrases]\n",
    "    \n",
    "    precision_5, recall_5 = [], []\n",
    "    precision_10, recall_10 = [], []\n",
    "    mrr = []\n",
    "    \n",
    "    for _ in range(n_tests):\n",
    "        n = random.randint(2, 5)\n",
    "        # Ensure we don't sample more symptoms than available\n",
    "        sample_size = min(n, len(testable))\n",
    "        if sample_size == 0: continue\n",
    "            \n",
    "        gt_symptoms = set(random.sample(testable, sample_size))\n",
    "        \n",
    "        phrases = [random.choice(paraphrases[s]) for s in gt_symptoms]\n",
    "        text = \". \".join(phrases)\n",
    "        \n",
    "        result = encoder.encode_symptoms(text)\n",
    "        \n",
    "        # Get Top-10 for evaluation\n",
    "        top_10 = encoder.get_top_symptoms(result['symptom_vector'], top_k=10, threshold=0.0)\n",
    "        top_10_names = [s[0] for s in top_10]\n",
    "        \n",
    "        # @5 Metrics\n",
    "        top_5_names = top_10_names[:5]\n",
    "        hits_5 = len(set(top_5_names) & gt_symptoms)\n",
    "        precision_5.append(hits_5 / 5)\n",
    "        recall_5.append(hits_5 / len(gt_symptoms))\n",
    "        \n",
    "        # @10 Metrics\n",
    "        hits_10 = len(set(top_10_names) & gt_symptoms)\n",
    "        precision_10.append(hits_10 / 10)\n",
    "        recall_10.append(hits_10 / len(gt_symptoms))\n",
    "        \n",
    "        # MRR\n",
    "        for rank, (sym, _) in enumerate(top_10, 1):\n",
    "            if sym in gt_symptoms:\n",
    "                mrr.append(1.0 / rank)\n",
    "                break\n",
    "        else:\n",
    "            mrr.append(0.0)\n",
    "    \n",
    "    # helper for safe F1\n",
    "    def calc_f1(p, r): \n",
    "        if p + r == 0:\n",
    "            return 0.0\n",
    "        return 2 * p * r / (p + r)\n",
    "\n",
    "    p5, r5 = np.mean(precision_5), np.mean(recall_5)\n",
    "    p10, r10 = np.mean(precision_10), np.mean(recall_10)\n",
    "    \n",
    "    return {\n",
    "        'P@5': p5, \n",
    "        'R@5': r5, \n",
    "        'F1@5': calc_f1(p5, r5),\n",
    "        # Aliases for backward compatibility\n",
    "        'F1': calc_f1(p5, r5), \n",
    "        'P@10': p10,\n",
    "        'R@10': r10,\n",
    "        'F1@10': calc_f1(p10, r10),\n",
    "        'MRR': np.mean(mrr)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL COMPARISON (with checkpointing)\n",
      "============================================================\n",
      "  \u2705 Loaded checkpoint 'model_comparison': 6 items completed\n",
      "  \u23ed\ufe0f all-mpnet-base-v2: SKIPPED (already completed)\n",
      "  \u23ed\ufe0f multi-qa-mpnet-base-dot-v1: SKIPPED (already completed)\n",
      "  \u23ed\ufe0f all-MiniLM-L12-v2: SKIPPED (already completed)\n",
      "  \u23ed\ufe0f paraphrase-mpnet-base-v2: SKIPPED (already completed)\n",
      "  \u23ed\ufe0f paraphrase-MiniLM-L6-v2: SKIPPED (already completed)\n",
      "  \u23ed\ufe0f sentence-transformers/msmarco-distilbert-cos-v5: SKIPPED (already completed)\n",
      "\n",
      "============================================================\n",
      "MODEL COMPARISON RESULTS\n",
      "============================================================\n",
      "  multi-qa-mpnet-base-dot-v1                 P@5= 34.9% R@5= 51.4% F1= 41.6%\n",
      "  all-MiniLM-L12-v2                          P@5= 27.0% R@5= 40.1% F1= 32.3%\n",
      "  paraphrase-mpnet-base-v2                   P@5= 26.7% R@5= 38.6% F1= 31.5%\n",
      "  paraphrase-MiniLM-L6-v2                    P@5= 24.0% R@5= 35.2% F1= 28.6%\n",
      "  sentence-transformers/msmarco-distilbert-cos-v5 P@5= 17.4% R@5= 25.3% F1= 20.6%\n",
      "  all-mpnet-base-v2                          P@5= 10.1% R@5= 14.2% F1= 11.8%\n",
      "\n",
      "\ud83c\udfc6 Best Model: multi-qa-mpnet-base-dot-v1\n"
     ]
    }
   ],
   "source": [
    "# ==== Model Comparison with Checkpointing ====\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL COMPARISON (with checkpointing)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "state = load_checkpoint('model_comparison')\n",
    "completed = state['completed']\n",
    "all_results = state['results']\n",
    "\n",
    "for model_name in CANDIDATE_MODELS:\n",
    "    if model_name in completed:\n",
    "        print(f\"  \u23ed\ufe0f {model_name}: SKIPPED (already completed)\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n\u25b6 Testing: {model_name}\")\n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        encoder = SemanticSymptomEncoder(\n",
    "            model_name=model_name,\n",
    "            device='cpu',\n",
    "            threshold=DEFAULT_THRESHOLD,\n",
    "            exponent=DEFAULT_EXPONENT\n",
    "        )\n",
    "        \n",
    "        metrics = evaluate_encoder_on_paraphrases(encoder, paraphrases, n_tests=300)\n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        result = {\n",
    "            'model': model_name,\n",
    "            'threshold': DEFAULT_THRESHOLD,\n",
    "            'exponent': DEFAULT_EXPONENT,\n",
    "            **metrics,\n",
    "            'time': elapsed\n",
    "        }\n",
    "        \n",
    "        all_results.append(result)\n",
    "        completed[model_name] = True\n",
    "        \n",
    "        state['completed'] = completed\n",
    "        state['results'] = all_results\n",
    "        save_checkpoint('model_comparison', state)\n",
    "        \n",
    "        print(f\"   P@5: {metrics['P@5']*100:.1f}% | R@5: {metrics['R@5']*100:.1f}% | F1: {metrics['F1']*100:.1f}% ({elapsed:.1f}s)\")\n",
    "        \n",
    "        del encoder\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   \u274c Failed: {e}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL COMPARISON RESULTS\")\n",
    "print(\"=\"*60)\n",
    "results_sorted = sorted(all_results, key=lambda x: x['F1'], reverse=True)\n",
    "for r in results_sorted:\n",
    "    print(f\"  {r['model']:42s} P@5={r['P@5']*100:5.1f}% R@5={r['R@5']*100:5.1f}% F1={r['F1']*100:5.1f}%\")\n",
    "\n",
    "BEST_MODEL = results_sorted[0]['model']\n",
    "print(f\"\\n\ud83c\udfc6 Best Model: {BEST_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: 5-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(X, y, df_full, disease_classes, n_folds=5, cv_name='default'):\n",
    "    \"\"\"Perform stratified k-fold CV with checkpointing.\"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    cv_state = load_cv_state(cv_name)\n",
    "    fold_results_dict = cv_state.get('fold_results', {})\n",
    "    \n",
    "    indices = np.arange(len(y))\n",
    "    \n",
    "    for fold, (train_idx_cv, test_idx_cv) in enumerate(skf.split(indices, y)):\n",
    "        if fold in fold_results_dict:\n",
    "            print(f\"  Fold {fold+1}: SKIPPED (Top-1={fold_results_dict[fold]['Top-1']*100:.2f}%)\")\n",
    "            continue\n",
    "        \n",
    "        X_train_cv = X.iloc[train_idx_cv]\n",
    "        X_test_cv = X.iloc[test_idx_cv]\n",
    "        y_train_cv = y[train_idx_cv]\n",
    "        y_test_cv = y[test_idx_cv]\n",
    "        df_train_cv = df_full.iloc[train_idx_cv]\n",
    "        \n",
    "        results, _, _, _ = evaluate_hierarchical(\n",
    "            X_train_cv, X_test_cv, y_train_cv, y_test_cv, df_train_cv, disease_classes\n",
    "        )\n",
    "        \n",
    "        fold_results_dict[fold] = results\n",
    "        cv_state['fold_results'] = fold_results_dict\n",
    "        save_cv_state(cv_name, cv_state)\n",
    "        \n",
    "        print(f\"  Fold {fold+1}: Top-1={results['Top-1']*100:.2f}% (saved)\")\n",
    "    \n",
    "    fold_results = [fold_results_dict[i] for i in sorted(fold_results_dict.keys())]\n",
    "    \n",
    "    summary = {}\n",
    "    for metric in fold_results[0].keys():\n",
    "        values = [r[metric] for r in fold_results]\n",
    "        summary[metric] = {'mean': np.mean(values), 'std': np.std(values)}\n",
    "    \n",
    "    print(f\"  CV complete: {len(fold_results)} folds aggregated\")\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base features: 375\n",
      "Augmented features: 456\n",
      "Demo features: 458\n"
     ]
    }
   ],
   "source": [
    "# Prepare feature columns\n",
    "feature_cols_base = [c for c in df_base.columns if c not in non_feature_cols]\n",
    "feature_cols_aug = [c for c in df_augmented_filtered.columns if c not in non_feature_cols]\n",
    "\n",
    "# For demographics, add age_normalized and sex_encoded\n",
    "df_demo_filtered['sex_encoded'] = (df_demo_filtered['sex'] == 'M').astype(int)\n",
    "df_demo_filtered['age_normalized'] = df_demo_filtered['age'] / 100.0\n",
    "feature_cols_demo = feature_cols_aug + ['age_normalized', 'sex_encoded']\n",
    "feature_cols_demo = [c for c in feature_cols_demo if c in df_demo_filtered.columns]\n",
    "\n",
    "print(f\"Base features: {len(feature_cols_base)}\")\n",
    "print(f\"Augmented features: {len(feature_cols_aug)}\")\n",
    "print(f\"Demo features: {len(feature_cols_demo)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "5-FOLD CV ON BASE/REAL DATA\n",
      "============================================================\n",
      "  Loaded checkpoint for 'base': 5 folds completed\n",
      "  Fold 1: SKIPPED (Top-1=82.04%)\n",
      "  Fold 2: SKIPPED (Top-1=81.84%)\n",
      "  Fold 3: SKIPPED (Top-1=81.51%)\n",
      "  Fold 4: SKIPPED (Top-1=82.09%)\n",
      "  Fold 5: SKIPPED (Top-1=81.65%)\n",
      "  CV complete: 5 folds aggregated\n",
      "\n",
      "Cross-Validation Results (Base):\n",
      "  Top-1: 81.83% \u00b1 0.22%\n",
      "  Top-3: 94.07% \u00b1 0.06%\n",
      "  Top-5: 96.54% \u00b1 0.05%\n",
      "  Macro-F1: 70.53% \u00b1 0.32%\n"
     ]
    }
   ],
   "source": [
    "# ==== Run 5-Fold CV on Base Data ====\n",
    "print(\"=\"*60)\n",
    "print(\"5-FOLD CV ON BASE/REAL DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "X_base, y_base, _ = prepare_features(df_base_filtered, feature_cols_base)\n",
    "all_diseases_base = np.unique(y_base)\n",
    "\n",
    "cv_results_base = cross_validate(X_base, y_base, df_base_filtered, all_diseases_base, cv_name='base')\n",
    "\n",
    "print(f\"\\nCross-Validation Results (Base):\")\n",
    "for metric, stats in cv_results_base.items():\n",
    "    print(f\"  {metric}: {stats['mean']*100:.2f}% \u00b1 {stats['std']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "5-FOLD CV ON AUGMENTED DATA\n",
      "============================================================\n",
      "  Training Category Classifier...\n",
      "Training SymptomCategoryClassifier with shape (165993, 456)\n",
      "  Training 14 Specialist Models (n_jobs=3)...\n",
      "  Fold 1: Top-1=81.55% (saved)\n",
      "  Training Category Classifier...\n",
      "Training SymptomCategoryClassifier with shape (165993, 456)\n",
      "  Training 14 Specialist Models (n_jobs=3)...\n",
      "  Fold 2: Top-1=81.75% (saved)\n",
      "  Training Category Classifier...\n",
      "Training SymptomCategoryClassifier with shape (165994, 456)\n",
      "  Training 14 Specialist Models (n_jobs=3)...\n",
      "  Fold 3: Top-1=81.68% (saved)\n",
      "  Training Category Classifier...\n",
      "Training SymptomCategoryClassifier with shape (165994, 456)\n",
      "  Training 14 Specialist Models (n_jobs=3)...\n",
      "  Fold 4: Top-1=81.65% (saved)\n",
      "  Training Category Classifier...\n",
      "Training SymptomCategoryClassifier with shape (165994, 456)\n",
      "  Training 14 Specialist Models (n_jobs=3)...\n",
      "  Fold 5: Top-1=81.64% (saved)\n",
      "  CV complete: 5 folds aggregated\n",
      "\n",
      "Cross-Validation Results (Augmented):\n",
      "  Top-1: 81.65% \u00b1 0.07%\n",
      "  Top-3: 93.81% \u00b1 0.07%\n",
      "  Top-5: 96.36% \u00b1 0.05%\n",
      "  Macro-F1: 70.75% \u00b1 0.23%\n"
     ]
    }
   ],
   "source": [
    "# ==== Run 5-Fold CV on Augmented Data ====\n",
    "print(\"=\"*60)\n",
    "print(\"5-FOLD CV ON AUGMENTED DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "X_aug, y_aug, _ = prepare_features(df_augmented_filtered, feature_cols_aug)\n",
    "all_diseases_aug = np.unique(y_aug)\n",
    "\n",
    "cv_results_aug = cross_validate(X_aug, y_aug, df_augmented_filtered, all_diseases_aug, cv_name='augmented')\n",
    "\n",
    "print(f\"\\nCross-Validation Results (Augmented):\")\n",
    "for metric, stats in cv_results_aug.items():\n",
    "    print(f\"  {metric}: {stats['mean']*100:.2f}% \u00b1 {stats['std']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "5-FOLD CV ON DEMOGRAPHICS DATA\n",
      "============================================================\n",
      "  Training Category Classifier...\n",
      "Training SymptomCategoryClassifier with shape (165993, 458)\n",
      "  Training 14 Specialist Models (n_jobs=3)...\n",
      "  Fold 1: Top-1=83.65% (saved)\n",
      "  Training Category Classifier...\n",
      "Training SymptomCategoryClassifier with shape (165993, 458)\n",
      "  Training 14 Specialist Models (n_jobs=3)...\n",
      "  Fold 2: Top-1=84.18% (saved)\n",
      "  Training Category Classifier...\n",
      "Training SymptomCategoryClassifier with shape (165994, 458)\n",
      "  Training 14 Specialist Models (n_jobs=3)...\n",
      "  Fold 3: Top-1=84.01% (saved)\n",
      "  Training Category Classifier...\n",
      "Training SymptomCategoryClassifier with shape (165994, 458)\n",
      "  Training 14 Specialist Models (n_jobs=3)...\n",
      "  Fold 4: Top-1=83.94% (saved)\n",
      "  Training Category Classifier...\n",
      "Training SymptomCategoryClassifier with shape (165994, 458)\n",
      "  Training 14 Specialist Models (n_jobs=3)...\n",
      "  Fold 5: Top-1=83.97% (saved)\n",
      "  CV complete: 5 folds aggregated\n",
      "\n",
      "Cross-Validation Results (Demographics):\n",
      "  Top-1: 83.95% \u00b1 0.17%\n",
      "  Top-3: 95.05% \u00b1 0.07%\n",
      "  Top-5: 97.16% \u00b1 0.05%\n",
      "  Macro-F1: 72.55% \u00b1 0.40%\n",
      "\n",
      "\ud83d\udcca Demographics Contribution:\n",
      "  Demographics Effect: +2.29%\n"
     ]
    }
   ],
   "source": [
    "# ==== Run 5-Fold CV on Demographics Data ====\n",
    "print(\"=\"*60)\n",
    "print(\"5-FOLD CV ON DEMOGRAPHICS DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "X_demo = df_demo_filtered[feature_cols_demo]\n",
    "y_demo = df_demo_filtered['diseases'].values\n",
    "all_diseases_demo = np.unique(y_demo)\n",
    "\n",
    "cv_results_demo = cross_validate(X_demo, y_demo, df_demo_filtered, all_diseases_demo, cv_name='demographics')\n",
    "\n",
    "print(f\"\\nCross-Validation Results (Demographics):\")\n",
    "for metric, stats in cv_results_demo.items():\n",
    "    print(f\"  {metric}: {stats['mean']*100:.2f}% \u00b1 {stats['std']*100:.2f}%\")\n",
    "\n",
    "# Demographics contribution\n",
    "print(f\"\\n\ud83d\udcca Demographics Contribution:\")\n",
    "demo_effect = (cv_results_demo['Top-1']['mean'] - cv_results_aug['Top-1']['mean']) * 100\n",
    "print(f\"  Demographics Effect: {demo_effect:+.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "# Part 4: Baseline Models Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BASELINE MODEL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prepare simple split for baselines (using base data)\n",
    "X_b, y_b, _ = prepare_features(df_base_filtered, feature_cols_base)\n",
    "X_train_b, X_test_b, y_train_b, y_test_b = train_test_split(X_b, y_b, test_size=0.2, random_state=42)\n",
    "print(f\"Training baselines on {len(X_train_b):,} samples...\")\n",
    "\n",
    "# Logistic Regression\n",
    "start = time.time()\n",
    "lr_clf = LogisticRegression(max_iter=500, n_jobs=-1, random_state=42)\n",
    "lr_clf.fit(X_train_b, y_train_b)\n",
    "lr_acc = accuracy_score(y_test_b, lr_clf.predict(X_test_b))\n",
    "print(f\"Logistic Regression: {lr_acc*100:.2f}% ({time.time() - start:.1f}s)\")\n",
    "\n",
    "# Random Forest\n",
    "start = time.time()\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, max_depth=20, n_jobs=-1, random_state=42)\n",
    "rf_clf.fit(X_train_b, y_train_b)\n",
    "rf_acc = accuracy_score(y_test_b, rf_clf.predict(X_test_b))\n",
    "print(f\"Random Forest: {rf_acc*100:.2f}% ({time.time() - start:.1f}s)\")\n",
    "\n",
    "# Compare with our best model (Demographics CV Average)\n",
    "our_acc = cv_results_demo['Top-1']['mean']\n",
    "print(f\"\\nOurs (Hierarchical + Demo): {our_acc*100:.2f}%\")\n",
    "print(f\"Improvement over LR: +{(our_acc-lr_acc)*100:.1f}%\")\n",
    "print(f\"Improvement over RF: +{(our_acc-rf_acc)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "# Part 5: Statistical Significance (McNemar's Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "\n",
    "# Prepare data for hold-out comparison\n",
    "X_s, y_s, _ = prepare_features(df_augmented_filtered, feature_cols_aug)\n",
    "X_d, y_d, _ = prepare_features(df_demo_filtered, feature_cols_demo)\n",
    "\n",
    "# Align indices (intersection)\n",
    "common_idx = df_augmented_filtered.index.intersection(df_demo_filtered.index)\n",
    "X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(X_s.loc[common_idx], y_s[common_idx], test_size=0.1, random_state=42)\n",
    "X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(X_d.loc[common_idx], y_d[common_idx], test_size=0.1, random_state=42)\n",
    "\n",
    "# Train models on single split for direct comparison\n",
    "print(\"Training models for McNemar's test...\")\n",
    "model_s = SymptomDiseaseClassifier(n_estimators=50) # Faster for stat test\n",
    "model_s.fit(X_train_s, y_train_s)\n",
    "pred_s = model_s.predict(X_test_s)\n",
    "\n",
    "model_d = SymptomDiseaseClassifier(n_estimators=50)\n",
    "model_d.fit(X_train_d, y_train_d)\n",
    "pred_d = model_d.predict(X_test_d)\n",
    "\n",
    "# Calculate Contingency Table\n",
    "correct_s = (pred_s == y_test_s)\n",
    "correct_d = (pred_d == y_test_d)\n",
    "\n",
    "b = ((~correct_s) & correct_d).sum()  # symptoms wrong, demo right\n",
    "c = (correct_s & (~correct_d)).sum()  # symptoms right, demo wrong\n",
    "\n",
    "print(f\"\\nMcNemar's Test Results:\")\n",
    "print(f\"  Symptoms Accuracy: {correct_s.mean()*100:.2f}%\")\n",
    "print(f\"  +Demo Accuracy: {correct_d.mean()*100:.2f}%\")\n",
    "print(f\"  Disagreement: b={b}, c={c}\")\n",
    "\n",
    "if b + c > 0:\n",
    "    mcnemar_stat = (abs(b - c) - 1)**2 / (b + c)\n",
    "    p_value = 1 - chi2.cdf(mcnemar_stat, df=1)\n",
    "    print(f\"  Chi-sq Statistic: {mcnemar_stat:.2f}\")\n",
    "    print(f\"  P-value: {p_value:.6f}\")\n",
    "    print(f\"  Significant (p<0.05): {'Yes' if p_value < 0.05 else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "# Part 6: Literature Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "literature = pd.DataFrame({\n",
    "    'System': ['Ada Health', 'Babylon Health', 'Isabel Healthcare', 'WebMD', 'Ours (Hierarchical CV)'],\n",
    "    'Diseases': ['~1000', '~500', '~6000', '~200', str(len(all_diseases_demo))],\n",
    "    'Top-1': ['51%*', '60%*', '48%*', '35%*', f\"{cv_results_demo['Top-1']['mean']*100:.1f}%\"],\n",
    "    'Top-5': ['70%*', '80%*', '74%*', '58%*', f\"{cv_results_demo['Top-5']['mean']*100:.1f}%\"]\n",
    "})\n",
    "print(\"LITERATURE COMPARISON (* = approximate from published evaluations):\")\n",
    "print(literature.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 7: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Results saved to: c:\\Users\\henry\\Desktop\\Programming\\Python\\Multimodal_Diagnosis\\notebooks\\figures\\rigorous_eval_results.json\n"
     ]
    }
   ],
   "source": [
    "# Save CV results\n",
    "results_path = project_root / 'notebooks' / 'figures' / 'rigorous_eval_results.json'\n",
    "\n",
    "# Load existing or create new\n",
    "if results_path.exists():\n",
    "    with open(results_path, 'r') as f:\n",
    "        final_results = json.load(f)\n",
    "else:\n",
    "    final_results = {}\n",
    "\n",
    "# Update CV results\n",
    "final_results['cv_real'] = {k: {'mean': v['mean'], 'std': v['std']} for k, v in cv_results_base.items()}\n",
    "final_results['cv_augmented'] = {k: {'mean': v['mean'], 'std': v['std']} for k, v in cv_results_aug.items()}\n",
    "final_results['cv_augmented_demo'] = {k: {'mean': v['mean'], 'std': v['std']} for k, v in cv_results_demo.items()}\n",
    "\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(final_results, f, indent=2, default=float)\n",
    "\n",
    "print(f\"\u2705 Results saved to: {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 8: Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ABLATION STUDY SUMMARY (5-Fold Cross-Validation)\n",
      "======================================================================\n",
      "           Configuration Top-1 (Mean \u00b1 Std) Top-5 Mean Macro-F1 Mean\n",
      "          Base (Cleaned)     81.83% \u00b1 0.22%     96.54%        70.53%\n",
      "               Augmented     81.65% \u00b1 0.07%     96.36%        70.75%\n",
      "Augmented + Demographics     83.95% \u00b1 0.17%     97.16%        72.55%\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ABLATION STUDY SUMMARY (5-Fold Cross-Validation)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "ablation_cv_df = pd.DataFrame({\n",
    "    'Configuration': ['Base (Cleaned)', 'Augmented', 'Augmented + Demographics'],\n",
    "    'Top-1 (Mean \u00b1 Std)': [\n",
    "        f\"{cv_results_base['Top-1']['mean']*100:.2f}% \u00b1 {cv_results_base['Top-1']['std']*100:.2f}%\",\n",
    "        f\"{cv_results_aug['Top-1']['mean']*100:.2f}% \u00b1 {cv_results_aug['Top-1']['std']*100:.2f}%\",\n",
    "        f\"{cv_results_demo['Top-1']['mean']*100:.2f}% \u00b1 {cv_results_demo['Top-1']['std']*100:.2f}%\"\n",
    "    ],\n",
    "    'Top-5 Mean': [\n",
    "        f\"{cv_results_base['Top-5']['mean']*100:.2f}%\",\n",
    "        f\"{cv_results_aug['Top-5']['mean']*100:.2f}%\",\n",
    "        f\"{cv_results_demo['Top-5']['mean']*100:.2f}%\"\n",
    "    ],\n",
    "    'Macro-F1 Mean': [\n",
    "        f\"{cv_results_base['Macro-F1']['mean']*100:.2f}%\",\n",
    "        f\"{cv_results_aug['Macro-F1']['mean']*100:.2f}%\",\n",
    "        f\"{cv_results_demo['Macro-F1']['mean']*100:.2f}%\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(ablation_cv_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 9: User-in-Loop Pipeline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered: 207,492 rows, 616 diseases\n"
     ]
    }
   ],
   "source": [
    "# Load hierarchical models for pipeline evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from models.architectures.symptom_classifier import SymptomCategoryClassifier, SymptomDiseaseClassifier\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "# Prepare data\n",
    "non_feature_cols = ['diseases', 'disease_category', 'symptoms', 'age', 'sex', 'age_normalized', 'sex_encoded']\n",
    "feature_cols = [c for c in df.columns if c not in non_feature_cols]\n",
    "\n",
    "# Filter to diseases with enough samples\n",
    "disease_counts = df['diseases'].value_counts()\n",
    "valid_diseases = disease_counts[disease_counts >= 5].index\n",
    "df_filtered = df[df['diseases'].isin(valid_diseases)].copy()\n",
    "\n",
    "print(f\"Filtered: {len(df_filtered):,} rows, {df_filtered['diseases'].nunique()} diseases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_specialist(category, X_train, y_train, y_categories):\n",
    "    mask = y_categories == category\n",
    "    X_cat = X_train[mask]\n",
    "    y_cat = y_train[mask]\n",
    "    if len(y_cat) < 5:\n",
    "        return None, None\n",
    "    model = SymptomDiseaseClassifier(category=category, n_estimators=100)\n",
    "    model.fit(X_cat, y_cat)\n",
    "    return category, model\n",
    "\n",
    "def train_hierarchical_model(X_train, y_train, df_train):\n",
    "    \"\"\"Train hierarchical model.\"\"\"\n",
    "    n_jobs = max(1, multiprocessing.cpu_count() - 1)\n",
    "    \n",
    "    y_categories = df_train['disease_category'].values\n",
    "    cat_encoder = LabelEncoder()\n",
    "    y_cat_encoded = cat_encoder.fit_transform(y_categories)\n",
    "    \n",
    "    cat_clf = SymptomCategoryClassifier(n_estimators=100)\n",
    "    cat_clf.fit(X_train, y_cat_encoded)\n",
    "    cat_clf.encoder_ = cat_encoder\n",
    "    \n",
    "    unique_categories = np.unique(y_categories)\n",
    "    results = Parallel(n_jobs=n_jobs, backend=\"loky\", verbose=0)(\n",
    "        delayed(_train_specialist)(cat, X_train, y_train, y_categories)\n",
    "        for cat in unique_categories\n",
    "    )\n",
    "    \n",
    "    specialist_models = {cat: model for cat, model in results if model is not None}\n",
    "    return cat_clf, specialist_models\n",
    "\n",
    "def predict_hierarchical(X_test, cat_clf, specialist_models, all_diseases):\n",
    "    cat_probs = cat_clf.predict_proba(X_test)\n",
    "    cat_encoder = cat_clf.encoder_\n",
    "    \n",
    "    final_probs = np.zeros((len(X_test), len(all_diseases)))\n",
    "    disease_to_idx = {d: i for i, d in enumerate(all_diseases)}\n",
    "    \n",
    "    for i, cat_idx in enumerate(cat_clf.categories):\n",
    "        cat_name = cat_encoder.inverse_transform([cat_idx])[0]\n",
    "        if cat_name not in specialist_models:\n",
    "            continue\n",
    "        model = specialist_models[cat_name]\n",
    "        specialist_probs = model.predict_proba(X_test)\n",
    "        weight = cat_probs[:, i][:, np.newaxis]\n",
    "        \n",
    "        for local_idx, disease in enumerate(model.diseases):\n",
    "            if disease in disease_to_idx:\n",
    "                global_idx = disease_to_idx[disease]\n",
    "                final_probs[:, global_idx] += weight.ravel() * specialist_probs[:, local_idx]\n",
    "    \n",
    "    return final_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training hierarchical classifier...\n",
      "Training SymptomCategoryClassifier with shape (186742, 456)\n",
      "Trained 14 specialist models\n"
     ]
    }
   ],
   "source": [
    "# Train hierarchical model (needed for pipeline eval)\n",
    "print(\"Training hierarchical classifier...\")\n",
    "\n",
    "X = df_filtered[feature_cols].fillna(0)\n",
    "y = df_filtered['diseases'].values\n",
    "all_diseases = np.unique(y)\n",
    "\n",
    "indices = np.arange(len(df_filtered))\n",
    "train_idx, test_idx = train_test_split(indices, test_size=0.1, random_state=42, stratify=y)\n",
    "\n",
    "X_train = X.iloc[train_idx]\n",
    "y_train = y[train_idx]\n",
    "df_train = df_filtered.iloc[train_idx]\n",
    "\n",
    "cat_clf, specialist_models = train_hierarchical_model(X_train, y_train, df_train)\n",
    "print(f\"Trained {len(specialist_models)} specialist models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_user_in_loop(\n",
    "    encoder, X_true, y_true, symptom_cols, cat_clf, specialist_models, \n",
    "    all_diseases, paraphrases, top_k=20, max_samples=2500\n",
    "):\n",
    "    \"\"\"Simulate user-in-loop pipeline WITH PARAPHRASES.\"\"\"\n",
    "    # Create bidirectional mapping between symptom_cols and encoder symptoms\n",
    "    col_to_symptom = {}\n",
    "    symptom_to_col_idx = {}\n",
    "    \n",
    "    for col_idx, col in enumerate(symptom_cols):\n",
    "        normalized = encoder._normalize(col.replace(\"_\", \" \"))\n",
    "        col_to_symptom[col] = normalized\n",
    "        if normalized in encoder.symptom_to_idx:\n",
    "            symptom_to_col_idx[normalized] = col_idx\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    idxs = np.random.choice(len(X_true), size=min(max_samples, len(X_true)), replace=False)\n",
    "    \n",
    "    y_true_idx = []\n",
    "    y_pred_proba = []\n",
    "    disease_to_idx = {d: i for i, d in enumerate(all_diseases)}\n",
    "    \n",
    "    for i in idxs:\n",
    "        # Get ground truth symptoms\n",
    "        active_cols = [col for j, col in enumerate(symptom_cols) if X_true[i, j] == 1.0]\n",
    "        if len(active_cols) < 2:\n",
    "            continue\n",
    "        \n",
    "        # \u2705 USE PARAPHRASES, not exact names!\n",
    "        active_syms = [col_to_symptom[c] for c in active_cols]\n",
    "        phrases = []\n",
    "        for sym in active_syms:\n",
    "            if sym in paraphrases:\n",
    "                phrases.append(random.choice(paraphrases[sym]))\n",
    "            else:\n",
    "                phrases.append(sym)  # fallback\n",
    "        text = \". \".join(phrases)\n",
    "        \n",
    "        # Encode\n",
    "        result = encoder.encode_symptoms(text)\n",
    "        symptom_vector = result[\"symptom_vector\"]\n",
    "        \n",
    "        # Get detected symptoms using encoder's threshold\n",
    "        # This time, threshold REALLY matters because paraphrases might have lower similarity\n",
    "        top_results = encoder.get_top_symptoms(symptom_vector, top_k=100, threshold=encoder.threshold)\n",
    "        detected_syms = {sym for sym, score in top_results}\n",
    "        \n",
    "        # Simulate user confirmation with false positive rejection\n",
    "        X_sim = np.zeros(len(symptom_cols))\n",
    "        confirmed_count = 0\n",
    "        \n",
    "        for sym_name in detected_syms:\n",
    "            if sym_name in active_syms and sym_name in symptom_to_col_idx:\n",
    "                # \u2705 User confirms true positive\n",
    "                col_idx = symptom_to_col_idx[sym_name]\n",
    "                X_sim[col_idx] = 1.0\n",
    "                confirmed_count += 1\n",
    "            # Note: We're NOT adding false positives (user rejects them)\n",
    "        \n",
    "        # Skip if no symptoms confirmed\n",
    "        if confirmed_count == 0:\n",
    "            # Maybe use top-1 symptom as fallback?\n",
    "            continue\n",
    "        \n",
    "        # Predict\n",
    "        proba = predict_hierarchical(\n",
    "            X_sim.reshape(1, -1), cat_clf, specialist_models, all_diseases\n",
    "        )[0]\n",
    "        \n",
    "        y_pred_proba.append(proba)\n",
    "        y_true_idx.append(disease_to_idx[y_true[i]])\n",
    "    \n",
    "    if len(y_pred_proba) == 0:\n",
    "        return {'top1': 0.0, 'top3': 0.0, 'top5': 0.0, 'samples': 0}\n",
    "    \n",
    "    y_pred_proba = np.array(y_pred_proba)\n",
    "    y_true_idx = np.array(y_true_idx)\n",
    "    \n",
    "    top1 = accuracy_score(y_true_idx, np.argmax(y_pred_proba, axis=1))\n",
    "    top3 = top_k_accuracy_score(y_true_idx, y_pred_proba, k=3, labels=np.arange(len(all_diseases)))\n",
    "    top5 = top_k_accuracy_score(y_true_idx, y_pred_proba, k=5, labels=np.arange(len(all_diseases)))\n",
    "    \n",
    "    return {'top1': top1, 'top3': top3, 'top5': top5, 'samples': len(y_true_idx)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "USER-IN-LOOP PIPELINE EVALUATION (with checkpointing)\n",
      "============================================================\n",
      "  Cleared checkpoint 'pipeline_sweep'\n",
      "Total configs: 30, Already completed: 0, To run: 30\n",
      "[Encoder] Loading model: multi-qa-mpnet-base-dot-v1\n",
      "[Encoder] Loaded cached symptom embeddings\n",
      "[Encoder] Initialized with 456 symptoms\n",
      "Using model: multi-qa-mpnet-base-dot-v1\n",
      "\n",
      "\u25b6 Testing thresh=0.15, exp=0.5...\n",
      "  Non-zero symptoms with thresh=0.15: 456\n",
      "  Top-1=69.0%, Top-3=85.1%, Top-5=89.5%\n",
      "\n",
      "\u25b6 Testing thresh=0.15, exp=1.0...\n",
      "  Non-zero symptoms with thresh=0.15: 456\n",
      "  Top-1=67.5%, Top-3=84.7%, Top-5=89.1%\n",
      "\n",
      "\u25b6 Testing thresh=0.15, exp=1.5...\n",
      "  Non-zero symptoms with thresh=0.15: 456\n",
      "  Top-1=67.0%, Top-3=83.5%, Top-5=87.7%\n",
      "\n",
      "\u25b6 Testing thresh=0.15, exp=2.0...\n",
      "  Non-zero symptoms with thresh=0.15: 456\n",
      "  Top-1=63.5%, Top-3=81.0%, Top-5=86.5%\n",
      "\n",
      "\u25b6 Testing thresh=0.15, exp=2.5...\n",
      "  Non-zero symptoms with thresh=0.15: 456\n",
      "  Top-1=60.6%, Top-3=80.2%, Top-5=84.3%\n",
      "\n",
      "\u25b6 Testing thresh=0.25, exp=0.5...\n",
      "  Non-zero symptoms with thresh=0.25: 453\n",
      "  Top-1=66.7%, Top-3=84.0%, Top-5=88.8%\n",
      "\n",
      "\u25b6 Testing thresh=0.25, exp=1.0...\n",
      "  Non-zero symptoms with thresh=0.25: 453\n",
      "  Top-1=64.0%, Top-3=81.0%, Top-5=85.7%\n",
      "\n",
      "\u25b6 Testing thresh=0.25, exp=1.5...\n",
      "  Non-zero symptoms with thresh=0.25: 453\n",
      "  Top-1=61.1%, Top-3=79.3%, Top-5=83.6%\n",
      "\n",
      "\u25b6 Testing thresh=0.25, exp=2.0...\n",
      "  Non-zero symptoms with thresh=0.25: 453\n",
      "  Top-1=61.0%, Top-3=78.2%, Top-5=83.0%\n",
      "\n",
      "\u25b6 Testing thresh=0.25, exp=2.5...\n",
      "  Non-zero symptoms with thresh=0.25: 453\n",
      "  Top-1=59.6%, Top-3=78.1%, Top-5=82.8%\n",
      "\n",
      "\u25b6 Testing thresh=0.35, exp=0.5...\n",
      "  Non-zero symptoms with thresh=0.35: 334\n",
      "  Top-1=65.0%, Top-3=82.4%, Top-5=87.9%\n",
      "\n",
      "\u25b6 Testing thresh=0.35, exp=1.0...\n",
      "  Non-zero symptoms with thresh=0.35: 334\n",
      "  Top-1=62.0%, Top-3=80.1%, Top-5=84.8%\n",
      "\n",
      "\u25b6 Testing thresh=0.35, exp=1.5...\n",
      "  Non-zero symptoms with thresh=0.35: 334\n",
      "  Top-1=61.1%, Top-3=78.8%, Top-5=83.7%\n",
      "\n",
      "\u25b6 Testing thresh=0.35, exp=2.0...\n",
      "  Non-zero symptoms with thresh=0.35: 334\n",
      "  Top-1=62.0%, Top-3=79.2%, Top-5=83.3%\n",
      "\n",
      "\u25b6 Testing thresh=0.35, exp=2.5...\n",
      "  Non-zero symptoms with thresh=0.35: 334\n",
      "  Top-1=60.6%, Top-3=79.1%, Top-5=83.5%\n",
      "\n",
      "\u25b6 Testing thresh=0.4, exp=0.5...\n",
      "  Non-zero symptoms with thresh=0.4: 132\n",
      "  Top-1=62.5%, Top-3=80.7%, Top-5=85.3%\n",
      "\n",
      "\u25b6 Testing thresh=0.4, exp=1.0...\n",
      "  Non-zero symptoms with thresh=0.4: 132\n",
      "  Top-1=61.2%, Top-3=79.3%, Top-5=84.1%\n",
      "\n",
      "\u25b6 Testing thresh=0.4, exp=1.5...\n",
      "  Non-zero symptoms with thresh=0.4: 132\n",
      "  Top-1=60.7%, Top-3=78.7%, Top-5=83.2%\n",
      "\n",
      "\u25b6 Testing thresh=0.4, exp=2.0...\n",
      "  Non-zero symptoms with thresh=0.4: 132\n",
      "  Top-1=61.3%, Top-3=78.4%, Top-5=83.7%\n",
      "\n",
      "\u25b6 Testing thresh=0.4, exp=2.5...\n",
      "  Non-zero symptoms with thresh=0.4: 132\n",
      "  Top-1=60.7%, Top-3=78.2%, Top-5=83.3%\n",
      "\n",
      "\u25b6 Testing thresh=0.45, exp=0.5...\n",
      "  Non-zero symptoms with thresh=0.45: 34\n",
      "  Top-1=62.0%, Top-3=79.7%, Top-5=83.7%\n",
      "\n",
      "\u25b6 Testing thresh=0.45, exp=1.0...\n",
      "  Non-zero symptoms with thresh=0.45: 34\n",
      "  Top-1=61.0%, Top-3=78.2%, Top-5=82.9%\n",
      "\n",
      "\u25b6 Testing thresh=0.45, exp=1.5...\n",
      "  Non-zero symptoms with thresh=0.45: 34\n",
      "  Top-1=60.7%, Top-3=79.3%, Top-5=83.8%\n",
      "\n",
      "\u25b6 Testing thresh=0.45, exp=2.0...\n",
      "  Non-zero symptoms with thresh=0.45: 34\n",
      "  Top-1=61.0%, Top-3=79.9%, Top-5=83.5%\n",
      "\n",
      "\u25b6 Testing thresh=0.45, exp=2.5...\n",
      "  Non-zero symptoms with thresh=0.45: 34\n",
      "  Top-1=61.8%, Top-3=78.4%, Top-5=83.5%\n",
      "\n",
      "\u25b6 Testing thresh=0.5, exp=0.5...\n",
      "  Non-zero symptoms with thresh=0.5: 7\n",
      "  Top-1=60.1%, Top-3=78.4%, Top-5=82.7%\n",
      "\n",
      "\u25b6 Testing thresh=0.5, exp=1.0...\n",
      "  Non-zero symptoms with thresh=0.5: 7\n",
      "  Top-1=60.5%, Top-3=77.6%, Top-5=82.3%\n",
      "\n",
      "\u25b6 Testing thresh=0.5, exp=1.5...\n",
      "  Non-zero symptoms with thresh=0.5: 7\n",
      "  Top-1=59.3%, Top-3=78.7%, Top-5=83.8%\n",
      "\n",
      "\u25b6 Testing thresh=0.5, exp=2.0...\n",
      "  Non-zero symptoms with thresh=0.5: 7\n",
      "  Top-1=60.5%, Top-3=78.6%, Top-5=83.7%\n",
      "\n",
      "\u25b6 Testing thresh=0.5, exp=2.5...\n",
      "  Non-zero symptoms with thresh=0.5: 7\n",
      "  Top-1=60.3%, Top-3=78.6%, Top-5=83.9%\n",
      "\n",
      "\ud83c\udfc6 Best Pipeline Config: threshold=0.15, exponent=0.5\n",
      "   Top-1=69.0%, Top-3=85.1%, Top-5=89.5%\n"
     ]
    }
   ],
   "source": [
    "# ==== User-in-Loop Pipeline Sweep with Checkpointing ====\n",
    "print(\"=\"*60)\n",
    "print(\"USER-IN-LOOP PIPELINE EVALUATION (with checkpointing)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Clear checkpoint because evaluation logic changed\n",
    "clear_checkpoint('pipeline_sweep')\n",
    "state = load_checkpoint('pipeline_sweep')\n",
    "completed = state['completed']\n",
    "pipeline_results = state['results']\n",
    "\n",
    "# Pre-compute X for speed\n",
    "X_values = df_filtered[feature_cols].fillna(0).values\n",
    "y_values = df_filtered['diseases'].values\n",
    "\n",
    "# Full parameter space\n",
    "PIPELINE_THRESHOLDS = [0.15, 0.25, 0.35, 0.40, 0.45, 0.50]\n",
    "PIPELINE_EXPONENTS = [0.5, 1.0, 1.5, 2.0, 2.5]\n",
    "\n",
    "# Build list of configs to run\n",
    "configs_to_run = []\n",
    "\n",
    "for thresh in PIPELINE_THRESHOLDS:\n",
    "    for exp in PIPELINE_EXPONENTS:\n",
    "        config_key = f\"{thresh}_{exp}\"\n",
    "        \n",
    "        # Skip if already completed (checkpoint takes priority)\n",
    "        if config_key in completed:\n",
    "            continue\n",
    "        \n",
    "        configs_to_run.append((thresh, exp))\n",
    "\n",
    "total = len(PIPELINE_THRESHOLDS) * len(PIPELINE_EXPONENTS)\n",
    "print(f\"Total configs: {total}, Already completed: {len(completed)}, To run: {len(configs_to_run)}\")\n",
    "\n",
    "# \u2705 Use the BEST model from Part 1, not hardcoded\n",
    "base_encoder = SemanticSymptomEncoder(\n",
    "    model_name=\"multi-qa-mpnet-base-dot-v1\",  # <-- Changed from \"all-mpnet-base-v2\"\n",
    "    threshold=0.15,\n",
    "    exponent=1.0,\n",
    "    symptom_list=feature_cols\n",
    ")\n",
    "\n",
    "print(f\"Using model: {BEST_MODEL}\")\n",
    "\n",
    "for (thresh, exp) in configs_to_run:\n",
    "    config_key = f\"{thresh}_{exp}\"\n",
    "    \n",
    "    print(f\"\\n\u25b6 Testing thresh={thresh}, exp={exp}...\")\n",
    "    \n",
    "    # Update encoder parameters\n",
    "    base_encoder.threshold = thresh\n",
    "    base_encoder.exponent = exp\n",
    "    \n",
    "    # Quick debug to verify parameters work\n",
    "    test_text = \"headache and fever\"\n",
    "    result = base_encoder.encode_symptoms(test_text)\n",
    "    symptom_vector = result[\"symptom_vector\"]\n",
    "    \n",
    "    # Count non-zero symptoms\n",
    "    non_zero = np.sum(symptom_vector > 0.0)\n",
    "    print(f\"  Non-zero symptoms with thresh={thresh}: {non_zero}\")\n",
    "    \n",
    "    # Run the actual evaluation\n",
    "    metrics = evaluate_user_in_loop(\n",
    "        base_encoder, X_values, y_values, feature_cols,\n",
    "        cat_clf, specialist_models, all_diseases,\n",
    "        paraphrases,  # <-- Fixed: was 'paraphrase'\n",
    "        top_k=20, max_samples=500\n",
    "    )\n",
    "    \n",
    "    result = {\n",
    "        'threshold': thresh,\n",
    "        'exponent': exp,\n",
    "        **metrics\n",
    "    }\n",
    "    \n",
    "    pipeline_results.append(result)\n",
    "    completed[config_key] = True\n",
    "    \n",
    "    state['completed'] = completed\n",
    "    state['results'] = pipeline_results\n",
    "    save_checkpoint('pipeline_sweep', state)\n",
    "    \n",
    "    print(f\"  Top-1={metrics['top1']*100:.1f}%, Top-3={metrics['top3']*100:.1f}%, Top-5={metrics['top5']*100:.1f}%\")\n",
    "\n",
    "# Clean up\n",
    "del base_encoder\n",
    "\n",
    "# Best result\n",
    "if pipeline_results:\n",
    "    best_pipeline = max(pipeline_results, key=lambda x: x['top1'])\n",
    "    print(f\"\\n\ud83c\udfc6 Best Pipeline Config: threshold={best_pipeline['threshold']}, exponent={best_pipeline['exponent']}\")\n",
    "    print(f\"   Top-1={best_pipeline['top1']*100:.1f}%, Top-3={best_pipeline['top3']*100:.1f}%, Top-5={best_pipeline['top5']*100:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}